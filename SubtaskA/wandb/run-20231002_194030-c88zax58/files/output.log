c:\Users\Ghiki\AppData\Local\Programs\Python\Python311\Lib\site-packages\transformers\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

100%|██████████| 125/125 [00:03<00:00, 32.76it/s]
  0%|          | 0/5850 [00:00<?, ?it/s]c:\Users\Ghiki\AppData\Local\Programs\Python\Python311\Lib\site-packages\transformers\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(




























 10%|▉         | 584/5850 [00:59<08:28, 10.35it/s]


  warnings.warn(
 10%|█         | 586/5850 [01:05<1:27:58,  1.00s/it]




























 20%|█▉        | 1169/5850 [02:03<07:38, 10.21it/s]


  warnings.warn(
 20%|██        | 1171/5850 [02:09<1:18:15,  1.00s/it]

















 26%|██▌       | 1511/5850 [02:43<07:14,  9.99it/s]












 30%|██▉       | 1754/5850 [03:07<06:44, 10.12it/s]



 30%|███       | 1755/5850 [03:12<06:44, 10.12it/c:\Users\Ghiki\AppData\Local\Programs\Python\Python311\Lib\site-packages\transformers\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(




























 40%|████      | 2340/5850 [04:15<05:22, 10.89it/s]



 40%|████      | 2340/5850 [04:19<05:22, 10.89it/c:\Users\Ghiki\AppData\Local\Programs\Python\Python311\Lib\site-packages\transformers\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(




























 50%|████▉     | 2924/5850 [05:18<04:35, 10.61it/s]



 50%|█████     | 2925/5850 [05:23<04:35, 10.61it/c:\Users\Ghiki\AppData\Local\Programs\Python\Python311\Lib\site-packages\transformers\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(



 51%|█████▏    | 3000/5850 [05:32<04:45,  9.99it/s]
























 60%|██████    | 3510/5850 [06:21<03:34, 10.90it/s]




 60%|██████    | 3510/5850 [06:26<03:34, 10.90it/c:\Users\Ghiki\AppData\Local\Programs\Python\Python311\Lib\site-packages\transformers\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(




























 70%|██████▉   | 4094/5850 [07:26<02:47, 10.50it/s]



 70%|███████   | 4095/5850 [07:32<03:14,  9.05it/s]
c:\Users\Ghiki\AppData\Local\Programs\Python\Python311\Lib\site-packages\transformers\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
 30%|███       | 38/125 [00:01<00:02, 29.65it/s]

100%|██████████| 125/125 [00:04<00:00, 29.44it/s]
c:\Users\Ghiki\AppData\Local\Programs\Python\Python311\Lib\site-packages\transformers\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

100%|██████████| 125/125 [00:04<00:00, 26.25it/s]
c:\Users\Ghiki\AppData\Local\Programs\Python\Python311\Lib\site-packages\transformers\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(


100%|██████████| 125/125 [00:04<00:00, 30.50it/s]
[34m[1mwandb[39m[22m: [33mWARNING[39m Calling wandb.login() after wandb.init() has no effect.
Dataset size before balancing: (5000, 2)
Dataset size after balancing: (4736, 1)
Entried dropped: 264
Balanced DataFrame:
label
0    2368
1    2368
Name: count, dtype: int64
Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-large and are newly initialized: ['classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
c:\Users\Ghiki\AppData\Local\Programs\Python\Python311\Lib\site-packages\transformers\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(




100%|██████████| 125/125 [00:10<00:00, 11.94it/s]
c:\Users\Ghiki\AppData\Local\Programs\Python\Python311\Lib\site-packages\transformers\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(




100%|██████████| 125/125 [00:10<00:00, 11.52it/s]
  0%|          | 0/5920 [00:00<?, ?it/s]c:\Users\Ghiki\AppData\Local\Programs\Python\Python311\Lib\site-packages\transformers\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(









































































 10%|█         | 592/5920 [02:26<22:29,  3.95it/s]





100%|██████████| 125/125 [00:11<00:00, 11.52it/s]

 10%|█         | 592/5920 [02:38<22:29,  3.95it/sc:\Users\Ghiki\AppData\Local\Programs\Python\Python311\Lib\site-packages\transformers\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(






































































 20%|██        | 1184/5920 [05:14<18:33,  4.25it/s]






 20%|██        | 1184/5920 [05:24<18:33,  4.25it/c:\Users\Ghiki\AppData\Local\Programs\Python\Python311\Lib\site-packages\transformers\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(





































 25%|██▌       | 1500/5920 [06:48<17:39,  4.17it/s]
































 30%|███       | 1776/5920 [07:53<15:57,  4.33it/s]




  _warn_prf(average, modifier, msg_start, len(result))
 30%|███       | 1776/5920 [08:03<15:57,  4.33it/s]
 30%|███       | 1776/5920 [08:03<15:57,  4.33it/c:\Users\Ghiki\AppData\Local\Programs\Python\Python311\Lib\site-packages\transformers\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(





































































 40%|████      | 2368/5920 [10:32<13:27,  4.40it/s]




  _warn_prf(average, modifier, msg_start, len(result))
 40%|████      | 2368/5920 [10:43<13:27,  4.40it/s]
 40%|████      | 2368/5920 [10:52<16:18,  3.63it/s]
c:\Users\Ghiki\AppData\Local\Programs\Python\Python311\Lib\site-packages\transformers\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
  2%|▏         | 2/125 [00:00<00:14,  8.28it/s]




100%|██████████| 125/125 [00:10<00:00, 12.31it/s]
c:\Users\Ghiki\AppData\Local\Programs\Python\Python311\Lib\site-packages\transformers\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(





100%|██████████| 125/125 [00:10<00:00, 11.98it/s]
c:\Users\Ghiki\AppData\Local\Programs\Python\Python311\Lib\site-packages\transformers\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(





100%|██████████| 125/125 [00:10<00:00, 12.45it/s]
[34m[1mwandb[39m[22m: [33mWARNING[39m Calling wandb.login() after wandb.init() has no effect.
Dataset size before balancing: (5000, 2)
Dataset size after balancing: (4736, 1)
Entried dropped: 264
Balanced DataFrame:
label
0    2368
1    2368
Name: count, dtype: int64
Some weights of DebertaForSequenceClassification were not initialized from the model checkpoint at microsoft/deberta-large and are newly initialized: ['classifier.bias', 'classifier.weight', 'pooler.dense.weight', 'pooler.dense.bias']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
c:\Users\Ghiki\AppData\Local\Programs\Python\Python311\Lib\site-packages\transformers\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(






100%|██████████| 125/125 [00:13<00:00,  9.11it/s]
c:\Users\Ghiki\AppData\Local\Programs\Python\Python311\Lib\site-packages\transformers\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(






100%|██████████| 125/125 [00:14<00:00,  8.90it/s]
  0%|          | 0/5920 [00:00<?, ?it/s]c:\Users\Ghiki\AppData\Local\Programs\Python\Python311\Lib\site-packages\transformers\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(






























































































 10%|█         | 592/5920 [03:09<27:27,  3.23it/s]








 10%|█         | 592/5920 [03:23<27:27,  3.23it/sc:\Users\Ghiki\AppData\Local\Programs\Python\Python311\Lib\site-packages\transformers\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(




























































































 20%|██        | 1184/5920 [06:39<24:15,  3.25it/s]








 20%|██        | 1184/5920 [06:53<24:15,  3.25it/c:\Users\Ghiki\AppData\Local\Programs\Python\Python311\Lib\site-packages\transformers\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(


















































 25%|██▌       | 1500/5920 [08:42<24:15,  3.04it/s]











































 30%|███       | 1776/5920 [10:09<20:56,  3.30it/s]








 30%|███       | 1776/5920 [10:24<20:56,  3.30it/c:\Users\Ghiki\AppData\Local\Programs\Python\Python311\Lib\site-packages\transformers\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(






























































































 40%|████      | 2368/5920 [13:36<18:22,  3.22it/s]








 40%|████      | 2368/5920 [13:50<18:22,  3.22it/c:\Users\Ghiki\AppData\Local\Programs\Python\Python311\Lib\site-packages\transformers\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(





























































































 50%|█████     | 2960/5920 [17:02<15:01,  3.28it/s]






 97%|█████████▋| 121/125 [00:13<00:00,  8.96it/s]

 50%|█████     | 2960/5920 [17:16<15:01,  3.28it/c:\Users\Ghiki\AppData\Local\Programs\Python\Python311\Lib\site-packages\transformers\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(







 51%|█████     | 3004/5920 [17:38<15:46,  3.08it/s]






















































































 60%|██████    | 3552/5920 [20:31<15:24,  2.56it/s]








 60%|██████    | 3552/5920 [20:45<15:24,  2.56it/c:\Users\Ghiki\AppData\Local\Programs\Python\Python311\Lib\site-packages\transformers\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(






























































































 70%|███████   | 4144/5920 [23:58<08:48,  3.36it/s]








 70%|███████   | 4144/5920 [24:12<08:48,  3.36it/c:\Users\Ghiki\AppData\Local\Programs\Python\Python311\Lib\site-packages\transformers\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(

























































 76%|███████▌  | 4504/5920 [26:12<07:27,  3.16it/s]





































 80%|████████  | 4736/5920 [27:27<06:30,  3.03it/s]






 91%|█████████ | 114/125 [00:12<00:01,  8.77it/s]

 80%|████████  | 4736/5920 [27:41<06:30,  3.03it/c:\Users\Ghiki\AppData\Local\Programs\Python\Python311\Lib\site-packages\transformers\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(






























































































 90%|█████████ | 5328/5920 [30:56<03:06,  3.17it/s]






 86%|████████▋ | 108/125 [00:12<00:01,  8.62it/s]

 90%|█████████ | 5328/5920 [31:10<03:06,  3.17it/c:\Users\Ghiki\AppData\Local\Programs\Python\Python311\Lib\site-packages\transformers\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(






























































































100%|██████████| 5920/5920 [34:25<00:00,  3.23it/s]






 98%|█████████▊| 122/125 [00:13<00:00,  8.86it/s]


100%|██████████| 5920/5920 [34:43<00:00,  2.84it/s]
c:\Users\Ghiki\AppData\Local\Programs\Python\Python311\Lib\site-packages\transformers\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(






100%|██████████| 125/125 [00:13<00:00,  9.14it/s]
c:\Users\Ghiki\AppData\Local\Programs\Python\Python311\Lib\site-packages\transformers\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(






100%|██████████| 125/125 [00:14<00:00,  8.84it/s]
c:\Users\Ghiki\AppData\Local\Programs\Python\Python311\Lib\site-packages\transformers\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(







100%|██████████| 125/125 [00:13<00:00,  9.21it/s]
Dataset size before balancing: (5000, 2)
Dataset size after balancing: (4736, 1)
Entried dropped: 264
Balanced DataFrame:
label
0    2368
1    2368
Name: count, dtype: int64
[34m[1mwandb[39m[22m: [33mWARNING[39m Calling wandb.login() after wandb.init() has no effect.
Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.
c:\Users\Ghiki\AppData\Local\Programs\Python\Python311\Lib\site-packages\transformers\tokenization_utils_base.py:2418: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).
  warnings.warn(
 49%|████▉     | 61/125 [00:02<00:02, 28.89it/s]
 96%|█████████▌| 120/125 [00:04<00:00, 29.51it/s]
100%|██████████| 125/125 [00:04<00:00, 28.85it/s]
100%|██████████| 125/125 [00:04<00:00, 28.85it/s]
  0%|          | 10/5920 [00:01<09:54,  9.95it/s]
  1%|          | 30/5920 [00:02<09:04, 10.81it/s]
  1%|          | 52/5920 [00:05<09:21, 10.46it/s]
  1%|          | 72/5920 [00:06<09:52,  9.86it/s]
  2%|▏         | 91/5920 [00:08<09:39, 10.07it/s]
  2%|▏         | 110/5920 [00:10<11:24,  8.49it/s]
  2%|▏         | 131/5920 [00:13<09:49,  9.82it/s]
  3%|▎         | 153/5920 [00:15<09:06, 10.56it/s]
  3%|▎         | 173/5920 [00:16<09:08, 10.48it/s]
  3%|▎         | 195/5920 [00:19<09:12, 10.36it/s]
  4%|▎         | 215/5920 [00:20<09:34,  9.93it/s]
  4%|▍         | 237/5920 [00:23<08:48, 10.75it/s]
  4%|▍         | 259/5920 [00:25<08:37, 10.94it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
{'eval_loss': 0.39613446593284607, 'eval_accuracy': 0.886, 'eval_f1': 0.8853118712273643, 'eval_auc': 0.8877573930335493, 'eval_precision': 0.8494208494208494, 'eval_recall': 0.9243697478991597, 'eval_runtime': 4.545, 'eval_samples_per_second': 220.022, 'eval_steps_per_second': 27.503, 'epoch': 1.0}
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
{'eval_loss': 0.4487890899181366, 'eval_accuracy': 0.877, 'eval_f1': 0.8825214899713467, 'eval_auc': 0.8812864840592726, 'eval_precision': 0.809106830122592, 'eval_recall': 0.9705882352941176, 'eval_runtime': 4.676, 'eval_samples_per_second': 213.856, 'eval_steps_per_second': 26.732, 'epoch': 2.0}
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
{'loss': 0.2951, 'learning_rate': 4.077490774907749e-05, 'epoch': 2.53}
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
{'eval_loss': 0.3517392575740814, 'eval_accuracy': 0.923, 'eval_f1': 0.922300706357215, 'eval_auc': 0.9246985053563411, 'eval_precision': 0.887378640776699, 'eval_recall': 0.9600840336134454, 'eval_runtime': 4.4143, 'eval_samples_per_second': 226.539, 'eval_steps_per_second': 28.317, 'epoch': 3.0}
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
{'eval_loss': 0.9765346646308899, 'eval_accuracy': 0.849, 'eval_f1': 0.8610855565777369, 'eval_auc': 0.8551462569760729, 'eval_precision': 0.7659574468085106, 'eval_recall': 0.9831932773109243, 'eval_runtime': 4.5487, 'eval_samples_per_second': 219.845, 'eval_steps_per_second': 27.481, 'epoch': 4.0}
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
{'eval_loss': 0.8035147786140442, 'eval_accuracy': 0.885, 'eval_f1': 0.8897411313518695, 'eval_auc': 0.8891125152351017, 'eval_precision': 0.818342151675485, 'eval_recall': 0.9747899159663865, 'eval_runtime': 4.4865, 'eval_samples_per_second': 222.893, 'eval_steps_per_second': 27.862, 'epoch': 5.0}
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
{'loss': 0.0572, 'learning_rate': 2.693726937269373e-05, 'epoch': 5.07}
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
  5%|▍         | 279/5920 [00:27<09:02, 10.39it/s]
{'eval_loss': 0.9513245820999146, 'eval_accuracy': 0.889, 'eval_f1': 0.8935762224352828, 'eval_auc': 0.8931217525178011, 'eval_precision': 0.8218694885361552, 'eval_recall': 0.9789915966386554, 'eval_runtime': 4.5183, 'eval_samples_per_second': 221.323, 'eval_steps_per_second': 27.665, 'epoch': 6.0}
  6%|▌         | 7/125 [00:00<00:04, 27.51it/s]/s]
  6%|▌         | 7/125 [00:00<00:04, 27.51it/s]/s]
  0%|          | 0/125 [00:00<?, ?it/s].51it/s]/s]
  0%|          | 0/125 [00:00<?, ?it/s].51it/s]/s]
  0%|          | 0/125 [00:00<?, ?it/s].51it/s]/s]
 32%|███▏      | 40/125 [00:01<00:02, 28.85it/s]s]
 32%|███▏      | 40/125 [00:01<00:02, 28.85it/s]s]
 32%|███▏      | 40/125 [00:01<00:02, 28.85it/s]s]
