{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from statistics import mode\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline, Trainer\n",
    "from datasets import Dataset, load_metric\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Variables and parameters'''\n",
    "\n",
    "MODEL1='bert-base-uncased'\n",
    "MODEL2='microsoft/deberta-large'\n",
    "MODEL3='roberta-base'\n",
    "MODEL4='roberta-large'\n",
    "\n",
    "MODEL_PATH1='SavedModels/bert-base-uncased5k'\n",
    "MODEL_PATH2='SavedModels/deberta-large5k'\n",
    "MODEL_PATH3='SavedModels/roberta-base5k'\n",
    "MODEL_PATH4='SavedModels/roberta-large5k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''metrics'''\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    labels = p.label_ids\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    auc = roc_auc_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'auc': auc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Load tokenizers and models'''\n",
    "\n",
    "tokenizer1 = AutoTokenizer.from_pretrained(MODEL1)\n",
    "model1 = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH1)\n",
    "\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(MODEL2)\n",
    "model2 = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH2)\n",
    "\n",
    "tokenizer3 = AutoTokenizer.from_pretrained(MODEL3)\n",
    "model3 = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH3)\n",
    "\n",
    "tokenizer4 = AutoTokenizer.from_pretrained(MODEL4)\n",
    "model4 = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH4)\n",
    "\n",
    "pipe1 = pipeline(\"text-classification\", model=model1, tokenizer=tokenizer1, device=0)\n",
    "pipe2 = pipeline(\"text-classification\", model=model2, tokenizer=tokenizer2, device=0)\n",
    "pipe3 = pipeline(\"text-classification\", model=model3, tokenizer=tokenizer3, device=0)\n",
    "pipe4 = pipeline(\"text-classification\", model=model4, tokenizer=tokenizer4, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size before balancing: (5000, 2)\n",
      "Dataset size after balancing: (5000, 1)\n",
      "Entried dropped: 0\n",
      "\n",
      "Balanced DataFrame:\n",
      "label\n",
      "0    2500\n",
      "1    2500\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "'''Loading data'''\n",
    "\n",
    "import pandas as pd,os\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "SAMPLES_TO_TRAIN=5000\n",
    "\n",
    "df = pd.read_json(os.getcwd()+'/datasets/subtaskA_dev_monolingual.jsonl', lines=True)\n",
    "df = df[['text', 'label']]\n",
    "\n",
    "# test_df=df.sample(round(SAMPLES_TO_TRAIN))\n",
    "\n",
    "# Shuffle the DataFrame\n",
    "df = df.sample(frac=1, random_state=42)\n",
    "\n",
    "# Reset index\n",
    "test_df = df.reset_index(drop=True)\n",
    "\n",
    "# we balance the training set\n",
    "print(f'Dataset size before balancing: {test_df.shape}')\n",
    "counts = test_df['label'].value_counts()\n",
    "sampler = RandomUnderSampler(random_state=42)\n",
    "x_text, y = sampler.fit_resample(test_df[['text']], test_df['label'])\n",
    "\n",
    "print(f'Dataset size after balancing: {x_text.shape}')\n",
    "print(f'Entried dropped: {test_df.shape[0]-x_text.shape[0]}')\n",
    "\n",
    "# Create a new balanced DataFrame\n",
    "test_df = pd.DataFrame({'text': x_text['text'], 'label': y})\n",
    "\n",
    "# Print the balanced DataFrame\n",
    "print(\"\\nBalanced DataFrame:\")\n",
    "print(test_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing with pipe1:   0%|          | 8/5000 [00:00<01:04, 76.92it/s]c:\\Users\\Ghiki\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pipelines\\base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "Processing with pipe1: 100%|██████████| 5000/5000 [00:42<00:00, 117.84it/s]\n",
      "Processing with pipe2: 100%|██████████| 5000/5000 [02:18<00:00, 36.17it/s]\n",
      "Processing with pipe3: 100%|██████████| 5000/5000 [00:43<00:00, 115.59it/s]\n",
      "Processing with pipe4: 100%|██████████| 5000/5000 [01:20<00:00, 62.47it/s]\n"
     ]
    }
   ],
   "source": [
    "'''Getting predictions from models'''\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "test_texts = test_df['text'].tolist()\n",
    "\n",
    "results1 = [pipe1(text, truncation=True, max_length=256) for text in tqdm(test_texts, desc=\"Processing with pipe1\")]\n",
    "results2 = [pipe2(text, truncation=True, max_length=256) for text in tqdm(test_texts, desc=\"Processing with pipe2\")]\n",
    "results3 = [pipe3(text, truncation=True, max_length=256) for text in tqdm(test_texts, desc=\"Processing with pipe3\")]\n",
    "results4 = [pipe4(text, truncation=True, max_length=256) for text in tqdm(test_texts, desc=\"Processing with pipe4\")]\n",
    "\n",
    "labels1 = [0 if item['label'] == 'LABEL_0' else 1 for d in results1 for item in d]\n",
    "scores1 = [item['score'] for d in results1 for item in d]\n",
    "\n",
    "labels2 = [0 if item['label'] == 'LABEL_0' else 1 for d in results2 for item in d]\n",
    "scores2 = [item['score'] for d in results2 for item in d]\n",
    "\n",
    "labels3 = [0 if item['label'] == 'LABEL_0' else 1 for d in results3 for item in d]\n",
    "scores3 = [item['score'] for d in results3 for item in d]\n",
    "\n",
    "labels4 = [0 if item['label'] == 'LABEL_0' else 1 for d in results4 for item in d]\n",
    "scores4 = [item['score'] for d in results4 for item in d]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Get metrics'''\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score\n",
    "\n",
    "def getMetrics(predicted_labels, true_labels):\n",
    "    # Ensure the labels are numpy arrays\n",
    "    predicted_labels = np.array(predicted_labels)\n",
    "    true_labels = np.array(true_labels)\n",
    "\n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    f1 = f1_score(true_labels, predicted_labels, average='binary')\n",
    "    precision = precision_score(true_labels, predicted_labels, average='binary')\n",
    "    recall = recall_score(true_labels, predicted_labels, average='binary')\n",
    "    auc = roc_auc_score(true_labels, predicted_labels)\n",
    "\n",
    "    # Create a dictionary of metrics\n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'auc': auc\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.732, 'f1': 0.6903881700554528, 'precision': 0.8172866520787746, 'recall': 0.5976, 'auc': 0.732}\n",
      "{'accuracy': 0.6868, 'f1': 0.570958904109589, 'precision': 0.9060869565217391, 'recall': 0.4168, 'auc': 0.6868000000000001}\n",
      "{'accuracy': 0.7944, 'f1': 0.7503642544924721, 'precision': 0.9548825710754018, 'recall': 0.618, 'auc': 0.7943999999999999}\n",
      "{'accuracy': 0.839, 'f1': 0.8466958674538183, 'precision': 0.8080697928026173, 'recall': 0.8892, 'auc': 0.839}\n"
     ]
    }
   ],
   "source": [
    "print(getMetrics(labels1,test_df['label'].tolist()))\n",
    "print(getMetrics(labels2,test_df['label'].tolist()))\n",
    "print(getMetrics(labels3,test_df['label'].tolist()))\n",
    "print(getMetrics(labels4,test_df['label'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Voting\n",
      "{'accuracy': 0.7368, 'f1': 0.6556776556776557, 'precision': 0.9478063540090772, 'recall': 0.5012, 'auc': 0.7367999999999999}\n",
      "\n",
      "Weighted Voting\n",
      "{'accuracy': 0.803, 'f1': 0.7715147297610763, 'precision': 0.9182771949199338, 'recall': 0.6652, 'auc': 0.803}\n"
     ]
    }
   ],
   "source": [
    "'''Simple Ensemble models'''\n",
    "\n",
    "# Assume predictions is a 2D numpy array where each column represents predictions from one model\n",
    "predictions = np.array([labels1, labels2, labels3, labels4]).T\n",
    "\n",
    "print('Majority Voting')\n",
    "\n",
    "from scipy.stats import mode\n",
    "import numpy as np\n",
    "\n",
    "# Compute the mode along axis 1\n",
    "final_predictions, _ = mode(predictions, axis=1)\n",
    "final_predictions_MV = final_predictions.flatten()\n",
    "\n",
    "print(getMetrics(final_predictions_MV,test_df['label'].tolist()))\n",
    "\n",
    "\n",
    "print('\\nWeighted Voting')\n",
    "weights = np.array([0.4, 0.3, 0.2, 0.1])  # for example\n",
    "weighted_predictions = np.average(predictions, axis=1, weights=weights)\n",
    "\n",
    "# Round to nearest integer to get final class labels\n",
    "final_predictions_WV = np.round(weighted_predictions).astype(int)\n",
    "print(getMetrics(final_predictions_WV,test_df['label'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal weights: [0.25 0.25 0.25 0.25]\n"
     ]
    }
   ],
   "source": [
    "'''optimize weights to get best metrics'''\n",
    "\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# This is the optimization objective function, it should return the value that you want to minimize\n",
    "def objective(weights: np.ndarray, *args):\n",
    "    weighted_predictions = np.average(predictions, axis=1, weights=weights)\n",
    "    final_predictions = np.round(weighted_predictions).astype(int)\n",
    "    \n",
    "    metrics = getMetrics(final_predictions, args[0])  # args[0] is expected to be true labels\n",
    "    f1_score = metrics['f1']\n",
    "    \n",
    "    return -f1_score  # We negate the f1_score because we want to maximize it\n",
    "\n",
    "predictions = np.array([labels1, labels2, labels3, labels4]).T\n",
    "true_labels = np.array(test_df['label'].tolist())\n",
    "\n",
    "initial_weights = [0.25, 0.25, 0.25, 0.25]  # For example\n",
    "\n",
    "# Weights should be bounded between 0 and 1\n",
    "bounds = [(0, 1)] * len(initial_weights)\n",
    "\n",
    "# Constraints: weights should sum to 1\n",
    "constraints = ({'type': 'eq', 'fun': lambda weights: np.sum(weights) - 1})\n",
    "\n",
    "# Optimize weights\n",
    "result = minimize(objective, initial_weights, args=(true_labels,), \n",
    "                  method='SLSQP', bounds=bounds, constraints=constraints)\n",
    "\n",
    "# The optimal weights are now in result.x\n",
    "optimal_weights = result.x\n",
    "\n",
    "print(\"Optimal weights:\", optimal_weights)\n",
    "\n",
    "# like Majority Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.8636, 'f1': 0.8533333333333334, 'precision': 0.9227906976744186, 'recall': 0.7936, 'auc': 0.8636}\n"
     ]
    }
   ],
   "source": [
    "'''Complex Ensemble models'''\n",
    "\n",
    "'''Stacking'''\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Training the stacking model\n",
    "meta_model = LogisticRegression()\n",
    "meta_model.fit(predictions, test_df['label'].tolist())\n",
    "\n",
    "final_predictions_ST=meta_model.predict(predictions)\n",
    "print(getMetrics(final_predictions_ST,test_df['label'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.892, 'f1': 0.8934911242603549, 'precision': 0.8813229571984436, 'recall': 0.906, 'auc': 0.8920000000000001}\n"
     ]
    }
   ],
   "source": [
    "'''Complex Ensemble models'''\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "'''Random Forest'''\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Labels_Model1': labels1,\n",
    "    'Scores_Model1': scores1,\n",
    "    'Labels_Model2': labels2,\n",
    "    'Scores_Model2': scores2,\n",
    "    'Labels_Model3': labels3,\n",
    "    'Scores_Model3': scores3,\n",
    "    'Labels_Model4': labels4,\n",
    "    'Scores_Model4': scores4,\n",
    "})\n",
    "\n",
    "labels = test_df['label'].tolist()\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a Random Forest Classifier and train it on the training data\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(getMetrics(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for the training dataset the best (5k samples) is random forest:\n",
      "{'accuracy': 0.9936305732484076, 'f1': 0.9936575052854123, 'precision': 0.9915611814345991, 'recall': 0.9957627118644068, 'auc': 0.9936260367832672}\n",
      "\n",
      "for the dev dataset the best (5k samples) is random forest:\n",
      "{'accuracy': 0.892, 'f1': 0.8934911242603549, 'precision': 0.8813229571984436, 'recall': 0.906, 'auc': 0.8920000000000001}\n"
     ]
    }
   ],
   "source": [
    "print('''for the training dataset the best (5k samples) is random forest:\\n{'accuracy': 0.9936305732484076, 'f1': 0.9936575052854123, 'precision': 0.9915611814345991, 'recall': 0.9957627118644068, 'auc': 0.9936260367832672}''')\n",
    "\n",
    "print('''\\nfor the dev dataset the best (5k samples) is random forest:\\n{'accuracy': 0.892, 'f1': 0.8934911242603549, 'precision': 0.8813229571984436, 'recall': 0.906, 'auc': 0.8920000000000001}''')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
