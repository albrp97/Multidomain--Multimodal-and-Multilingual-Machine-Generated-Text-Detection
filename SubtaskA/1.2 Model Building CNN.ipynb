{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ghiki\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malberto-rodero557\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Import libraries'''\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import wandb\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, AdamW, Trainer, TrainingArguments\n",
    "from tqdm import tqdm\n",
    "from torch.nn import functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Variables and parameters'''\n",
    "\n",
    "SAMPLES_TO_TRAIN=10000\n",
    "DIMENSIONS=200\n",
    "\n",
    "N_LABELS=2\n",
    "MAX_LEN = 256\n",
    "EPOCHS=50\n",
    "PATIENCE=10\n",
    "LEARNING_RATE=.00005\n",
    "WEIGHT_DECAY=.01\n",
    "BATCH_SIZE=16\n",
    "METRIC_FOR_BEST_MODEL='eval_loss'\n",
    "if METRIC_FOR_BEST_MODEL=='eval_loss':\n",
    "    GREATER_IS_BETTER = False\n",
    "else:\n",
    "    GREATER_IS_BETTER = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size before balancing: (10000, 2)\n",
      "Dataset size after balancing: (9454, 1)\n",
      "Entried dropped: 546\n",
      "\n",
      "Balanced DataFrame:\n",
      "label\n",
      "0    4727\n",
      "1    4727\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "'''Preparing dataset'''\n",
    "\n",
    "df = pd.read_json(os.getcwd()+'/datasets/subtaskA_train_monolingual.jsonl', lines=True)\n",
    "df = df[['text', 'label']]\n",
    "\n",
    "df=df.sample(round(SAMPLES_TO_TRAIN))\n",
    "# test_train_df=df.sample(round(SAMPLES_TO_TRAIN*.2))\n",
    "\n",
    "# df = pd.read_json(os.getcwd()+'/datasets/subtaskA_dev_monolingual.jsonl', lines=True)\n",
    "# df = df[['text', 'label']]\n",
    "\n",
    "# val_df= df.sample(round(SAMPLES_TO_TRAIN*.2))\n",
    "# test_dev_df= df.sample(round(SAMPLES_TO_TRAIN*.2))\n",
    "\n",
    "# we balance the training set\n",
    "print(f'Dataset size before balancing: {df.shape}')\n",
    "counts = df['label'].value_counts()\n",
    "sampler = RandomUnderSampler(random_state=42)\n",
    "x_text, y = sampler.fit_resample(df[['text']], df['label'])\n",
    "\n",
    "print(f'Dataset size after balancing: {x_text.shape}')\n",
    "print(f'Entried dropped: {df.shape[0]-x_text.shape[0]}')\n",
    "\n",
    "# Create a new balanced DataFrame\n",
    "df = pd.DataFrame({'text': x_text['text'], 'label': y})\n",
    "\n",
    "# Print the balanced DataFrame\n",
    "print(\"\\nBalanced DataFrame:\")\n",
    "print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "'''loading glove'''\n",
    "embeddings_index={}\n",
    "with open('../0 playground and indoor/OtherData/glove.6B.200d.txt','r',encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values=line.split()\n",
    "        word=values[0]\n",
    "        vectors=np.asarray(values[1:],'float32')\n",
    "        embeddings_index[word]=vectors\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training df:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9454/9454 [00:13<00:00, 696.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9454, 200)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''glove building'''\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm \n",
    "\n",
    "def sent2vec(s):\n",
    "    \"\"\" Function Creates a normalized vector for the whole sentence\"\"\"\n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(embeddings_index[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(200)\n",
    "    return v / np.sqrt((v ** 2).sum())\n",
    "\n",
    "print('Training df:')\n",
    "df_x = np.array([sent2vec(x) for x in tqdm(df['text'])])\n",
    "print(df_x.shape)\n",
    "train_y=df['label']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Preparing for training'''\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# Fit the scaler to the training data and transform the data\n",
    "train_x = scaler.fit_transform(df_x)\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Save the trained scaler\n",
    "with open('scaler.pkl', 'wb') as scaler_file:\n",
    "    pickle.dump(scaler, scaler_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''metrics'''\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    labels = p.label_ids\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    auc = roc_auc_score(labels, preds)\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'auc': auc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32).unsqueeze(1)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "        self.len = len(X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {'x': self.X[index], 'label': self.y[index], 'label_ids': index}\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train_x, train_y.values, test_size=0.2, random_state=42)\n",
    "traindata = Data(X_train, y_train)\n",
    "testdata = Data(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of features (len of X cols)\n",
    "input_dim = train_x.shape[-1]\n",
    "\n",
    "# number of classes (unique of y)\n",
    "output_dim = 2\n",
    "\n",
    "class CNN1D(nn.Module):\n",
    "    def __init__(self, input_dim, num_classes):\n",
    "        super(CNN1D, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=100, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(100)\n",
    "        self.conv2 = nn.Conv1d(in_channels=100, out_channels=150, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(150)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(150 * 200, 256)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(256, num_classes)\n",
    "\n",
    "    def forward(self, x, labels=None):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        # Flatten the output for the dense layer\n",
    "        x = torch.flatten(x, 1) \n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(x, labels)\n",
    "            return loss, x\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Instantiate the model with appropriate dimensions\n",
    "model = CNN1D(input_dim=200, num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [00:00<00:00, 729.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6939518451690674, 'eval_accuracy': 0.5076679005817029, 'eval_f1': 0.6119216340141727, 'eval_auc': 0.5086693110862032, 'eval_precision': 0.5037748798901853, 'eval_recall': 0.7791932059447984, 'eval_runtime': 0.1669, 'eval_samples_per_second': 11331.875, 'eval_steps_per_second': 713.111}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 466/23650 [00:01<01:12, 320.03it/s]\n",
      "  2%|▏         | 499/23650 [00:02<02:00, 192.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5128917694091797, 'eval_accuracy': 0.7440507667900582, 'eval_f1': 0.7034313725490196, 'eval_auc': 0.7435539477246135, 'eval_precision': 0.8318840579710145, 'eval_recall': 0.6093418259023354, 'eval_runtime': 0.1136, 'eval_samples_per_second': 16639.943, 'eval_steps_per_second': 1047.146, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 938/23650 [00:03<01:12, 312.79it/s]\n",
      "  4%|▍         | 970/23650 [00:03<01:56, 194.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.48852044343948364, 'eval_accuracy': 0.7514542570068746, 'eval_f1': 0.7528916929547845, 'eval_auc': 0.7514860877132931, 'eval_precision': 0.7458333333333333, 'eval_recall': 0.7600849256900213, 'eval_runtime': 0.1107, 'eval_samples_per_second': 17080.271, 'eval_steps_per_second': 1074.856, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 1396/23650 [00:05<01:06, 332.66it/s]\n",
      "  6%|▌         | 1430/23650 [00:05<01:47, 207.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4398241639137268, 'eval_accuracy': 0.7948175568482284, 'eval_f1': 0.7844444444444445, 'eval_auc': 0.7946503079563022, 'eval_precision': 0.8228438228438228, 'eval_recall': 0.7494692144373672, 'eval_runtime': 0.1078, 'eval_samples_per_second': 17543.14, 'eval_steps_per_second': 1103.984, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 1860/23650 [00:06<01:04, 337.24it/s]\n",
      "  8%|▊         | 1922/23650 [00:07<01:36, 224.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4376523196697235, 'eval_accuracy': 0.7937599153886833, 'eval_f1': 0.7724620770128354, 'eval_auc': 0.7934242995755953, 'eval_precision': 0.8575129533678757, 'eval_recall': 0.70276008492569, 'eval_runtime': 0.1047, 'eval_samples_per_second': 18061.441, 'eval_steps_per_second': 1136.6, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|▉         | 2357/23650 [00:08<01:05, 323.55it/s]\n",
      " 10%|█         | 2390/23650 [00:08<01:45, 200.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4150914251804352, 'eval_accuracy': 0.7985193019566367, 'eval_f1': 0.7804034582132564, 'eval_auc': 0.7982248606757811, 'eval_precision': 0.8537200504413619, 'eval_recall': 0.7186836518046709, 'eval_runtime': 0.1086, 'eval_samples_per_second': 17408.832, 'eval_steps_per_second': 1095.532, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 2829/23650 [00:10<01:06, 313.52it/s]\n",
      " 12%|█▏        | 2861/23650 [00:10<01:47, 194.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.46151089668273926, 'eval_accuracy': 0.7932310946589106, 'eval_f1': 0.7651651651651652, 'eval_auc': 0.792799549866996, 'eval_precision': 0.8810511756569848, 'eval_recall': 0.6762208067940552, 'eval_runtime': 0.1125, 'eval_samples_per_second': 16814.063, 'eval_steps_per_second': 1058.103, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 3297/23650 [00:11<00:58, 345.78it/s]\n",
      " 14%|█▍        | 3332/23650 [00:12<01:35, 212.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4040926694869995, 'eval_accuracy': 0.8254891591750396, 'eval_f1': 0.8252118644067797, 'eval_auc': 0.8254945981802276, 'eval_precision': 0.8234672304439746, 'eval_recall': 0.826963906581741, 'eval_runtime': 0.1054, 'eval_samples_per_second': 17941.324, 'eval_steps_per_second': 1129.042, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 3774/23650 [00:13<01:01, 321.25it/s]\n",
      " 16%|█▌        | 3807/23650 [00:13<01:36, 206.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4192061722278595, 'eval_accuracy': 0.8202009518773136, 'eval_f1': 0.8227320125130344, 'eval_auc': 0.820265046008873, 'eval_precision': 0.8084016393442623, 'eval_recall': 0.8375796178343949, 'eval_runtime': 0.102, 'eval_samples_per_second': 18532.718, 'eval_steps_per_second': 1166.258, 'epoch': 8.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 4249/23650 [00:15<00:57, 334.70it/s]\n",
      " 18%|█▊        | 4283/23650 [00:15<01:35, 203.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4224974811077118, 'eval_accuracy': 0.8180856689582232, 'eval_f1': 0.8084632516703787, 'eval_auc': 0.8179109085661742, 'eval_precision': 0.8501170960187353, 'eval_recall': 0.7707006369426752, 'eval_runtime': 0.1074, 'eval_samples_per_second': 17611.348, 'eval_steps_per_second': 1108.276, 'epoch': 9.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 4721/23650 [00:16<00:56, 335.99it/s]\n",
      " 20%|██        | 4755/23650 [00:17<01:31, 205.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.42359766364097595, 'eval_accuracy': 0.8291909042834479, 'eval_f1': 0.823593664664118, 'eval_auc': 0.8290848115907011, 'eval_precision': 0.8481439820022497, 'eval_recall': 0.8004246284501062, 'eval_runtime': 0.1063, 'eval_samples_per_second': 17781.958, 'eval_steps_per_second': 1119.013, 'epoch': 10.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 5199/23650 [00:18<00:56, 324.48it/s]\n",
      " 22%|██▏       | 5203/23650 [00:18<00:56, 324.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4150660037994385, 'eval_accuracy': 0.8244315177154945, 'eval_f1': 0.8173817381738175, 'eval_auc': 0.8242999111815095, 'eval_precision': 0.8481735159817352, 'eval_recall': 0.7887473460721869, 'eval_runtime': 0.1171, 'eval_samples_per_second': 16142.149, 'eval_steps_per_second': 1015.82, 'epoch': 11.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 5675/23650 [00:20<00:52, 340.53it/s]\n",
      " 24%|██▍       | 5676/23650 [00:20<00:52, 340.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.44658663868904114, 'eval_accuracy': 0.8281332628239026, 'eval_f1': 0.8232735182164219, 'eval_auc': 0.8280428163291789, 'eval_precision': 0.8439241917502787, 'eval_recall': 0.8036093418259024, 'eval_runtime': 0.1131, 'eval_samples_per_second': 16720.132, 'eval_steps_per_second': 1052.192, 'epoch': 12.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 6145/23650 [00:21<00:53, 325.36it/s]\n",
      " 26%|██▌       | 6149/23650 [00:21<00:53, 325.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.43658632040023804, 'eval_accuracy': 0.8191433104177683, 'eval_f1': 0.8097886540600667, 'eval_auc': 0.8189724796914397, 'eval_precision': 0.8504672897196262, 'eval_recall': 0.772823779193206, 'eval_runtime': 0.1083, 'eval_samples_per_second': 17453.615, 'eval_steps_per_second': 1098.35, 'epoch': 13.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 6609/23650 [00:23<00:50, 337.21it/s]\n",
      " 28%|██▊       | 6643/23650 [00:23<01:22, 207.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.438464492559433, 'eval_accuracy': 0.8328926493918561, 'eval_f1': 0.8302900107411386, 'eval_auc': 0.8328472926021133, 'eval_precision': 0.8402173913043478, 'eval_recall': 0.8205944798301487, 'eval_runtime': 0.1082, 'eval_samples_per_second': 17472.032, 'eval_steps_per_second': 1099.509, 'epoch': 14.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 7075/23650 [00:25<00:49, 336.54it/s]\n",
      " 30%|███       | 7109/23650 [00:25<01:19, 207.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.44370487332344055, 'eval_accuracy': 0.8365943945002644, 'eval_f1': 0.8344938403856453, 'eval_auc': 0.8365588763677936, 'eval_precision': 0.8421621621621621, 'eval_recall': 0.826963906581741, 'eval_runtime': 0.1039, 'eval_samples_per_second': 18204.545, 'eval_steps_per_second': 1145.606, 'epoch': 15.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 7554/23650 [00:26<00:47, 340.83it/s]\n",
      " 32%|███▏      | 7589/23650 [00:27<01:17, 208.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.45774608850479126, 'eval_accuracy': 0.8323638286620836, 'eval_f1': 0.831651619755709, 'eval_auc': 0.8323595739397153, 'eval_precision': 0.8320935175345378, 'eval_recall': 0.8312101910828026, 'eval_runtime': 0.1092, 'eval_samples_per_second': 17321.01, 'eval_steps_per_second': 1090.005, 'epoch': 16.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 8024/23650 [00:28<00:48, 320.72it/s]\n",
      " 34%|███▍      | 8041/23650 [00:28<00:55, 279.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4620455503463745, 'eval_accuracy': 0.8318350079323109, 'eval_f1': 0.8312101910828026, 'eval_auc': 0.8318327035498312, 'eval_precision': 0.8312101910828026, 'eval_recall': 0.8312101910828026, 'eval_runtime': 0.1044, 'eval_samples_per_second': 18117.302, 'eval_steps_per_second': 1140.116, 'epoch': 17.0}\n",
      "{'train_runtime': 28.7852, 'train_samples_per_second': 13136.946, 'train_steps_per_second': 821.602, 'train_loss': 0.3387091230684927, 'epoch': 17.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [00:00<00:00, 1092.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.4040926694869995, 'eval_accuracy': 0.8254891591750396, 'eval_f1': 0.8252118644067797, 'eval_auc': 0.8254945981802276, 'eval_precision': 0.8234672304439746, 'eval_recall': 0.826963906581741, 'eval_runtime': 0.1119, 'eval_samples_per_second': 16892.273, 'eval_steps_per_second': 1063.025, 'epoch': 17.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import EarlyStoppingCallback\n",
    "\n",
    "model = CNN1D(input_dim=200, num_classes=2)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=EPOCHS,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    metric_for_best_model=METRIC_FOR_BEST_MODEL,\n",
    "    greater_is_better=GREATER_IS_BETTER,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=15000,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    push_to_hub=False,\n",
    "    logging_first_step=False,\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"wandb\"\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated 🤗 Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=traindata,             # training dataset\n",
    "    eval_dataset=testdata, \n",
    "    compute_metrics=compute_metrics,# training dataset\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=PATIENCE)]\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(trainer.evaluate())\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "print(trainer.evaluate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 250/250 [00:00<00:00, 1069.73it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.6834174990653992,\n",
       " 'eval_accuracy': 0.522,\n",
       " 'eval_f1': 0.0020876826722338207,\n",
       " 'eval_auc': 0.5005224660397074,\n",
       " 'eval_precision': 1.0,\n",
       " 'eval_recall': 0.0010449320794148381,\n",
       " 'eval_runtime': 0.2357,\n",
       " 'eval_samples_per_second': 8485.118,\n",
       " 'eval_steps_per_second': 1060.64,\n",
       " 'epoch': 9.0}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first CNN\n",
    "# 'eval_loss': 0.4162973463535309, 'eval_accuracy': 0.8286620835536753, 'eval_f1': 0.8205980066445182\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
