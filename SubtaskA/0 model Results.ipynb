{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using 4 branching models\n",
    "\n",
    "\n",
    "SubtaskA/subtaskA_train_monolingual.jsonl\n",
    "\n",
    "1k sample\n",
    "\n",
    "ROBERTA\n",
    "\n",
    "roberta-base-openai-detector: \n",
    "'eval_loss': 0.4055726230144501,\n",
    "'eval_accuracy': 0.91,\n",
    "'eval_f1': 0.8977272727272727,\n",
    "'eval_auc': 0.9229166666666667,\n",
    "'eval_precision': 0.8229166666666666,\n",
    "'eval_recall': 0.9875,\n",
    "\n",
    "roberta-base:\n",
    "'eval_loss': 0.10473192483186722,\n",
    "'eval_accuracy': 0.975,\n",
    "'eval_f1': 0.9685534591194969,\n",
    "'eval_auc': 0.9729166666666667,\n",
    "'eval_precision': 0.9746835443037974,\n",
    "'eval_recall': 0.9625,\n",
    "\n",
    "Hello-SimpleAI/chatgpt-detector-roberta:\n",
    "'eval_loss': 0.35624417662620544,\n",
    "'eval_accuracy': 0.895,\n",
    "'eval_f1': 0.8864864864864864,\n",
    "'eval_auc': 0.8947262479871175,\n",
    "'eval_precision': 0.8817204301075269,\n",
    "'eval_recall': 0.8913043478260869,\n",
    "\n",
    "roberta-large:\n",
    "(slow training)\n",
    "'eval_loss': 0.11909046024084091,\n",
    "'eval_accuracy': 0.98,\n",
    "'eval_f1': 0.978021978021978,\n",
    "'eval_auc': 0.9790660225442834,\n",
    "'eval_precision': 0.9888888888888889,\n",
    "'eval_recall': 0.967391304347826,\n",
    "\n",
    "PlanTL-GOB-ES/roberta-large-bne:\n",
    "'eval_loss': 0.2164173275232315,\n",
    "'eval_accuracy': 0.945,\n",
    "'eval_f1': 0.9424083769633508,\n",
    "'eval_auc': 0.947463768115942,\n",
    "'eval_precision': 0.9090909090909091,\n",
    "'eval_recall': 0.9782608695652174,\n",
    "\n",
    "\n",
    "BERT\n",
    "\n",
    "bert-base-uncased:\n",
    "'eval_loss': 0.5306625366210938,\n",
    "'eval_accuracy': 0.78,\n",
    "'eval_f1': 0.676470588235294,\n",
    "'eval_auc': 0.7458333333333332,\n",
    "'eval_precision': 0.8214285714285714,\n",
    "'eval_recall': 0.575,\n",
    "\n",
    "distilbert-base-uncased:\n",
    "'eval_loss': 0.5169451236724854,\n",
    "'eval_accuracy': 0.895,\n",
    "'eval_f1': 0.8813559322033899,\n",
    "'eval_auc': 0.9083333333333333,\n",
    "'eval_precision': 0.8041237113402062,\n",
    "'eval_recall': 0.975,\n",
    "\n",
    "\n",
    "DEBERTA\n",
    "\n",
    "microsoft/deberta-large:\n",
    "(very slow training)\n",
    "'eval_loss': 0.13007394969463348,\n",
    "'eval_accuracy': 0.97,\n",
    "'eval_f1': 0.967741935483871,\n",
    "'eval_auc': 0.9706119162640902,\n",
    "'eval_precision': 0.9574468085106383,\n",
    "'eval_recall': 0.9782608695652174,\n",
    "\n",
    "microsoft/deberta-v3-base:\n",
    "'eval_loss': 0.31638476252555847,\n",
    "'eval_accuracy': 0.845,\n",
    "'eval_f1': 0.8457711442786071,\n",
    "'eval_auc': 0.8508454106280193,\n",
    "'eval_precision': 0.7798165137614679,\n",
    "'eval_recall': 0.9239130434782609,\n",
    "\n",
    "\n",
    "The best ones are clearly\n",
    "\n",
    "1. roberta-large\n",
    "2. roberta-base\n",
    "3. microsoft/deberta-large\n",
    "\n",
    "Based on training time **roberta-base seems** to be the best option"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "This is for training dataset divided in train and eval\n",
    "\n",
    "When we use training as train and dev for eval metrics drastically drop\n",
    "\n",
    "But when we use dev for train and eval everything is good, i dont know what to make of this\n",
    "\n",
    "The conclusion i arrive is that training dataset is signically different to dev dataset, which makes no sense on concept designing the task\n",
    "\n",
    "Even if you perfectly train on training you will never get close to dev dataset\n",
    "\n",
    "I will continue now using ***training for train and .2 size dev for eval, and then a .2 test training and .2 test dev***, until i ask Jacinto\n",
    "\n",
    "I will use the best 3 models and try to make my own 3 models to build the ensemble, and after that, worry about optimizing models and datasets\n",
    "\n",
    "---\n",
    "my own models have such bad metrics, guess the task is too complex. Will train top 3 LLMs with about 10k entries and build the ensemble with it\n",
    "\n",
    "---\n",
    "the problem still lies in the high difference between training and dev, the more the model trains on training the further the difference with dev dataset. This makes no sense since it happens with LLMs too"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "After using \n",
    "\n",
    "- MODEL1='bert-base-uncased'\n",
    "- MODEL2='microsoft/deberta-large'\n",
    "- MODEL3='roberta-base'\n",
    "- MODEL4='roberta-large'\n",
    "\n",
    "with 5k training samples and making ensemble models majority voting seems to be the better and faster option"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
