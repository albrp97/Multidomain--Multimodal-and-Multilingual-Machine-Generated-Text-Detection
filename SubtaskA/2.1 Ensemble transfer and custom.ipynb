{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ghiki\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from statistics import mode\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline, Trainer\n",
    "from datasets import Dataset, load_metric\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Variables and parameters transfer learning'''\n",
    "\n",
    "SAMPLES_TO_TRAIN = 20000\n",
    "\n",
    "MODEL1='bert-base-uncased'\n",
    "MODEL2='microsoft/deberta-large'\n",
    "MODEL3='roberta-base'\n",
    "MODEL4='roberta-large'\n",
    "MODEL5='Hello-SimpleAI/chatgpt-detector-roberta'\n",
    "MODEL6='roberta-base-openai-detector'\n",
    "\n",
    "MODEL7='Hello-SimpleAI/chatgpt-detector-roberta'\n",
    "MODEL8='Hello-SimpleAI/chatgpt-detector-roberta'\n",
    "\n",
    "MODEL_PATH1='SavedModels/bert-base-uncased20k'\n",
    "MODEL_PATH2='SavedModels/deberta-large5k'\n",
    "MODEL_PATH3='SavedModels/roberta-base20k'\n",
    "MODEL_PATH4='SavedModels/roberta-large5k'\n",
    "MODEL_PATH5='SavedModels/chatgpt-detector-roberta5k'\n",
    "MODEL_PATH6='SavedModels/roberta-base-openai-detector20k'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Load tokenizers and models'''\n",
    "\n",
    "tokenizer1 = AutoTokenizer.from_pretrained(MODEL1)\n",
    "model1 = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH1)\n",
    "\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(MODEL2)\n",
    "model2 = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH2)\n",
    "\n",
    "tokenizer3 = AutoTokenizer.from_pretrained(MODEL3)\n",
    "model3 = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH3)\n",
    "\n",
    "tokenizer4 = AutoTokenizer.from_pretrained(MODEL4)\n",
    "model4 = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH4)\n",
    "\n",
    "tokenizer5 = AutoTokenizer.from_pretrained(MODEL5)\n",
    "model5 = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH5)\n",
    "\n",
    "tokenizer6 = AutoTokenizer.from_pretrained(MODEL6)\n",
    "model6 = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH6)\n",
    "\n",
    "pipe1 = pipeline(\"text-classification\", model=model1, tokenizer=tokenizer1, device=0)\n",
    "pipe2 = pipeline(\"text-classification\", model=model2, tokenizer=tokenizer2, device=0)\n",
    "pipe3 = pipeline(\"text-classification\", model=model3, tokenizer=tokenizer3, device=0)\n",
    "pipe4 = pipeline(\"text-classification\", model=model4, tokenizer=tokenizer4, device=0)\n",
    "pipe5 = pipeline(\"text-classification\", model=model5, tokenizer=tokenizer5, device=0)\n",
    "pipe6 = pipeline(\"text-classification\", model=model6, tokenizer=tokenizer6, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Custom model architectures'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "input_dim = 200\n",
    "\n",
    "# number of classes (unique of y)\n",
    "output_dim = 2\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_dim, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.linear2 = nn.Linear(512, 512)\n",
    "        self.bn2 = nn.BatchNorm1d(512)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.linear3 = nn.Linear(512, 256)\n",
    "        self.bn3 = nn.BatchNorm1d(256)\n",
    "        self.dropout3 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.linear4 = nn.Linear(256, output_dim)\n",
    "        \n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input_ids, labels=None):\n",
    "        x1 = F.leaky_relu(self.linear1(input_ids))\n",
    "        x1 = self.bn1(x1)\n",
    "        x1 = self.dropout1(x1)\n",
    "        \n",
    "        x2 = F.leaky_relu(self.linear2(x1))\n",
    "        x2 = self.bn2(x2)\n",
    "        x2 = self.dropout2(x2)\n",
    "        \n",
    "        # Adding the first skip connection\n",
    "        x2 += x1\n",
    "        \n",
    "        x3 = F.leaky_relu(self.linear3(x2))\n",
    "        x3 = self.bn3(x3)\n",
    "        x3 = self.dropout3(x3)\n",
    "        \n",
    "        x4 = self.linear4(x3)\n",
    "        \n",
    "        outputs = (x4,)\n",
    "        if labels is not None:\n",
    "            loss = self.loss(x4, labels)\n",
    "            outputs = (loss,) + outputs\n",
    "            \n",
    "        return (outputs if len(outputs) > 1 else outputs[0])\n",
    "class CNN1D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN1D, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=100, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn1 = nn.BatchNorm1d(100)\n",
    "        self.conv2 = nn.Conv1d(in_channels=100, out_channels=150, kernel_size=5, stride=1, padding=2)\n",
    "        self.bn2 = nn.BatchNorm1d(150)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        \n",
    "        self.fc1 = nn.Linear(150 * 200, 256)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(256, output_dim)\n",
    "\n",
    "    def forward(self, x, labels=None):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        # Flatten the output for the dense layer\n",
    "        x = torch.flatten(x, 1) \n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(x, labels)\n",
    "            return loss, x\n",
    "        \n",
    "        return x\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNNModel, self).__init__()\n",
    "        \n",
    "        self.lstm1 = nn.LSTM(input_dim, 512, batch_first=True)\n",
    "        self.ln1 = nn.LayerNorm(512)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.lstm2 = nn.LSTM(512, 512, batch_first=True)\n",
    "        self.ln2 = nn.LayerNorm(512)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.fc = nn.Linear(512, output_dim)\n",
    "        \n",
    "        self.loss = nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, input_ids, labels=None):\n",
    "        # print(f\"Input shape: {input_ids.shape}\")\n",
    "        \n",
    "        x, _ = self.lstm1(input_ids)\n",
    "        # print(f\"After LSTM1: {x.shape}\")\n",
    "\n",
    "        x = self.ln1(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        # print(f\"Before LSTM2: {x.shape}\")\n",
    "        \n",
    "        x, _ = self.lstm2(x)\n",
    "        # print(f\"After LSTM2: {x.shape}\")\n",
    "\n",
    "        x = self.ln2(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        # print(f\"Output shape: {x.shape}\")\n",
    "        \n",
    "        outputs = (x,)\n",
    "        if labels is not None:\n",
    "            loss = self.loss(x, labels)\n",
    "            outputs = (loss,) + outputs\n",
    "            \n",
    "        return (outputs if len(outputs) > 1 else outputs[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Custom model preparation'''\n",
    "\n",
    "CUSTOM_MODEL_NAME1='dense'\n",
    "CUSTOM_MODEL_NAME2='cnn'\n",
    "CUSTOM_MODEL_NAME3='lstm'\n",
    "CUSTOM_MODEL_NAME4='random_forest'\n",
    "\n",
    "CUSTOM_MODEL_NAME_PATH_1='./SavedModels/dense_0k'\n",
    "CUSTOM_MODEL_NAME_PATH_2='./SavedModels/cnn_0k'\n",
    "CUSTOM_MODEL_NAME_PATH_3='./SavedModels/lstm_0k'\n",
    "CUSTOM_MODEL_NAME_PATH_4='./SavedModels/randomforest_0k.pkl'\n",
    "\n",
    "custom_model1 = Network()\n",
    "custom_model1.load_state_dict(torch.load(CUSTOM_MODEL_NAME_PATH_1+\"/pytorch_model.bin\"))\n",
    "\n",
    "custom_model2 = CNN1D()\n",
    "custom_model2.load_state_dict(torch.load(CUSTOM_MODEL_NAME_PATH_2+\"/pytorch_model.bin\"))\n",
    "\n",
    "custom_model3 = RNNModel()\n",
    "custom_model3.load_state_dict(torch.load(CUSTOM_MODEL_NAME_PATH_3+\"/pytorch_model.bin\"))\n",
    "\n",
    "with open(CUSTOM_MODEL_NAME_PATH_4, 'rb') as file:\n",
    "    custom_model4 = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20000, 200)\n",
      "(20000,)\n"
     ]
    }
   ],
   "source": [
    "'''Preparing custom data'''\n",
    "\n",
    "with open('datasets/subtaskA_glove_train_dev_monolingual.pkl', 'rb') as f:\n",
    "    loaded_datasets = pickle.load(f)\n",
    "\n",
    "# Accessing loaded datasets\n",
    "loaded_train_x = loaded_datasets['train_x']\n",
    "loaded_train_y = loaded_datasets['train_y']\n",
    "# loaded_dev_x = loaded_datasets['dev_x']\n",
    "# loaded_dev_y = loaded_datasets['dev_y']\n",
    "\n",
    "loaded_dev_x = loaded_train_x\n",
    "loaded_dev_y = loaded_train_y\n",
    "\n",
    "random_indices = np.random.choice(loaded_dev_x.shape[0], size=SAMPLES_TO_TRAIN, replace=False)\n",
    "\n",
    "# Extract the samples from both the array and the series using the same indices\n",
    "loaded_dev_x = loaded_dev_x[random_indices, :]\n",
    "loaded_dev_y = loaded_dev_y.iloc[random_indices]\n",
    "\n",
    "print(loaded_dev_x.shape)\n",
    "print(loaded_dev_y.shape)\n",
    "\n",
    "class Data(Dataset):\n",
    "    def __init__(self, X_train, y_train):\n",
    "        self.X = torch.from_numpy(X_train.astype(np.float32))\n",
    "        self.y = torch.from_numpy(y_train).type(torch.LongTensor)\n",
    "        self.len = self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {'input_ids': self.X[index], 'labels': self.y[index]}\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "class DataCnn(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32).unsqueeze(1)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "        self.len = len(X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return {'x': self.X[index], 'label': self.y[index], 'label_ids': index}\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "denseData=Data(loaded_dev_x, loaded_dev_y.values)\n",
    "cnnData=DataCnn(loaded_dev_x, loaded_dev_y.values)\n",
    "\n",
    "\n",
    "loaded_dev_y=loaded_dev_y.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Getting predictions from custom models'''\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "'''Dense'''\n",
    "\n",
    "# Create a DataLoader\n",
    "data_loader = DataLoader(dataset=denseData, batch_size=32, shuffle=False)  # Adjust batch size as needed\n",
    "\n",
    "# Put the model in evaluation mode\n",
    "custom_model1.eval()\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move model to the appropriate device\n",
    "custom_model1.to(device)\n",
    "\n",
    "# Container to store predictions\n",
    "all_predictions = []\n",
    "\n",
    "# Iterate over the DataLoader to get predictions\n",
    "with torch.no_grad():\n",
    "    for batch in data_loader:\n",
    "        input_data = batch['input_ids'].to(device)  # Moving data to device\n",
    "        labels = batch['labels'].to(device)  # This line is not necessary if you're only doing predictions, but move to device if you use them\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = custom_model1(input_data)\n",
    "\n",
    "        # Get the predicted labels (assuming a classification task here)\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "        # Store predictions\n",
    "        all_predictions.extend(preds.cpu().numpy())  # Moving predictions back to cpu before converting to numpy\n",
    "\n",
    "predictions_dense=all_predictions\n",
    "\n",
    "'''CNN'''\n",
    "custom_model2 = custom_model2.to(device)\n",
    "# Create DataLoader for the CNN data\n",
    "cnn_data_loader = DataLoader(dataset=cnnData, batch_size=32, shuffle=False)  # Adjust batch size as needed\n",
    "\n",
    "# Put the model in evaluation mode\n",
    "custom_model2.eval()\n",
    "\n",
    "# Container to store predictions\n",
    "all_cnn_predictions = []\n",
    "\n",
    "# Iterate over the DataLoader to get predictions\n",
    "with torch.no_grad():\n",
    "    for batch in cnn_data_loader:\n",
    "        input_data = batch['x'].to(device)  # Moving data to device\n",
    "        labels = batch['label'].to(device)  # Move to device if you use them, not necessary for only predictions\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = custom_model2(input_data)\n",
    "\n",
    "        # Get the predicted labels (assuming a classification task here)\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "        # Store predictions\n",
    "        all_cnn_predictions.extend(preds.cpu().numpy())  # Moving predictions back to CPU before converting to numpy\n",
    "predictions_cnn=all_cnn_predictions\n",
    "\n",
    "'''LSTM'''\n",
    "# Create a DataLoader\n",
    "data_loader = DataLoader(dataset=denseData, batch_size=32, shuffle=False)  # Adjust batch size as needed\n",
    "\n",
    "# Put the model in evaluation mode\n",
    "custom_model3.eval()\n",
    "\n",
    "# Check if CUDA is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Move model to the appropriate device\n",
    "custom_model3.to(device)\n",
    "\n",
    "# Container to store predictions\n",
    "all_predictions = []\n",
    "\n",
    "# Iterate over the DataLoader to get predictions\n",
    "with torch.no_grad():\n",
    "    for batch in data_loader:\n",
    "        input_data = batch['input_ids'].to(device)  # Moving data to device\n",
    "        labels = batch['labels'].to(device)  # This line is not necessary if you're only doing predictions, but move to device if you use them\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = custom_model3(input_data)\n",
    "\n",
    "        # Get the predicted labels (assuming a classification task here)\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "        # Store predictions\n",
    "        all_predictions.extend(preds.cpu().numpy())  # Moving predictions back to cpu before converting to numpy\n",
    "\n",
    "predictions_lstm=all_predictions\n",
    "\n",
    "predictions_randomforest=custom_model4.predict(loaded_dev_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Balanced DataFrame:\n",
      "label\n",
      "0    10587\n",
      "1     9413\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "'''Loading data'''\n",
    "\n",
    "import pandas as pd,os\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "df = pd.read_json(os.getcwd()+'/datasets/subtaskA_train_monolingual.jsonl', lines=True)\n",
    "df = df[['text', 'label']]\n",
    "\n",
    "test_df=df.iloc[random_indices]\n",
    "\n",
    "SAMPLES_TO_TRAIN=test_df.shape[0]\n",
    "\n",
    "# Print the balanced DataFrame\n",
    "print(\"\\nBalanced DataFrame:\")\n",
    "print(test_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Getting predictions from transfer models'''\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "test_texts = test_df['text'].tolist()\n",
    "\n",
    "results1 = [pipe1(text, truncation=True, max_length=256) for text in tqdm(test_texts, desc=f\"Processing with {MODEL1}\")]\n",
    "results2 = [pipe2(text, truncation=True, max_length=256) for text in tqdm(test_texts, desc=f\"Processing with {MODEL2}\")]\n",
    "results3 = [pipe3(text, truncation=True, max_length=256) for text in tqdm(test_texts, desc=f\"Processing with {MODEL3}\")]\n",
    "results4 = [pipe4(text, truncation=True, max_length=256) for text in tqdm(test_texts, desc=f\"Processing with {MODEL4}\")]\n",
    "results5 = [pipe5(text, truncation=True, max_length=256) for text in tqdm(test_texts, desc=f\"Processing with {MODEL5}\")]\n",
    "results6 = [pipe6(text, truncation=True, max_length=256) for text in tqdm(test_texts, desc=f\"Processing with {MODEL6}\")]\n",
    "\n",
    "labels1 = [0 if item['label'] == 'LABEL_0' else 1 for d in results1 for item in d]\n",
    "scores1 = [item['score'] for d in results1 for item in d]\n",
    "\n",
    "labels2 = [0 if item['label'] == 'LABEL_0' else 1 for d in results2 for item in d]\n",
    "scores2 = [item['score'] for d in results2 for item in d]\n",
    "\n",
    "labels3 = [0 if item['label'] == 'LABEL_0' else 1 for d in results3 for item in d]\n",
    "scores3 = [item['score'] for d in results3 for item in d]\n",
    "\n",
    "labels4 = [0 if item['label'] == 'LABEL_0' else 1 for d in results4 for item in d]\n",
    "scores4 = [item['score'] for d in results4 for item in d]\n",
    "\n",
    "labels5 = [0 if item['label'] == 'Human' else 1 for d in results5 for item in d]\n",
    "scores5 = [item['score'] for d in results5 for item in d]\n",
    "\n",
    "labels6 = [1 if item['label'] == 'Real' else 0 for d in results6 for item in d]\n",
    "scores6 = [item['score'] for d in results6 for item in d]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Get metrics'''\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "def getMetrics(predicted_labels, true_labels):\n",
    "    # Ensure the labels are numpy arrays\n",
    "    predicted_labels = np.array(predicted_labels)\n",
    "    true_labels = np.array(true_labels)\n",
    "\n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    f1 = f1_score(true_labels, predicted_labels, average='binary')\n",
    "    precision = precision_score(true_labels, predicted_labels, average='binary')\n",
    "    recall = recall_score(true_labels, predicted_labels, average='binary')\n",
    "    auc = roc_auc_score(true_labels, predicted_labels)\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "    # Create a dictionary of metrics\n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'auc': auc,\n",
    "        'confusion_matrix': cm.tolist()  # Convert confusion matrix to a list for JSON serialization\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense\n",
      "{'accuracy': 0.9052, 'f1': 0.8964952505732067, 'precision': 0.9220662549129702, 'recall': 0.8723042600658664, 'auc': 0.9033760839386666, 'confusion_matrix': [[9893, 694], [1202, 8211]]}\n",
      "cnn\n",
      "{'accuracy': 0.8907, 'f1': 0.8807419530823786, 'precision': 0.9052371873948637, 'recall': 0.8575374482099224, 'auc': 0.8888612904599248, 'confusion_matrix': [[9742, 845], [1341, 8072]]}\n",
      "lstm\n",
      "{'accuracy': 0.8852, 'f1': 0.8754205100379816, 'precision': 0.8946434512587335, 'recall': 0.8570062679273346, 'auc': 0.8836367884455791, 'confusion_matrix': [[9637, 950], [1346, 8067]]}\n",
      "random_forest\n",
      "{'accuracy': 0.9589, 'f1': 0.9556873315363882, 'precision': 0.9701214840757361, 'recall': 0.9416764049718475, 'auc': 0.9579450316159889, 'confusion_matrix': [[10314, 273], [549, 8864]]}\n"
     ]
    }
   ],
   "source": [
    "print(CUSTOM_MODEL_NAME1)\n",
    "print(getMetrics(predictions_dense,test_df['label'].tolist()))\n",
    "print(CUSTOM_MODEL_NAME2)\n",
    "print(getMetrics(predictions_cnn,test_df['label'].tolist()))\n",
    "print(CUSTOM_MODEL_NAME3)\n",
    "print(getMetrics(predictions_lstm,test_df['label'].tolist()))\n",
    "print(CUSTOM_MODEL_NAME4)\n",
    "print(getMetrics(predictions_randomforest,test_df['label'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-uncased\n",
      "{'accuracy': 0.95495, 'f1': 0.9521330287414332, 'precision': 0.9522848034006376, 'recall': 0.9519813024540529, 'auc': 0.9547853995032143, 'confusion_matrix': [[10138, 449], [452, 8961]]}\n",
      "microsoft/deberta-large\n",
      "{'accuracy': 0.9884, 'f1': 0.9877559636900992, 'precision': 0.9814368117461982, 'recall': 0.994157016891533, 'auc': 0.9887191998597648, 'confusion_matrix': [[10410, 177], [55, 9358]]}\n",
      "roberta-base\n",
      "{'accuracy': 0.9909, 'f1': 0.9903744446795009, 'precision': 0.9860979462875198, 'recall': 0.9946881971741209, 'auc': 0.9911100379466525, 'confusion_matrix': [[10455, 132], [50, 9363]]}\n",
      "roberta-large\n",
      "{'accuracy': 0.92785, 'f1': 0.9273889196397122, 'precision': 0.8809751434034416, 'recall': 0.9789652608095187, 'auc': 0.9306841039100018, 'confusion_matrix': [[9342, 1245], [198, 9215]]}\n",
      "Hello-SimpleAI/chatgpt-detector-roberta\n",
      "{'accuracy': 0.9785, 'f1': 0.9773708030733607, 'precision': 0.9684012931483992, 'recall': 0.9865080208222671, 'auc': 0.9789440075774697, 'confusion_matrix': [[10284, 303], [127, 9286]]}\n",
      "roberta-base-openai-detector\n",
      "{'accuracy': 0.994, 'f1': 0.9936373276776245, 'precision': 0.9918492643167143, 'recall': 0.995431849569744, 'auc': 0.9940793894113006, 'confusion_matrix': [[10510, 77], [43, 9370]]}\n"
     ]
    }
   ],
   "source": [
    "print(MODEL1)\n",
    "print(getMetrics(labels1,test_df['label'].tolist()))\n",
    "print(MODEL2)\n",
    "print(getMetrics(labels2,test_df['label'].tolist()))\n",
    "print(MODEL3)\n",
    "print(getMetrics(labels3,test_df['label'].tolist()))\n",
    "print(MODEL4)\n",
    "print(getMetrics(labels4,test_df['label'].tolist()))\n",
    "print(MODEL5)\n",
    "print(getMetrics(labels5,test_df['label'].tolist()))\n",
    "print(MODEL6)\n",
    "print(getMetrics(labels6,test_df['label'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16000, 16)\n",
      "(4000, 16)\n",
      "{'accuracy': 0.997, 'f1': 0.9968102073365231, 'precision': 0.9973404255319149, 'recall': 0.9962805526036131, 'auc': 0.9969599174727225, 'confusion_matrix': [[2113, 5], [7, 1875]]}\n"
     ]
    }
   ],
   "source": [
    "'''Complex Ensemble models'''\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "'''Random Forest'''\n",
    "# Create a DataFrame\n",
    "df = pd.DataFrame({\n",
    "    f'Labels_{MODEL1}': labels1,\n",
    "    f'Scores_{MODEL1}': scores1,\n",
    "    f'Labels_{MODEL2}': labels2,\n",
    "    f'Scores_{MODEL2}': scores2,\n",
    "    f'Labels_{MODEL3}': labels3,\n",
    "    f'Scores_{MODEL3}': scores3,\n",
    "    f'Labels_{MODEL4}': labels4,\n",
    "    f'Scores_{MODEL4}': scores4,\n",
    "    f'Labels_{MODEL5}': labels5,\n",
    "    f'Scores_{MODEL5}': scores5,\n",
    "    f'Labels_{MODEL6}': labels6,\n",
    "    f'Scores_{MODEL6}': scores6,\n",
    "    'Prediction_dense':predictions_dense,\n",
    "    'Prediction_cnn':predictions_cnn,\n",
    "    'Prediction_lstm':predictions_lstm,\n",
    "    'Prediction_randomforest':predictions_randomforest,\n",
    "})\n",
    "\n",
    "labels = test_df['label'].tolist()\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "# Create a Random Forest Classifier and train it on the training data\n",
    "clf = RandomForestClassifier(n_estimators=350, random_state=42,min_samples_split=3,min_samples_leaf=1,max_depth=None)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(getMetrics(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.9994, 'f1': 0.9993625159371017, 'precision': 0.9994687068324302, 'recall': 0.9992563476043769, 'auc': 0.9993920351415669, 'confusion_matrix': [[10582, 5], [7, 9406]]}\n"
     ]
    }
   ],
   "source": [
    "finalPrediction = clf.predict(df)\n",
    "print(getMetrics(finalPrediction,labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('SavedModels/'+'ensemble_randomforest_train.pkl', 'wb') as model_file:\n",
    "    pickle.dump(clf, model_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
