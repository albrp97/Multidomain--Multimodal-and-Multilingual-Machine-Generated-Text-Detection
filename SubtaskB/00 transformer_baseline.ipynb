{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ghiki\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding, AutoTokenizer, set_seed\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.special import softmax\n",
    "import argparse\n",
    "import logging\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import wandb\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score, confusion_matrix\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding, AutoTokenizer, set_seed\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.special import softmax\n",
    "import argparse\n",
    "import logging\n",
    "import pandas as pd,os\n",
    "import torch\n",
    "from statistics import mode\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline, Trainer\n",
    "from datasets import Dataset, load_metric\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples, **fn_kwargs):\n",
    "    return fn_kwargs['tokenizer'](examples[\"text\"], truncation=True)\n",
    "\n",
    "def get_data(train_path, test_path, random_seed):\n",
    "    \"\"\"\n",
    "    function to read dataframe with columns\n",
    "    \"\"\"\n",
    "\n",
    "    train_df = pd.read_json(train_path, lines=True)\n",
    "    test_df = pd.read_json(test_path, lines=True)\n",
    "    \n",
    "    train_df, val_df = train_test_split(train_df, test_size=0.2, stratify=train_df['label'], random_state=random_seed)\n",
    "\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "\n",
    "    f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    \n",
    "    results = {}\n",
    "    results.update(f1_metric.compute(predictions=predictions, references = labels, average=\"micro\"))\n",
    "\n",
    "    return results\n",
    "\n",
    "def fine_tune(train_df, valid_df, checkpoints_path, id2label, label2id, model):\n",
    "\n",
    "    # pandas dataframe to huggingface Dataset\n",
    "    train_dataset = Dataset.from_pandas(train_df)\n",
    "    valid_dataset = Dataset.from_pandas(valid_df)\n",
    "    \n",
    "    # get tokenizer and model from huggingface\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)     # put your model here\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "       model, num_labels=len(label2id), id2label=id2label, label2id=label2id    # put your model here\n",
    "    )\n",
    "    \n",
    "    # tokenize data for train/valid\n",
    "    tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True, fn_kwargs={'tokenizer': tokenizer})\n",
    "    tokenized_valid_dataset = valid_dataset.map(preprocess_function, batched=True,  fn_kwargs={'tokenizer': tokenizer})\n",
    "    \n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "    # create Trainer \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=checkpoints_path,\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train_dataset,\n",
    "        eval_dataset=tokenized_valid_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # save best model\n",
    "    best_model_path = checkpoints_path+'/best/'\n",
    "    \n",
    "    if not os.path.exists(best_model_path):\n",
    "        os.makedirs(best_model_path)\n",
    "    \n",
    "\n",
    "    trainer.save_model(best_model_path)\n",
    "\n",
    "def test(test_df, model_path, id2label, label2id):\n",
    "    \n",
    "    # load tokenizer from saved model \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "    # load best model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "       model_path, num_labels=len(label2id), id2label=id2label, label2id=label2id\n",
    "    )\n",
    "            \n",
    "    test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "    tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True,  fn_kwargs={'tokenizer': tokenizer})\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    # create Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    # get logits from predictions and evaluate results using classification report\n",
    "    predictions = trainer.predict(tokenized_test_dataset)\n",
    "    prob_pred = softmax(predictions.predictions, axis=-1)\n",
    "    preds = np.argmax(predictions.predictions, axis=-1)\n",
    "    metric = evaluate.load(\"bstrai/classification_report\")\n",
    "    results = metric.compute(predictions=preds, references=predictions.label_ids)\n",
    "    \n",
    "    # return dictionary of classification report\n",
    "    return results, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-multilingual-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 56821/56821 [00:13<00:00, 4241.23 examples/s]\n",
      "Map: 100%|██████████| 14206/14206 [00:03<00:00, 4108.75 examples/s]\n",
      "  0%|          | 0/10656 [00:00<?, ?it/s]You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "  5%|▍         | 500/10656 [01:55<38:44,  4.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.8958, 'learning_rate': 1.9061561561561564e-05, 'epoch': 0.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 1000/10656 [03:49<36:50,  4.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4968, 'learning_rate': 1.8123123123123126e-05, 'epoch': 0.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 1500/10656 [05:43<35:08,  4.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3964, 'learning_rate': 1.7184684684684688e-05, 'epoch': 0.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 2000/10656 [07:38<33:04,  4.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.333, 'learning_rate': 1.6246246246246247e-05, 'epoch': 0.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 2500/10656 [09:32<31:01,  4.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3216, 'learning_rate': 1.530780780780781e-05, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 3000/10656 [11:26<27:31,  4.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2834, 'learning_rate': 1.4369369369369371e-05, 'epoch': 0.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 3501/10656 [13:15<25:52,  4.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2628, 'learning_rate': 1.3430930930930931e-05, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 33%|███▎      | 3552/10656 [14:53<25:24,  4.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.542670488357544, 'eval_f1': 0.8362663663240884, 'eval_runtime': 87.127, 'eval_samples_per_second': 163.049, 'eval_steps_per_second': 10.192, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 4000/10656 [16:32<24:00,  4.62it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.199, 'learning_rate': 1.2492492492492493e-05, 'epoch': 1.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 42%|████▏     | 4501/10656 [18:21<21:44,  4.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1768, 'learning_rate': 1.1554054054054056e-05, 'epoch': 1.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 5000/10656 [20:09<20:28,  4.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.192, 'learning_rate': 1.0615615615615616e-05, 'epoch': 1.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 5500/10656 [21:59<20:01,  4.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1831, 'learning_rate': 9.677177177177178e-06, 'epoch': 1.55}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▋    | 6000/10656 [23:54<17:48,  4.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1737, 'learning_rate': 8.738738738738739e-06, 'epoch': 1.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 6500/10656 [25:48<15:51,  4.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1624, 'learning_rate': 7.8003003003003e-06, 'epoch': 1.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 7000/10656 [27:47<14:57,  4.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.159, 'learning_rate': 6.861861861861863e-06, 'epoch': 1.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                    \n",
      " 67%|██████▋   | 7104/10656 [29:48<14:33,  4.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3835514783859253, 'eval_f1': 0.9067295508939884, 'eval_runtime': 95.3078, 'eval_samples_per_second': 149.054, 'eval_steps_per_second': 9.317, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 7500/10656 [31:27<12:41,  4.14it/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1147, 'learning_rate': 5.923423423423423e-06, 'epoch': 2.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 8000/10656 [33:25<10:10,  4.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0936, 'learning_rate': 4.984984984984985e-06, 'epoch': 2.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|███████▉  | 8500/10656 [35:20<08:17,  4.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0969, 'learning_rate': 4.046546546546547e-06, 'epoch': 2.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 9000/10656 [37:16<06:19,  4.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0863, 'learning_rate': 3.1081081081081082e-06, 'epoch': 2.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 9500/10656 [39:11<04:24,  4.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0853, 'learning_rate': 2.16966966966967e-06, 'epoch': 2.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 10000/10656 [41:06<02:31,  4.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0734, 'learning_rate': 1.2312312312312314e-06, 'epoch': 2.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▊| 10500/10656 [43:05<00:38,  4.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0639, 'learning_rate': 2.927927927927928e-07, 'epoch': 2.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \n",
      "100%|██████████| 10656/10656 [45:18<00:00,  4.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5494930744171143, 'eval_f1': 0.8980712375052795, 'eval_runtime': 95.575, 'eval_samples_per_second': 148.637, 'eval_steps_per_second': 9.291, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10656/10656 [45:20<00:00,  3.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 2720.8119, 'train_samples_per_second': 62.652, 'train_steps_per_second': 3.916, 'train_loss': 0.2286621295475029, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3000/3000 [00:00<00:00, 5522.50 examples/s]\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████| 375/375 [00:16<00:00, 22.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6076907158380385\n"
     ]
    }
   ],
   "source": [
    "random_seed = 0\n",
    "model = 'roberta-base'\n",
    "model = 'google/electra-base-discriminator'\n",
    "model = 'bert-base-uncased'\n",
    "model = 'distilbert-base-uncased'\n",
    "model = 'bert-base-multilingual-cased'\n",
    "\n",
    "df= pd.read_json('datasets/subtaskB_train.jsonl', lines=True)\n",
    "\n",
    "# df=df.sample(20000)\n",
    "\n",
    "df.to_json('reducedTrainDataFrame.jsonl', orient='records', lines=True)\n",
    "\n",
    "train_path =  'reducedTrainDataFrame.jsonl'\n",
    "test_path =  'datasets/subtaskB_dev.jsonl'\n",
    "\n",
    "subtask =  'B'\n",
    "prediction_path = 'reducedPredictedDataFrame.jsonl'\n",
    "\n",
    "if not os.path.exists(train_path):\n",
    "    logging.error(\"File doesnt exists: {}\".format(train_path))\n",
    "    raise ValueError(\"File doesnt exists: {}\".format(train_path))\n",
    "\n",
    "if not os.path.exists(test_path):\n",
    "    logging.error(\"File doesnt exists: {}\".format(train_path))\n",
    "    raise ValueError(\"File doesnt exists: {}\".format(train_path))\n",
    "\n",
    "if subtask == 'A':\n",
    "    id2label = {0: \"human\", 1: \"machine\"}\n",
    "    label2id = {\"human\": 0, \"machine\": 1}\n",
    "elif subtask == 'B':\n",
    "    id2label = {0: 'human', 1: 'chatGPT', 2: 'cohere', 3: 'davinci', 4: 'bloomz', 5: 'dolly'}\n",
    "    label2id = {'human': 0, 'chatGPT': 1,'cohere': 2, 'davinci': 3, 'bloomz': 4, 'dolly': 5}\n",
    "else:\n",
    "    logging.error(\"Wrong subtask: {}. It should be A or B\".format(train_path))\n",
    "    raise ValueError(\"Wrong subtask: {}. It should be A or B\".format(train_path))\n",
    "\n",
    "set_seed(random_seed)\n",
    "\n",
    "train_df, valid_df, test_df = get_data(train_path, test_path, random_seed)\n",
    "\n",
    "fine_tune(train_df, valid_df, f\"{model}/subtask{subtask}/{random_seed}\", id2label, label2id, model)\n",
    "\n",
    "results, predictions = test(test_df, f\"{model}/subtask{subtask}/{random_seed}/best/\", id2label, label2id)\n",
    "print(results['macro avg']['f1-score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3000/3000 [00:00<00:00, 6045.18 examples/s]\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████| 375/375 [00:08<00:00, 44.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5754180918344265\n"
     ]
    }
   ],
   "source": [
    "results, predictions = test(test_df, f\"{model}/subtask{subtask}/{random_seed}/best/\", id2label, label2id)\n",
    "print(results['macro avg']['f1-score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'0': {'precision': 0.9753086419753086,\n",
       "   'recall': 0.316,\n",
       "   'f1-score': 0.4773413897280967,\n",
       "   'support': 500.0},\n",
       "  '1': {'precision': 0.8973105134474327,\n",
       "   'recall': 0.734,\n",
       "   'f1-score': 0.8074807480748075,\n",
       "   'support': 500.0},\n",
       "  '2': {'precision': 0.4484848484848485,\n",
       "   'recall': 0.592,\n",
       "   'f1-score': 0.5103448275862069,\n",
       "   'support': 500.0},\n",
       "  '3': {'precision': 0.18666666666666668,\n",
       "   'recall': 0.056,\n",
       "   'f1-score': 0.08615384615384615,\n",
       "   'support': 500.0},\n",
       "  '4': {'precision': 0.9191176470588235,\n",
       "   'recall': 1.0,\n",
       "   'f1-score': 0.9578544061302682,\n",
       "   'support': 500.0},\n",
       "  '5': {'precision': 0.4493023255813953,\n",
       "   'recall': 0.966,\n",
       "   'f1-score': 0.6133333333333333,\n",
       "   'support': 500.0},\n",
       "  'accuracy': 0.6106666666666667,\n",
       "  'macro avg': {'precision': 0.6460317738690792,\n",
       "   'recall': 0.6106666666666666,\n",
       "   'f1-score': 0.5754180918344265,\n",
       "   'support': 3000.0},\n",
       "  'weighted avg': {'precision': 0.6460317738690792,\n",
       "   'recall': 0.6106666666666667,\n",
       "   'f1-score': 0.5754180918344265,\n",
       "   'support': 3000.0}},\n",
       " array([1, 1, 1, ..., 5, 5, 5], dtype=int64))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results, predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
