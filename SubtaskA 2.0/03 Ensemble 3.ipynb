{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "import pandas as pd,os\n",
    "import torch\n",
    "from statistics import mode\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline, Trainer\n",
    "from datasets import Dataset, load_metric\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL1 = 'bert-base-uncased'\n",
    "MODEL2 = 'Hello-SimpleAI/chatgpt-detector-roberta'\n",
    "MODEL3 = 'roberta-base-openai-detector'\n",
    "MODEL4 = 'roberta-base'\n",
    "MODEL5 = 'distilbert-base-uncased'\n",
    "MODEL6 = 'google/electra-base-discriminator'\n",
    "\n",
    "MODEL_PATH1='SavedModels/bert-base-uncased12k'\n",
    "MODEL_PATH2='SavedModels/chatgpt-detector-roberta12k'\n",
    "MODEL_PATH3='SavedModels/roberta-base-openai-detector12k'\n",
    "MODEL_PATH4='SavedModels/roberta-base1k'\n",
    "MODEL_PATH5='SavedModels/distilbert-base-uncased15k'\n",
    "MODEL_PATH6='SavedModels/electra-base-discriminator9k'\n",
    "\n",
    "'''Load tokenizers and models'''\n",
    "\n",
    "tokenizer1 = AutoTokenizer.from_pretrained(MODEL1)\n",
    "model1 = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH1)\n",
    "\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(MODEL2)\n",
    "model2 = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH2)\n",
    "\n",
    "tokenizer3 = AutoTokenizer.from_pretrained(MODEL3)\n",
    "model3 = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH3)\n",
    "\n",
    "tokenizer4 = AutoTokenizer.from_pretrained(MODEL4)\n",
    "model4 = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH4)\n",
    "\n",
    "tokenizer5 = AutoTokenizer.from_pretrained(MODEL5)\n",
    "model5 = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH5)\n",
    "\n",
    "tokenizer6 = AutoTokenizer.from_pretrained(MODEL6)\n",
    "model6 = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH6)\n",
    "\n",
    "pipe1 = pipeline(\"text-classification\", model=model1, tokenizer=tokenizer1, device=0)\n",
    "pipe2 = pipeline(\"text-classification\", model=model2, tokenizer=tokenizer2, device=0)\n",
    "pipe3 = pipeline(\"text-classification\", model=model3, tokenizer=tokenizer3, device=0)\n",
    "pipe4 = pipeline(\"text-classification\", model=model4, tokenizer=tokenizer4, device=0)\n",
    "pipe5 = pipeline(\"text-classification\", model=model5, tokenizer=tokenizer5, device=0)\n",
    "pipe6 = pipeline(\"text-classification\", model=model6, tokenizer=tokenizer6, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    5000 non-null   object\n",
      " 1   label   5000 non-null   int64 \n",
      " 2   model   5000 non-null   object\n",
      " 3   source  5000 non-null   object\n",
      " 4   id      5000 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 195.4+ KB\n",
      "None\n",
      "\n",
      "label\n",
      "1    2500\n",
      "0    2500\n",
      "Name: count, dtype: int64\n",
      "\n",
      "model\n",
      "bloomz    2500\n",
      "human     2500\n",
      "Name: count, dtype: int64\n",
      "\n",
      "source\n",
      "wikihow      1000\n",
      "wikipedia    1000\n",
      "reddit       1000\n",
      "arxiv        1000\n",
      "peerread     1000\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing with bert-base-uncased\t\t\t:   0%|          | 10/5000 [00:00<00:53, 93.51it/s]c:\\Users\\Ghiki\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pipelines\\base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "Processing with bert-base-uncased\t\t\t: 100%|██████████| 5000/5000 [00:42<00:00, 117.89it/s]\n",
      "Processing with Hello-SimpleAI/chatgpt-detector-roberta\t: 100%|██████████| 5000/5000 [00:43<00:00, 115.35it/s]\n",
      "Processing with roberta-base-openai-detector\t\t: 100%|██████████| 5000/5000 [00:43<00:00, 113.89it/s]\n",
      "Processing with roberta-base\t\t\t\t: 100%|██████████| 5000/5000 [00:42<00:00, 117.40it/s]\n",
      "Processing with distilbert-base-uncased\t\t: 100%|██████████| 5000/5000 [00:26<00:00, 191.02it/s]\n",
      "Processing with google/electra-base-discriminator\t\t: 100%|██████████| 5000/5000 [00:41<00:00, 119.16it/s]\n"
     ]
    }
   ],
   "source": [
    "'''Preparing data'''\n",
    "\n",
    "df = pd.read_json('datasets/subtaskA_dev_monolingual.jsonl', lines=True)\n",
    "\n",
    "print(f'Original dataset')\n",
    "print(df.info())\n",
    "print(f'''\\n{df['label'].value_counts()}''')\n",
    "print(f'''\\n{df['model'].value_counts()}''')\n",
    "print(f'''\\n{df['source'].value_counts()}''')\n",
    "\n",
    "df = df[['text', 'label']]\n",
    "test_df=df\n",
    "\n",
    "test_texts = test_df['text'].tolist()\n",
    "\n",
    "results1 = [pipe1(text, truncation=True, max_length=256) for text in tqdm(test_texts, desc=f\"Processing with {MODEL1}\\t\\t\\t\")]\n",
    "results2 = [pipe2(text, truncation=True, max_length=256) for text in tqdm(test_texts, desc=f\"Processing with {MODEL2}\\t\")]\n",
    "results3 = [pipe3(text, truncation=True, max_length=256) for text in tqdm(test_texts, desc=f\"Processing with {MODEL3}\\t\\t\")]\n",
    "results4 = [pipe4(text, truncation=True, max_length=256) for text in tqdm(test_texts, desc=f\"Processing with {MODEL4}\\t\\t\\t\\t\")]\n",
    "results5 = [pipe5(text, truncation=True, max_length=256) for text in tqdm(test_texts, desc=f\"Processing with {MODEL5}\\t\\t\\t\")]\n",
    "results6 = [pipe6(text, truncation=True, max_length=256) for text in tqdm(test_texts, desc=f\"Processing with {MODEL6}\\t\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-uncased\n",
      "{'f1': 0.6305998782778969, 'confusion_matrix': [[2215, 285], [1460, 1040]], 'accuracy': 0.651, 'precision': 0.7849056603773585, 'recall': 0.416, 'auc': 0.6509999999999999}\n",
      "Hello-SimpleAI/chatgpt-detector-roberta\n",
      "{'f1': 0.6448845285349909, 'confusion_matrix': [[2298, 202], [1461, 1039]], 'accuracy': 0.6674, 'precision': 0.8372280419016922, 'recall': 0.4156, 'auc': 0.6674}\n",
      "roberta-base-openai-detector\n",
      "{'f1': 0.7094461014998555, 'confusion_matrix': [[2320, 180], [1211, 1289]], 'accuracy': 0.7218, 'precision': 0.8774676650782846, 'recall': 0.5156, 'auc': 0.7218}\n",
      "roberta-base\n",
      "{'f1': 0.8025985097631888, 'confusion_matrix': [[2146, 354], [630, 1870]], 'accuracy': 0.8032, 'precision': 0.8408273381294964, 'recall': 0.748, 'auc': 0.8032}\n",
      "distilbert-base-uncased\n",
      "{'f1': 0.6843230995648157, 'confusion_matrix': [[1873, 627], [945, 1555]], 'accuracy': 0.6856, 'precision': 0.7126489459211732, 'recall': 0.622, 'auc': 0.6856}\n",
      "google/electra-base-discriminator\n",
      "{'f1': 0.7379069127755429, 'confusion_matrix': [[2206, 294], [991, 1509]], 'accuracy': 0.743, 'precision': 0.8369384359400999, 'recall': 0.6036, 'auc': 0.7430000000000001}\n"
     ]
    }
   ],
   "source": [
    "def getMetrics(predicted_labels, true_labels):\n",
    "    # Ensure the labels are numpy arrays\n",
    "    predicted_labels = np.array(predicted_labels)\n",
    "    true_labels = np.array(true_labels)\n",
    "\n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    f1 = precision_recall_fscore_support(true_labels, predicted_labels, average='macro')[2]\n",
    "    precision = precision_score(true_labels, predicted_labels, average='binary')\n",
    "    recall = recall_score(true_labels, predicted_labels, average='binary')\n",
    "    auc = roc_auc_score(true_labels, predicted_labels)\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "    # Create a dictionary of metrics\n",
    "    metrics = {\n",
    "        'f1': f1,\n",
    "        'confusion_matrix': cm.tolist(),\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'auc': auc,\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "labels1 = [0 if item['label'] == 'human' else 1 for d in results1 for item in d]\n",
    "scores1 = [item['score'] for d in results1 for item in d]\n",
    "\n",
    "labels2 = [0 if item['label'] == 'human' else 1 for d in results2 for item in d]\n",
    "scores2 = [item['score'] for d in results2 for item in d]\n",
    "\n",
    "labels3 = [0 if item['label'] == 'human' else 1 for d in results3 for item in d]\n",
    "scores3 = [item['score'] for d in results3 for item in d]\n",
    "\n",
    "labels4 = [0 if item['label'] == 'human' else 1 for d in results4 for item in d]\n",
    "scores4 = [item['score'] for d in results4 for item in d]\n",
    "\n",
    "labels5 = [0 if item['label'] == 'human' else 1 for d in results5 for item in d]\n",
    "scores5 = [item['score'] for d in results5 for item in d]\n",
    "\n",
    "labels6 = [0 if item['label'] == 'human' else 1 for d in results6 for item in d]\n",
    "scores6 = [item['score'] for d in results6 for item in d]\n",
    "\n",
    "print(MODEL1)\n",
    "print(getMetrics(labels1,test_df['label'].tolist()))\n",
    "print(MODEL2)\n",
    "print(getMetrics(labels2,test_df['label'].tolist()))\n",
    "print(MODEL3)\n",
    "print(getMetrics(labels3,test_df['label'].tolist()))\n",
    "print(MODEL4)\n",
    "print(getMetrics(labels4,test_df['label'].tolist()))\n",
    "print(MODEL5)\n",
    "print(getMetrics(labels5,test_df['label'].tolist()))\n",
    "print(MODEL6)\n",
    "print(getMetrics(labels6,test_df['label'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 12 columns):\n",
      " #   Column                                          Non-Null Count  Dtype  \n",
      "---  ------                                          --------------  -----  \n",
      " 0   Labels_bert-base-uncased                        5000 non-null   int64  \n",
      " 1   Scores_bert-base-uncased                        5000 non-null   float64\n",
      " 2   Labels_Hello-SimpleAI/chatgpt-detector-roberta  5000 non-null   int64  \n",
      " 3   Scores_Hello-SimpleAI/chatgpt-detector-roberta  5000 non-null   float64\n",
      " 4   Labels_roberta-base-openai-detector             5000 non-null   int64  \n",
      " 5   Scores_roberta-base-openai-detector             5000 non-null   float64\n",
      " 6   Labels_roberta-base                             5000 non-null   int64  \n",
      " 7   Scores_roberta-base                             5000 non-null   float64\n",
      " 8   Labels_distilbert-base-uncased                  5000 non-null   int64  \n",
      " 9   Scores_distilbert-base-uncased                  5000 non-null   float64\n",
      " 10  Labels_google/electra-base-discriminator        5000 non-null   int64  \n",
      " 11  Scores_google/electra-base-discriminator        5000 non-null   float64\n",
      "dtypes: float64(6), int64(6)\n",
      "memory usage: 468.9 KB\n",
      "Majority Voting\n",
      "{'f1': 0.7515386275971341, 'confusion_matrix': [[2225, 275], [945, 1555]], 'accuracy': 0.756, 'precision': 0.8497267759562842, 'recall': 0.622, 'auc': 0.7559999999999999}\n",
      "Soft Voting\n",
      "{'f1': 0.7076293964662497, 'confusion_matrix': [[2224, 276], [1142, 1358]], 'accuracy': 0.7164, 'precision': 0.8310893512851897, 'recall': 0.5432, 'auc': 0.7164}\n",
      "Rank Voting\n",
      "{'f1': 0.6723539881056427, 'confusion_matrix': [[1921, 579], [1045, 1455]], 'accuracy': 0.6752, 'precision': 0.7153392330383481, 'recall': 0.582, 'auc': 0.6751999999999999}\n",
      "Borda Count\n",
      "{'f1': 0.7158153364771056, 'confusion_matrix': [[2211, 289], [1095, 1405]], 'accuracy': 0.7232, 'precision': 0.8293978748524203, 'recall': 0.562, 'auc': 0.7232000000000001}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7515386275971341"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "models = [MODEL1, MODEL2, MODEL3, MODEL4, MODEL5, MODEL6]\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    f'Labels_{MODEL1}': labels1,\n",
    "    f'Scores_{MODEL1}': scores1,\n",
    "    f'Labels_{MODEL2}': labels2,\n",
    "    f'Scores_{MODEL2}': scores2,\n",
    "    f'Labels_{MODEL3}': labels3,\n",
    "    f'Scores_{MODEL3}': scores3,\n",
    "    f'Labels_{MODEL4}': labels4,\n",
    "    f'Scores_{MODEL4}': scores4,\n",
    "    f'Labels_{MODEL5}': labels5,\n",
    "    f'Scores_{MODEL5}': scores5,\n",
    "    f'Labels_{MODEL6}': labels6,\n",
    "    f'Scores_{MODEL6}': scores6,\n",
    "})\n",
    "\n",
    "labels = test_df['label'].tolist()\n",
    "\n",
    "df.info()\n",
    "\n",
    "def ensemble_methods(df, models):\n",
    "    \n",
    "    majority_labels = []\n",
    "    score_based_labels = []\n",
    "    rank_voting_labels = []\n",
    "    borda_count_labels = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        label_counts = {0: 0, 1: 0}\n",
    "        score_sums = {0: 0.0, 1: 0.0}\n",
    "        weighted_scores = {0: 0.0, 1: 0.0}\n",
    "\n",
    "        for i, model in enumerate(models):\n",
    "            label = row[f'Labels_{model}']\n",
    "            score = row[f'Scores_{model}']\n",
    "            label_counts[label] += 1\n",
    "            score_sums[label] += score\n",
    "\n",
    "        # Majority Voting\n",
    "        majority_label = 0 if label_counts[0] > label_counts[1] else 1\n",
    "        majority_labels.append(majority_label)\n",
    "\n",
    "        # Soft Voting\n",
    "        avg_score_0 = score_sums[0] / (label_counts[0] if label_counts[0] else 1)\n",
    "        avg_score_1 = score_sums[1] / (label_counts[1] if label_counts[1] else 1)\n",
    "        score_based_label = 0 if avg_score_0 > avg_score_1 else 1\n",
    "        score_based_labels.append(score_based_label)\n",
    "        \n",
    "        # Rank Voting\n",
    "        ranks = [row[f'Scores_{model}'] for model in models]\n",
    "        ranked_labels = [label for _, label in sorted(zip(ranks, [row[f'Labels_{model}'] for model in models]))]\n",
    "        rank_voting_labels.append(ranked_labels[0])  # The label with the lowest rank\n",
    "\n",
    "        # Borda Count\n",
    "        borda_scores = {0: 0, 1: 0}\n",
    "        for rank, label in enumerate(ranked_labels):\n",
    "            borda_scores[label] += (len(models) - rank)\n",
    "        borda_count_labels.append(max(borda_scores, key=borda_scores.get))\n",
    "        \n",
    "\n",
    "    return {\n",
    "        'Majority Voting':majority_labels,\n",
    "        'Soft Voting':score_based_labels,\n",
    "        'Rank Voting':rank_voting_labels,\n",
    "        'Borda Count':borda_count_labels,\n",
    "    }\n",
    "\n",
    "ensemble_results = ensemble_methods(df, models)\n",
    "    \n",
    "finalScore=getMetrics(ensemble_results['Majority Voting'],labels)['f1']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best F1 Score: 0.840331597415255\n",
      "Best Model Combination: ('roberta-base', 'google/electra-base-discriminator')\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "# Let's define a list of all models you have\n",
    "all_models = [MODEL1, MODEL2, MODEL3, MODEL4, MODEL5, MODEL6]\n",
    "\n",
    "# This will store the best F1 score and the corresponding model combination\n",
    "best_f1_score = 0\n",
    "best_model_combination = None\n",
    "\n",
    "# Try all possible combinations of the models\n",
    "for r in range(1, len(all_models) + 1):\n",
    "    for model_combination in itertools.combinations(all_models, r):\n",
    "        # Generate the predictions using the ensemble of the current combination of models\n",
    "        ensemble_results = ensemble_methods(df, model_combination)\n",
    "        \n",
    "        # Calculate the F1 score for majority voting (you could extend this to other methods)\n",
    "        f1_score = getMetrics(ensemble_results['Majority Voting'], labels)['f1']\n",
    "        \n",
    "        # Update the best combination if the current one is better\n",
    "        if f1_score > best_f1_score:\n",
    "            best_f1_score = f1_score\n",
    "            best_model_combination = model_combination\n",
    "\n",
    "# Print the best combination and its score\n",
    "print(f\"Best F1 Score: {best_f1_score}\")\n",
    "print(f\"Best Model Combination: {best_model_combination}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Majority Voting\n",
      "{'f1': 0.840331597415255, 'confusion_matrix': [[1999, 296], [501, 2204]], 'accuracy': 0.8406, 'precision': 0.8816, 'recall': 0.8147874306839187, 'auc': 0.8429056979127655}\n",
      "Soft Voting\n",
      "{'f1': 0.8217486306432049, 'confusion_matrix': [[2249, 635], [251, 1865]], 'accuracy': 0.8228, 'precision': 0.746, 'recall': 0.8813799621928167, 'auc': 0.8305998285305276}\n",
      "Rank Voting\n",
      "{'f1': 0.7195076525735385, 'confusion_matrix': [[2103, 986], [397, 1514]], 'accuracy': 0.7234, 'precision': 0.6056, 'recall': 0.7922553636839351, 'auc': 0.7365291062511614}\n",
      "Borda Count\n",
      "{'f1': 0.7195076525735385, 'confusion_matrix': [[2103, 986], [397, 1514]], 'accuracy': 0.7234, 'precision': 0.6056, 'recall': 0.7922553636839351, 'auc': 0.7365291062511614}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.840331597415255"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MODEL1 = 'bert-base-uncased'\n",
    "# MODEL2 = 'Hello-SimpleAI/chatgpt-detector-roberta'\n",
    "# MODEL3 = 'roberta-base-openai-detector'\n",
    "# MODEL4 = 'roberta-base'\n",
    "# MODEL5 = 'distilbert-base-uncased'\n",
    "# MODEL6 = 'google/electra-base-discriminator'\n",
    "\n",
    "models = [MODEL4, MODEL6]\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    f'Labels_{MODEL4}': labels4,\n",
    "    f'Scores_{MODEL4}': scores4,\n",
    "    f'Labels_{MODEL6}': labels6,\n",
    "    f'Scores_{MODEL6}': scores6,\n",
    "})\n",
    "\n",
    "ensemble_results = ensemble_methods(df, models)\n",
    "    \n",
    "finalScore=getMetrics(ensemble_results['Majority Voting'],labels)['f1']\n",
    "\n",
    "for name,labelsEnsemble in ensemble_results.items():\n",
    "    print(f'{name}\\n{getMetrics(labels,labelsEnsemble)}')\n",
    "    \n",
    "finalScore"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
