{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ghiki\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding, AutoTokenizer, set_seed\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.special import softmax\n",
    "import argparse\n",
    "import logging\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import wandb\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score, confusion_matrix\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "import pandas as pd,os\n",
    "import torch\n",
    "from statistics import mode\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline, Trainer\n",
    "from datasets import Dataset, load_metric\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   text      5000 non-null   object\n",
      " 1   label     5000 non-null   int64 \n",
      " 2   model     5000 non-null   object\n",
      " 3   language  5000 non-null   object\n",
      " 4   id        5000 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 195.4+ KB\n",
      "None\n",
      "\n",
      "label\n",
      "1    2500\n",
      "0    2500\n",
      "Name: count, dtype: int64\n",
      "\n",
      "model\n",
      "bloomz    2500\n",
      "human     2500\n",
      "Name: count, dtype: int64\n",
      "\n",
      "language\n",
      "english    5000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Processing with bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5000/5000 [00:01<00:00, 4840.12 examples/s]\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████| 625/625 [00:19<00:00, 32.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing with Hello-SimpleAI/chatgpt-detector-roberta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5000/5000 [00:00<00:00, 5378.69 examples/s]\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████| 625/625 [00:19<00:00, 32.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing with roberta-base-openai-detector\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5000/5000 [00:00<00:00, 5071.69 examples/s]\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████| 625/625 [00:19<00:00, 32.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing with roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5000/5000 [00:00<00:00, 5063.38 examples/s]\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████| 625/625 [00:19<00:00, 32.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing with distilbert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5000/5000 [00:00<00:00, 5052.35 examples/s]\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████| 625/625 [00:10<00:00, 61.40it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing with google/electra-base-discriminator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5000/5000 [00:01<00:00, 4339.92 examples/s]\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████| 625/625 [00:20<00:00, 31.19it/s]\n"
     ]
    }
   ],
   "source": [
    "MODEL1 = 'bert-base-uncased'\n",
    "MODEL2 = 'Hello-SimpleAI/chatgpt-detector-roberta'\n",
    "MODEL3 = 'roberta-base-openai-detector'\n",
    "MODEL4 = 'roberta-base'\n",
    "MODEL5 = 'distilbert-base-uncased'\n",
    "MODEL6 = 'google/electra-base-discriminator'\n",
    "\n",
    "MODEL_PATH1='SavedModels/optimized-bert-base-uncased-22.5k'\n",
    "MODEL_PATH2='SavedModels/optimized-chatgpt-detector-roberta-17.5k'\n",
    "MODEL_PATH3='SavedModels/optimized-roberta-base-openai-detector-12k'\n",
    "MODEL_PATH4='SavedModels/optimized-roberta-base-0.5k'\n",
    "MODEL_PATH5='SavedModels/optimized-distilbert-base-uncased-15k'\n",
    "MODEL_PATH6='SavedModels/optimized-electra-base-discriminator-6k'\n",
    "\n",
    "\n",
    "'''Preparing data'''\n",
    "\n",
    "df = pd.read_json('datasets/subtaskA_dev_monolingual.jsonl', lines=True)\n",
    "\n",
    "df = df.rename(columns={'source': 'language'})\n",
    "non_language_sources = ['wikihow', 'wikipedia', 'reddit', 'arxiv', 'peerread']\n",
    "df['language'] = df['language'].replace(non_language_sources, 'english')\n",
    "\n",
    "print(f'Original dataset')\n",
    "print(df.info())\n",
    "\n",
    "print(f'''\\n{df['label'].value_counts()}''')\n",
    "print(f'''\\n{df['model'].value_counts()}''')\n",
    "print(f'''\\n{df['language'].value_counts()}''')\n",
    "\n",
    "df = df[['text', 'label']]\n",
    "test_df=df\n",
    "\n",
    "test_texts = test_df['text'].tolist()\n",
    "\n",
    "def getPrediction(model_path):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    id2label = {0: \"human\", 1: \"machine\"}\n",
    "    label2id = {\"human\": 0, \"machine\": 1}\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_path, num_labels=len(label2id), id2label=id2label, label2id=label2id\n",
    "        )\n",
    "\n",
    "    test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "    def preprocess_function(examples, **fn_kwargs):\n",
    "        return fn_kwargs['tokenizer'](examples[\"text\"], truncation=True)\n",
    "\n",
    "    tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True,  fn_kwargs={'tokenizer': tokenizer})\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    # create Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        # compute_metrics=compute_metrics,\n",
    "    )\n",
    "    # get logits from predictions and evaluate results using classification report\n",
    "    predictions = trainer.predict(tokenized_test_dataset)\n",
    "    preds = np.argmax(predictions.predictions, axis=-1)\n",
    "    probs = softmax(predictions.predictions, axis=-1)\n",
    "    label_specific_probs = probs[:, 1]  # This extracts the probability for label 1\n",
    "    \n",
    "    return list(preds),list(label_specific_probs)\n",
    "\n",
    "print(f'\\nProcessing with {MODEL1}')\n",
    "labels1,scores1=getPrediction(MODEL_PATH1)\n",
    "\n",
    "print(f'\\nProcessing with {MODEL2}')\n",
    "labels2,scores2=getPrediction(MODEL_PATH2)\n",
    "\n",
    "print(f'\\nProcessing with {MODEL3}')\n",
    "labels3,scores3=getPrediction(MODEL_PATH3)\n",
    "\n",
    "print(f'\\nProcessing with {MODEL4}')\n",
    "labels4,scores4=getPrediction(MODEL_PATH4)\n",
    "\n",
    "print(f'\\nProcessing with {MODEL5}')\n",
    "labels5,scores5=getPrediction(MODEL_PATH5)\n",
    "\n",
    "print(f'\\nProcessing with {MODEL6}')\n",
    "labels6,scores6=getPrediction(MODEL_PATH6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-uncased\n",
      "{'f1': 0.7835800547378446, 'confusion_matrix': [[1935, 565], [517, 1983]], 'accuracy': 0.7836, 'precision': 0.7782574568288854, 'recall': 0.7932, 'auc': 0.7836000000000001}\n",
      "Hello-SimpleAI/chatgpt-detector-roberta\n",
      "{'f1': 0.7197234156820624, 'confusion_matrix': [[2372, 128], [1208, 1292]], 'accuracy': 0.7328, 'precision': 0.9098591549295775, 'recall': 0.5168, 'auc': 0.7328}\n",
      "roberta-base-openai-detector\n",
      "{'f1': 0.8644663083985576, 'confusion_matrix': [[2371, 129], [544, 1956]], 'accuracy': 0.8654, 'precision': 0.9381294964028777, 'recall': 0.7824, 'auc': 0.8654}\n",
      "roberta-base\n",
      "{'f1': 0.8417475804685544, 'confusion_matrix': [[2059, 441], [350, 2150]], 'accuracy': 0.8418, 'precision': 0.8297954457738325, 'recall': 0.86, 'auc': 0.8418}\n",
      "distilbert-base-uncased\n",
      "{'f1': 0.7442622033685506, 'confusion_matrix': [[2189, 311], [947, 1553]], 'accuracy': 0.7484, 'precision': 0.8331545064377682, 'recall': 0.6212, 'auc': 0.7484}\n",
      "google/electra-base-discriminator\n",
      "{'f1': 0.777975985882633, 'confusion_matrix': [[1919, 581], [529, 1971]], 'accuracy': 0.778, 'precision': 0.7723354231974922, 'recall': 0.7884, 'auc': 0.778}\n",
      "Features Model\n",
      "{'f1': 0.6909230472951077, 'confusion_matrix': [[1846, 654], [888, 1612]], 'accuracy': 0.6916, 'precision': 0.7113857016769638, 'recall': 0.6448, 'auc': 0.6916}\n"
     ]
    }
   ],
   "source": [
    "def getMetrics(predicted_labels, true_labels):\n",
    "    # Ensure the labels are numpy arrays\n",
    "    predicted_labels = np.array(predicted_labels)\n",
    "    true_labels = np.array(true_labels)\n",
    "\n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    f1 = precision_recall_fscore_support(true_labels, predicted_labels, average='macro')[2]\n",
    "    precision = precision_score(true_labels, predicted_labels, average='binary')\n",
    "    recall = recall_score(true_labels, predicted_labels, average='binary')\n",
    "    auc = roc_auc_score(true_labels, predicted_labels)\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "    # Create a dictionary of metrics\n",
    "    metrics = {\n",
    "        'f1': f1,\n",
    "        'confusion_matrix': cm.tolist(),\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'auc': auc,\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "feature_df=pd.read_csv('datasets/features_df_test.csv')\n",
    "best_features=['word_count', 'avg_sentence_length', 'avg_word_length', 'gunning_fog_index', 'grammatical_errors']\n",
    "feature_df=feature_df[best_features]\n",
    "\n",
    "with open('SavedModels/optimized-rf-features-20k.pkl', 'rb') as file:\n",
    "    randomForest = pickle.load(file)\n",
    "MODEL7='Features Model'\n",
    "labels7=randomForest.predict(feature_df)\n",
    "probabilities = randomForest.predict_proba(feature_df)\n",
    "scores7 = probabilities.max(axis=1)\n",
    "\n",
    "print(MODEL1)\n",
    "print(getMetrics(labels1,test_df['label'].tolist()))\n",
    "print(MODEL2)\n",
    "print(getMetrics(labels2,test_df['label'].tolist()))\n",
    "print(MODEL3)\n",
    "print(getMetrics(labels3,test_df['label'].tolist()))\n",
    "print(MODEL4)\n",
    "print(getMetrics(labels4,test_df['label'].tolist()))\n",
    "print(MODEL5)\n",
    "print(getMetrics(labels5,test_df['label'].tolist()))\n",
    "print(MODEL6)\n",
    "print(getMetrics(labels6,test_df['label'].tolist()))\n",
    "print(MODEL7)\n",
    "print(getMetrics(labels7,test_df['label'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 14 columns):\n",
      " #   Column                                          Non-Null Count  Dtype  \n",
      "---  ------                                          --------------  -----  \n",
      " 0   Labels_bert-base-uncased                        5000 non-null   int64  \n",
      " 1   Scores_bert-base-uncased                        5000 non-null   float32\n",
      " 2   Labels_Hello-SimpleAI/chatgpt-detector-roberta  5000 non-null   int64  \n",
      " 3   Scores_Hello-SimpleAI/chatgpt-detector-roberta  5000 non-null   float32\n",
      " 4   Labels_roberta-base-openai-detector             5000 non-null   int64  \n",
      " 5   Scores_roberta-base-openai-detector             5000 non-null   float32\n",
      " 6   Labels_roberta-base                             5000 non-null   int64  \n",
      " 7   Scores_roberta-base                             5000 non-null   float32\n",
      " 8   Labels_distilbert-base-uncased                  5000 non-null   int64  \n",
      " 9   Scores_distilbert-base-uncased                  5000 non-null   float32\n",
      " 10  Labels_google/electra-base-discriminator        5000 non-null   int64  \n",
      " 11  Scores_google/electra-base-discriminator        5000 non-null   float32\n",
      " 12  Labels_Features Model                           5000 non-null   int64  \n",
      " 13  Scores_Features Model                           5000 non-null   float64\n",
      "dtypes: float32(6), float64(1), int64(7)\n",
      "memory usage: 429.8 KB\n",
      "Majority Voting: 0.8563749618069731\n",
      "Soft Voting: 0.7249660638800509\n",
      "Rank Voting: 0.5485598358494469\n",
      "Borda Count: 0.8077124198445851\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "models = [MODEL1, MODEL2, MODEL3, MODEL4, MODEL5, MODEL6, MODEL7]\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    f'Labels_{MODEL1}': labels1,\n",
    "    f'Scores_{MODEL1}': scores1,\n",
    "    f'Labels_{MODEL2}': labels2,\n",
    "    f'Scores_{MODEL2}': scores2,\n",
    "    f'Labels_{MODEL3}': labels3,\n",
    "    f'Scores_{MODEL3}': scores3,\n",
    "    f'Labels_{MODEL4}': labels4,\n",
    "    f'Scores_{MODEL4}': scores4,\n",
    "    f'Labels_{MODEL5}': labels5,\n",
    "    f'Scores_{MODEL5}': scores5,\n",
    "    f'Labels_{MODEL6}': labels6,\n",
    "    f'Scores_{MODEL6}': scores6,\n",
    "    f'Labels_{MODEL7}': labels7,\n",
    "    f'Scores_{MODEL7}': scores7,\n",
    "})\n",
    "\n",
    "labels = test_df['label'].tolist()\n",
    "\n",
    "df.info()\n",
    "\n",
    "def ensemble_methods(df, models):\n",
    "    \n",
    "    majority_labels = []\n",
    "    score_based_labels = []\n",
    "    rank_voting_labels = []\n",
    "    borda_count_labels = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        label_counts = {0: 0, 1: 0}\n",
    "        score_sums = {0: 0.0, 1: 0.0}\n",
    "        weighted_scores = {0: 0.0, 1: 0.0}\n",
    "\n",
    "        for i, model in enumerate(models):\n",
    "            label = row[f'Labels_{model}']\n",
    "            score = row[f'Scores_{model}']\n",
    "            label_counts[label] += 1\n",
    "            score_sums[label] += score\n",
    "\n",
    "        # Majority Voting\n",
    "        majority_label = 0 if label_counts[0] > label_counts[1] else 1\n",
    "        majority_labels.append(majority_label)\n",
    "\n",
    "        # Soft Voting\n",
    "        avg_score_0 = score_sums[0] / (label_counts[0] if label_counts[0] else 1)\n",
    "        avg_score_1 = score_sums[1] / (label_counts[1] if label_counts[1] else 1)\n",
    "        score_based_label = 0 if avg_score_0 > avg_score_1 else 1\n",
    "        score_based_labels.append(score_based_label)\n",
    "        \n",
    "        # Rank Voting\n",
    "        ranks = [row[f'Scores_{model}'] for model in models]\n",
    "        ranked_labels = [label for _, label in sorted(zip(ranks, [row[f'Labels_{model}'] for model in models]))]\n",
    "        rank_voting_labels.append(ranked_labels[0])  # The label with the lowest rank\n",
    "\n",
    "        # Borda Count\n",
    "        borda_scores = {0: 0, 1: 0}\n",
    "        for rank, label in enumerate(ranked_labels):\n",
    "            borda_scores[label] += (len(models) - rank)\n",
    "        borda_count_labels.append(max(borda_scores, key=borda_scores.get))\n",
    "        \n",
    "    return {\n",
    "        'Majority Voting':majority_labels,\n",
    "        'Soft Voting':score_based_labels,\n",
    "        'Rank Voting':rank_voting_labels,\n",
    "        'Borda Count':borda_count_labels,\n",
    "    }\n",
    "\n",
    "ensemble_results = ensemble_methods(df, models)\n",
    "\n",
    "for voting_method, details in ensemble_results.items():\n",
    "    print(f'''{voting_method}: {getMetrics(details,labels)['f1']}''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best F1 Score: 0.8795395577685059\n",
      "Best Model Combination: ('roberta-base-openai-detector', 'roberta-base', 'google/electra-base-discriminator')\n",
      "Best Voting Method: Majority Voting\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "# Assuming all_models is a list of your model names\n",
    "all_models = [MODEL1, MODEL2, MODEL3, MODEL4, MODEL5, MODEL6, MODEL7]\n",
    "\n",
    "# This will store the best F1 score, the corresponding model combination, and the voting method\n",
    "best_f1_score = 0\n",
    "best_model_combination = None\n",
    "best_voting_method = None\n",
    "\n",
    "# Define the voting methods you want to evaluate\n",
    "voting_methods = ['Majority Voting', 'Soft Voting', 'Rank Voting', 'Borda Count']\n",
    "\n",
    "# Try all possible combinations of the models\n",
    "for r in range(1, len(all_models) + 1):\n",
    "    for model_combination in itertools.combinations(all_models, r):\n",
    "        # Generate the predictions using the ensemble of the current combination of models\n",
    "        ensemble_results = ensemble_methods(df, model_combination)\n",
    "        \n",
    "        # Evaluate each voting method\n",
    "        for method in voting_methods:\n",
    "            f1_score = getMetrics(ensemble_results[method], labels)['f1']\n",
    "            \n",
    "            # Update the best combination if the current one is better\n",
    "            if f1_score > best_f1_score:\n",
    "                best_f1_score = f1_score\n",
    "                best_model_combination = model_combination\n",
    "                best_voting_method = method\n",
    "\n",
    "# Print the best combination, its score, and the voting method\n",
    "print(f\"Best F1 Score: {best_f1_score}\")\n",
    "print(f\"Best Model Combination: {best_model_combination}\")\n",
    "print(f\"Best Voting Method: {best_voting_method}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
