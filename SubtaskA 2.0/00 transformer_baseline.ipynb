{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding, AutoTokenizer, set_seed\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.special import softmax\n",
    "import argparse\n",
    "import logging\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import wandb\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score, confusion_matrix\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "import pandas as pd,os\n",
    "import torch\n",
    "from statistics import mode\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline, Trainer\n",
    "from datasets import Dataset, load_metric\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples, **fn_kwargs):\n",
    "    return fn_kwargs['tokenizer'](examples[\"text\"], truncation=True)\n",
    "\n",
    "\n",
    "def get_data(train_path, test_path, random_seed):\n",
    "    \"\"\"\n",
    "    function to read dataframe with columns\n",
    "    \"\"\"\n",
    "\n",
    "    train_df = pd.read_json(train_path, lines=True)\n",
    "    test_df = pd.read_json(test_path, lines=True)\n",
    "    \n",
    "    train_df, val_df = train_test_split(train_df, test_size=0.2, stratify=train_df['label'], random_state=random_seed)\n",
    "\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    labels = p.label_ids\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    auc = roc_auc_score(labels, preds)\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'auc': auc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'confusion_matrix': cm.tolist()\n",
    "    }\n",
    "\n",
    "\n",
    "def fine_tune(train_df, valid_df, checkpoints_path, id2label, label2id, model):\n",
    "\n",
    "    # pandas dataframe to huggingface Dataset\n",
    "    train_dataset = Dataset.from_pandas(train_df)\n",
    "    valid_dataset = Dataset.from_pandas(valid_df)\n",
    "    \n",
    "    # get tokenizer and model from huggingface\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)     # put your model here\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "       model, num_labels=len(label2id), id2label=id2label, label2id=label2id    # put your model here\n",
    "    )\n",
    "    \n",
    "    # tokenize data for train/valid\n",
    "    tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True, fn_kwargs={'tokenizer': tokenizer})\n",
    "    tokenized_valid_dataset = valid_dataset.map(preprocess_function, batched=True,  fn_kwargs={'tokenizer': tokenizer})\n",
    "    \n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "    # create Trainer \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=checkpoints_path,\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train_dataset,\n",
    "        eval_dataset=tokenized_valid_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "\n",
    "    # save best model\n",
    "    best_model_path = checkpoints_path+'/best/'\n",
    "    \n",
    "    if not os.path.exists(best_model_path):\n",
    "        os.makedirs(best_model_path)\n",
    "    \n",
    "\n",
    "    trainer.save_model(best_model_path)\n",
    "\n",
    "\n",
    "def test(test_df, model_path, id2label, label2id):\n",
    "    \n",
    "    # load tokenizer from saved model \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "    # load best model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "       model_path, num_labels=len(label2id), id2label=id2label, label2id=label2id\n",
    "    )\n",
    "            \n",
    "    test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "    tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True,  fn_kwargs={'tokenizer': tokenizer})\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    # create Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    # get logits from predictions and evaluate results using classification report\n",
    "    predictions = trainer.predict(tokenized_test_dataset)\n",
    "    prob_pred = softmax(predictions.predictions, axis=-1)\n",
    "    preds = np.argmax(predictions.predictions, axis=-1)\n",
    "    metric = evaluate.load(\"bstrai/classification_report\")\n",
    "    results = metric.compute(predictions=preds, references=predictions.label_ids)\n",
    "    \n",
    "    # return dictionary of classification report\n",
    "    return results, preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 23596/23596 [00:06<00:00, 3635.26 examples/s]\n",
      "Map: 100%|██████████| 5899/5899 [00:01<00:00, 3745.12 examples/s]\n",
      "  0%|          | 0/4425 [00:00<?, ?it/s]You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      " 11%|█▏        | 500/4425 [01:57<15:37,  4.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2914, 'learning_rate': 1.7740112994350286e-05, 'epoch': 0.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 1000/4425 [03:55<13:21,  4.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.152, 'learning_rate': 1.5480225988700566e-05, 'epoch': 0.68}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1475/4425 [05:46<10:49,  4.54it/s]Trainer is attempting to log a value of \"[[2665, 288], [17, 2929]]\" of type <class 'list'> for key \"eval/confusion_matrix\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "                                                   \n",
      " 33%|███▎      | 1475/4425 [06:22<10:49,  4.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.24676579236984253, 'eval_accuracy': 0.9482963214104085, 'eval_f1': 0.9505111147168587, 'eval_auc': 0.9483507629945407, 'eval_precision': 0.9104755983835872, 'eval_recall': 0.9942294636795656, 'eval_confusion_matrix': [[2665, 288], [17, 2929]], 'eval_runtime': 35.9069, 'eval_samples_per_second': 164.286, 'eval_steps_per_second': 10.277, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 1500/4425 [06:32<11:37,  4.19it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1116, 'learning_rate': 1.3220338983050848e-05, 'epoch': 1.02}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 2001/4425 [08:29<09:28,  4.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0685, 'learning_rate': 1.096045197740113e-05, 'epoch': 1.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▋    | 2500/4425 [10:26<07:30,  4.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0623, 'learning_rate': 8.700564971751413e-06, 'epoch': 1.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2950/4425 [12:13<05:26,  4.52it/s]Trainer is attempting to log a value of \"[[2783, 170], [17, 2929]]\" of type <class 'list'> for key \"eval/confusion_matrix\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "                                                   \n",
      " 67%|██████▋   | 2950/4425 [12:49<05:26,  4.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.15709815919399261, 'eval_accuracy': 0.968299711815562, 'eval_f1': 0.9690653432588916, 'eval_auc': 0.9683304446741885, 'eval_precision': 0.9451435947079703, 'eval_recall': 0.9942294636795656, 'eval_confusion_matrix': [[2783, 170], [17, 2929]], 'eval_runtime': 36.4753, 'eval_samples_per_second': 161.726, 'eval_steps_per_second': 10.116, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 3000/4425 [13:05<05:32,  4.28it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0596, 'learning_rate': 6.440677966101695e-06, 'epoch': 2.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 3501/4425 [15:03<03:36,  4.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0239, 'learning_rate': 4.180790960451978e-06, 'epoch': 2.37}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 4000/4425 [17:00<01:39,  4.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.0223, 'learning_rate': 1.92090395480226e-06, 'epoch': 2.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4425/4425 [18:41<00:00,  4.52it/s]Trainer is attempting to log a value of \"[[2688, 265], [5, 2941]]\" of type <class 'list'> for key \"eval/confusion_matrix\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "                                                   \n",
      "100%|██████████| 4425/4425 [19:17<00:00,  4.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.3114318549633026, 'eval_accuracy': 0.9542295304288863, 'eval_f1': 0.9561118335500651, 'eval_auc': 0.9542817676065096, 'eval_precision': 0.9173424828446662, 'eval_recall': 0.9983027834351663, 'eval_confusion_matrix': [[2688, 265], [5, 2941]], 'eval_runtime': 36.4737, 'eval_samples_per_second': 161.733, 'eval_steps_per_second': 10.117, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4425/4425 [19:21<00:00,  3.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1161.7878, 'train_samples_per_second': 60.93, 'train_steps_per_second': 3.809, 'train_loss': 0.09160447589421676, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4000/4000 [00:01<00:00, 3382.30 examples/s]\n",
      "You're using a XLMRobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████| 500/500 [00:21<00:00, 22.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7522601995214531\n"
     ]
    }
   ],
   "source": [
    "random_seed = 0\n",
    "\n",
    "df= pd.read_json('../../../SubtaskA/datasets/subtaskA_train_multilingual.jsonl', lines=True)\n",
    "df = df.rename(columns={'source': 'language'})\n",
    "non_language_sources = ['wikihow', 'wikipedia', 'reddit', 'arxiv', 'peerread']\n",
    "df['language'] = df['language'].replace(non_language_sources, 'english')\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_resampled, y_resampled = rus.fit_resample(df.drop('language', axis=1), df['language'])\n",
    "df = pd.concat([X_resampled, y_resampled], axis=1)\n",
    "# df=df.sample(4000)\n",
    "\n",
    "df.to_json('reducedTrainDataFrame.jsonl', orient='records', lines=True)\n",
    "\n",
    "train_path =  'reducedTrainDataFrame.jsonl'\n",
    "test_path =  '../../../SubtaskA/datasets/subtaskA_dev_multilingual.jsonl'\n",
    "\n",
    "model = 'xlm-roberta-base'\n",
    "\n",
    "subtask =  'A'\n",
    "prediction_path = 'reducedPredictedDataFrame.jsonl'\n",
    "\n",
    "if not os.path.exists(train_path):\n",
    "    logging.error(\"File doesnt exists: {}\".format(train_path))\n",
    "    raise ValueError(\"File doesnt exists: {}\".format(train_path))\n",
    "\n",
    "if not os.path.exists(test_path):\n",
    "    logging.error(\"File doesnt exists: {}\".format(train_path))\n",
    "    raise ValueError(\"File doesnt exists: {}\".format(train_path))\n",
    "\n",
    "if subtask == 'A':\n",
    "    id2label = {0: \"human\", 1: \"machine\"}\n",
    "    label2id = {\"human\": 0, \"machine\": 1}\n",
    "elif subtask == 'B':\n",
    "    id2label = {0: 'human', 1: 'chatGPT', 2: 'cohere', 3: 'davinci', 4: 'bloomz', 5: 'dolly'}\n",
    "    label2id = {'human': 0, 'chatGPT': 1,'cohere': 2, 'davinci': 3, 'bloomz': 4, 'dolly': 5}\n",
    "else:\n",
    "    logging.error(\"Wrong subtask: {}. It should be A or B\".format(train_path))\n",
    "    raise ValueError(\"Wrong subtask: {}. It should be A or B\".format(train_path))\n",
    "\n",
    "set_seed(random_seed)\n",
    "\n",
    "train_df, valid_df, test_df = get_data(train_path, test_path, random_seed)\n",
    "\n",
    "fine_tune(train_df, valid_df, f\"{model}/subtask{subtask}/{random_seed}\", id2label, label2id, model)\n",
    "\n",
    "results, predictions = test(test_df, f\"{model}/subtask{subtask}/{random_seed}/best/\", id2label, label2id)\n",
    "print(results['macro avg']['f1-score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'0': {'precision': 0.8186366479049406,\n",
       "   'recall': 0.6545,\n",
       "   'f1-score': 0.7274242845234787,\n",
       "   'support': 2000.0},\n",
       "  '1': {'precision': 0.7122032486463973,\n",
       "   'recall': 0.855,\n",
       "   'f1-score': 0.7770961145194273,\n",
       "   'support': 2000.0},\n",
       "  'accuracy': 0.75475,\n",
       "  'macro avg': {'precision': 0.765419948275669,\n",
       "   'recall': 0.75475,\n",
       "   'f1-score': 0.7522601995214531,\n",
       "   'support': 4000.0},\n",
       "  'weighted avg': {'precision': 0.765419948275669,\n",
       "   'recall': 0.75475,\n",
       "   'f1-score': 0.752260199521453,\n",
       "   'support': 4000.0}},\n",
       " array([0, 0, 1, ..., 0, 1, 1], dtype=int64))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f1': 0.7522601995214531,\n",
       " 'confusion_matrix': [[1309, 691], [290, 1710]],\n",
       " 'accuracy': 0.75475,\n",
       " 'precision': 0.7122032486463973,\n",
       " 'recall': 0.855,\n",
       " 'auc': 0.75475}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getMetrics(predicted_labels, true_labels):\n",
    "    # Ensure the labels are numpy arrays\n",
    "    predicted_labels = np.array(predicted_labels)\n",
    "    true_labels = np.array(true_labels)\n",
    "\n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    f1 = precision_recall_fscore_support(true_labels, predicted_labels, average='macro')[2]\n",
    "    precision = precision_score(true_labels, predicted_labels, average='binary')\n",
    "    recall = recall_score(true_labels, predicted_labels, average='binary')\n",
    "    auc = roc_auc_score(true_labels, predicted_labels)\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "    # Create a dictionary of metrics\n",
    "    metrics = {\n",
    "        'f1': f1,\n",
    "        'confusion_matrix': cm.tolist(),\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'auc': auc,\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "# test_df['label'].tolist()\n",
    "# list(predictions)\n",
    "getMetrics(list(predictions),test_df['label'].tolist())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
