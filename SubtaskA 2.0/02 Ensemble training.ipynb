{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ghiki\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "import pandas as pd,os\n",
    "import torch\n",
    "from statistics import mode\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline, Trainer\n",
    "from datasets import Dataset, load_metric\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Variables and parameters transfer learning'''\n",
    "\n",
    "SAMPLES_TO_TRAIN = 1000\n",
    "\n",
    "MODEL1 = 'bert-base-uncased'\n",
    "MODEL2 = 'Hello-SimpleAI/chatgpt-detector-roberta'\n",
    "MODEL3 = 'roberta-base-openai-detector'\n",
    "MODEL4 = 'roberta-base'\n",
    "MODEL5 = 'distilbert-base-uncased'\n",
    "MODEL6 = 'google/electra-base-discriminator'\n",
    "\n",
    "MODEL_PATH1='SavedModels/bert-base-uncased12k'\n",
    "MODEL_PATH2='SavedModels/chatgpt-detector-roberta12k'\n",
    "MODEL_PATH3='SavedModels/roberta-base-openai-detector12k'\n",
    "MODEL_PATH4='SavedModels/roberta-base1k'\n",
    "MODEL_PATH5='SavedModels/distilbert-base-uncased15k'\n",
    "MODEL_PATH6='SavedModels/electra-base-discriminator9k'\n",
    "\n",
    "'''Load tokenizers and models'''\n",
    "\n",
    "tokenizer1 = AutoTokenizer.from_pretrained(MODEL1)\n",
    "model1 = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH1)\n",
    "\n",
    "tokenizer2 = AutoTokenizer.from_pretrained(MODEL2)\n",
    "model2 = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH2)\n",
    "\n",
    "tokenizer3 = AutoTokenizer.from_pretrained(MODEL3)\n",
    "model3 = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH3)\n",
    "\n",
    "tokenizer4 = AutoTokenizer.from_pretrained(MODEL4)\n",
    "model4 = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH4)\n",
    "\n",
    "tokenizer5 = AutoTokenizer.from_pretrained(MODEL5)\n",
    "model5 = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH5)\n",
    "\n",
    "tokenizer6 = AutoTokenizer.from_pretrained(MODEL6)\n",
    "model6 = AutoModelForSequenceClassification.from_pretrained(MODEL_PATH6)\n",
    "\n",
    "pipe1 = pipeline(\"text-classification\", model=model1, tokenizer=tokenizer1, device=0)\n",
    "pipe2 = pipeline(\"text-classification\", model=model2, tokenizer=tokenizer2, device=0)\n",
    "pipe3 = pipeline(\"text-classification\", model=model3, tokenizer=tokenizer3, device=0)\n",
    "pipe4 = pipeline(\"text-classification\", model=model4, tokenizer=tokenizer4, device=0)\n",
    "pipe5 = pipeline(\"text-classification\", model=model5, tokenizer=tokenizer5, device=0)\n",
    "pipe6 = pipeline(\"text-classification\", model=model6, tokenizer=tokenizer6, device=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing with bert-base-uncased\t\t\t:   0%|          | 1/1000 [00:01<21:49,  1.31s/it]c:\\Users\\Ghiki\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\pipelines\\base.py:1101: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "Processing with bert-base-uncased\t\t\t: 100%|██████████| 1000/1000 [00:09<00:00, 105.96it/s]\n",
      "Processing with Hello-SimpleAI/chatgpt-detector-roberta\t: 100%|██████████| 1000/1000 [00:09<00:00, 107.08it/s]\n",
      "Processing with roberta-base-openai-detector\t\t: 100%|██████████| 1000/1000 [00:09<00:00, 106.69it/s]\n",
      "Processing with roberta-base\t\t\t\t: 100%|██████████| 1000/1000 [00:09<00:00, 107.16it/s]\n",
      "Processing with distilbert-base-uncased\t\t: 100%|██████████| 1000/1000 [00:05<00:00, 179.62it/s]\n",
      "Processing with google/electra-base-discriminator\t: 100%|██████████| 1000/1000 [00:08<00:00, 111.25it/s]\n"
     ]
    }
   ],
   "source": [
    "'''Preparing data'''\n",
    "\n",
    "df = pd.read_json('datasets/subtaskA_train_monolingual.jsonl', lines=True)\n",
    "df = df[['text', 'label']]\n",
    "test_df=df.sample(SAMPLES_TO_TRAIN)\n",
    "\n",
    "test_texts = test_df['text'].tolist()\n",
    "\n",
    "results1 = [pipe1(text, truncation=True, max_length=256) for text in tqdm(test_texts, desc=f\"Processing with {MODEL1}\\t\\t\\t\")]\n",
    "results2 = [pipe2(text, truncation=True, max_length=256) for text in tqdm(test_texts, desc=f\"Processing with {MODEL2}\\t\")]\n",
    "results3 = [pipe3(text, truncation=True, max_length=256) for text in tqdm(test_texts, desc=f\"Processing with {MODEL3}\\t\\t\")]\n",
    "results4 = [pipe4(text, truncation=True, max_length=256) for text in tqdm(test_texts, desc=f\"Processing with {MODEL4}\\t\\t\\t\\t\")]\n",
    "results5 = [pipe5(text, truncation=True, max_length=256) for text in tqdm(test_texts, desc=f\"Processing with {MODEL5}\\t\\t\\t\")]\n",
    "results6 = [pipe6(text, truncation=True, max_length=256) for text in tqdm(test_texts, desc=f\"Processing with {MODEL6}\\t\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-uncased\n",
      "{'f1': 0.9197109594540345, 'confusion_matrix': [[490, 48], [32, 430]], 'accuracy': 0.92, 'precision': 0.899581589958159, 'recall': 0.9307359307359307, 'auc': 0.9207582999404562}\n",
      "Hello-SimpleAI/chatgpt-detector-roberta\n",
      "{'f1': 0.9639168644557059, 'confusion_matrix': [[506, 32], [4, 458]], 'accuracy': 0.964, 'precision': 0.9346938775510204, 'recall': 0.9913419913419913, 'auc': 0.9659312187193229}\n",
      "roberta-base-openai-detector\n",
      "{'f1': 0.9689253898610564, 'confusion_matrix': [[509, 29], [2, 460]], 'accuracy': 0.969, 'precision': 0.9406952965235174, 'recall': 0.9956709956709957, 'auc': 0.9708838249730443}\n",
      "roberta-base\n",
      "{'f1': 0.9469107569824875, 'confusion_matrix': [[494, 44], [9, 453]], 'accuracy': 0.947, 'precision': 0.9114688128772636, 'recall': 0.9805194805194806, 'auc': 0.94936754695119}\n",
      "distilbert-base-uncased\n",
      "{'f1': 0.8729632863897667, 'confusion_matrix': [[428, 110], [17, 445]], 'accuracy': 0.873, 'precision': 0.8018018018018018, 'recall': 0.9632034632034632, 'auc': 0.8793712483303562}\n",
      "google/electra-base-discriminator\n",
      "{'f1': 0.9377450035344772, 'confusion_matrix': [[501, 37], [25, 437]], 'accuracy': 0.938, 'precision': 0.9219409282700421, 'recall': 0.9458874458874459, 'auc': 0.9385571058433513}\n"
     ]
    }
   ],
   "source": [
    "def getMetrics(predicted_labels, true_labels):\n",
    "    # Ensure the labels are numpy arrays\n",
    "    predicted_labels = np.array(predicted_labels)\n",
    "    true_labels = np.array(true_labels)\n",
    "\n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    f1 = precision_recall_fscore_support(true_labels, predicted_labels, average='macro')[2]\n",
    "    precision = precision_score(true_labels, predicted_labels, average='binary')\n",
    "    recall = recall_score(true_labels, predicted_labels, average='binary')\n",
    "    auc = roc_auc_score(true_labels, predicted_labels)\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "    # Create a dictionary of metrics\n",
    "    metrics = {\n",
    "        'f1': f1,\n",
    "        'confusion_matrix': cm.tolist(),\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'auc': auc,\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "labels1 = [0 if item['label'] == 'human' else 1 for d in results1 for item in d]\n",
    "scores1 = [item['score'] for d in results1 for item in d]\n",
    "\n",
    "labels2 = [0 if item['label'] == 'human' else 1 for d in results2 for item in d]\n",
    "scores2 = [item['score'] for d in results2 for item in d]\n",
    "\n",
    "labels3 = [0 if item['label'] == 'human' else 1 for d in results3 for item in d]\n",
    "scores3 = [item['score'] for d in results3 for item in d]\n",
    "\n",
    "labels4 = [0 if item['label'] == 'human' else 1 for d in results4 for item in d]\n",
    "scores4 = [item['score'] for d in results4 for item in d]\n",
    "\n",
    "labels5 = [0 if item['label'] == 'human' else 1 for d in results5 for item in d]\n",
    "scores5 = [item['score'] for d in results5 for item in d]\n",
    "\n",
    "labels6 = [0 if item['label'] == 'human' else 1 for d in results6 for item in d]\n",
    "scores6 = [item['score'] for d in results6 for item in d]\n",
    "\n",
    "print(MODEL1)\n",
    "print(getMetrics(labels1,test_df['label'].tolist()))\n",
    "print(MODEL2)\n",
    "print(getMetrics(labels2,test_df['label'].tolist()))\n",
    "print(MODEL3)\n",
    "print(getMetrics(labels3,test_df['label'].tolist()))\n",
    "print(MODEL4)\n",
    "print(getMetrics(labels4,test_df['label'].tolist()))\n",
    "print(MODEL5)\n",
    "print(getMetrics(labels5,test_df['label'].tolist()))\n",
    "print(MODEL6)\n",
    "print(getMetrics(labels6,test_df['label'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500 entries, 0 to 499\n",
      "Data columns (total 8 columns):\n",
      " #   Column                                          Non-Null Count  Dtype  \n",
      "---  ------                                          --------------  -----  \n",
      " 0   Labels_bert-base-uncased                        500 non-null    int64  \n",
      " 1   Scores_bert-base-uncased                        500 non-null    float64\n",
      " 2   Labels_Hello-SimpleAI/chatgpt-detector-roberta  500 non-null    int64  \n",
      " 3   Scores_Hello-SimpleAI/chatgpt-detector-roberta  500 non-null    float64\n",
      " 4   Labels_roberta-base-openai-detector             500 non-null    int64  \n",
      " 5   Scores_roberta-base-openai-detector             500 non-null    float64\n",
      " 6   Labels_roberta-base                             500 non-null    int64  \n",
      " 7   Scores_roberta-base                             500 non-null    float64\n",
      "dtypes: float64(4), int64(4)\n",
      "memory usage: 31.4 KB\n",
      "{'f1': 0.9879807692307692, 'confusion_matrix': [[257, 4], [2, 237]], 'accuracy': 0.988, 'precision': 0.983402489626556, 'recall': 0.9916317991631799, 'auc': 0.9881530643325478}\n",
      "{'f1': 0.9759861680327868, 'confusion_matrix': [[250, 11], [1, 238]], 'accuracy': 0.976, 'precision': 0.9558232931726908, 'recall': 0.99581589958159, 'auc': 0.976835152855929}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    f'Labels_{MODEL1}': labels1,\n",
    "    f'Scores_{MODEL1}': scores1,\n",
    "    f'Labels_{MODEL2}': labels2,\n",
    "    f'Scores_{MODEL2}': scores2,\n",
    "    f'Labels_{MODEL3}': labels3,\n",
    "    f'Scores_{MODEL3}': scores3,\n",
    "    f'Labels_{MODEL4}': labels4,\n",
    "    f'Scores_{MODEL4}': scores4,\n",
    "})\n",
    "\n",
    "labels = test_df['label'].tolist()\n",
    "\n",
    "df.info()\n",
    "\n",
    "def compute_labels_and_scores(df, models):\n",
    "    majority_labels = []\n",
    "    score_based_labels = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        label_counts = {0: 0, 1: 0}\n",
    "        score_sums = {0: 0.0, 1: 0.0}\n",
    "\n",
    "        # Count labels and sum scores for each label category\n",
    "        for model in models:\n",
    "            label = row[f'Labels_{model}']\n",
    "            score = row[f'Scores_{model}']\n",
    "            label_counts[label] += 1\n",
    "            score_sums[label] += score\n",
    "\n",
    "        # Check for majority label\n",
    "        if label_counts[0] > label_counts[1]:\n",
    "            majority_labels.append(0)\n",
    "        elif label_counts[1] > label_counts[0]:\n",
    "            majority_labels.append(1)\n",
    "        else:  # Tiebreaker case\n",
    "            # Prevent division by zero by adding a small epsilon if count is zero\n",
    "            avg_score_0 = score_sums[0] / (label_counts[0] if label_counts[0] else 1)\n",
    "            avg_score_1 = score_sums[1] / (label_counts[1] if label_counts[1] else 1)\n",
    "            majority_labels.append(0 if avg_score_0 > avg_score_1 else 1)\n",
    "\n",
    "        # For score-based label\n",
    "        # Avoid division by zero by checking if counts are non-zero before dividing\n",
    "        avg_score_0 = score_sums[0] / (label_counts[0] if label_counts[0] else 1)\n",
    "        avg_score_1 = score_sums[1] / (label_counts[1] if label_counts[1] else 1)\n",
    "        score_based_labels.append(0 if avg_score_0 > avg_score_1 else 1)\n",
    "\n",
    "    return majority_labels, score_based_labels\n",
    "\n",
    "# Assuming df is your DataFrame and models is a list of your model names\n",
    "models = [MODEL1, MODEL2, MODEL3, MODEL4]\n",
    "majority_labels, score_based_labels = compute_labels_and_scores(df, models)\n",
    "\n",
    "print(getMetrics(majority_labels,labels))\n",
    "print(getMetrics(score_based_labels,labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20000 entries, 0 to 19999\n",
      "Data columns (total 8 columns):\n",
      " #   Column                                          Non-Null Count  Dtype  \n",
      "---  ------                                          --------------  -----  \n",
      " 0   Labels_bert-base-uncased                        20000 non-null  int64  \n",
      " 1   Scores_bert-base-uncased                        20000 non-null  float64\n",
      " 2   Labels_Hello-SimpleAI/chatgpt-detector-roberta  20000 non-null  int64  \n",
      " 3   Scores_Hello-SimpleAI/chatgpt-detector-roberta  20000 non-null  float64\n",
      " 4   Labels_roberta-base-openai-detector             20000 non-null  int64  \n",
      " 5   Scores_roberta-base-openai-detector             20000 non-null  float64\n",
      " 6   Labels_roberta-base                             20000 non-null  int64  \n",
      " 7   Scores_roberta-base                             20000 non-null  float64\n",
      "dtypes: float64(4), int64(4)\n",
      "memory usage: 1.2 MB\n",
      "(16000, 8)\n",
      "(4000, 8)\n",
      "{'f1': 0.9922046775078228, 'confusion_matrix': [[2137, 26], [5, 1832]], 'accuracy': 0.99225, 'precision': 0.9860064585575888, 'recall': 0.9972781709308656, 'auc': 0.992628914406718}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    f'Labels_{MODEL1}': labels1,\n",
    "    f'Scores_{MODEL1}': scores1,\n",
    "    f'Labels_{MODEL2}': labels2,\n",
    "    f'Scores_{MODEL2}': scores2,\n",
    "    f'Labels_{MODEL3}': labels3,\n",
    "    f'Scores_{MODEL3}': scores3,\n",
    "    f'Labels_{MODEL4}': labels4,\n",
    "    f'Scores_{MODEL4}': scores4,\n",
    "})\n",
    "\n",
    "labels = test_df['label'].tolist()\n",
    "\n",
    "df.info()\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "\n",
    "# 'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 3, 'n_estimators': 250\n",
    "clf = RandomForestClassifier(n_estimators=250, random_state=42,min_samples_split=3,min_samples_leaf=1,max_depth=None,max_features='log2')\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(getMetrics(y_pred,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('SavedModels/'+'ensemble_randomforest_train.pkl', 'wb') as model_file:\n",
    "    pickle.dump(clf, model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 96 candidates, totalling 288 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ghiki\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:425: FitFailedWarning: \n",
      "96 fits failed out of a total of 288.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "25 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Ghiki\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Ghiki\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 1145, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\Ghiki\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 638, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\Ghiki\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'sqrt', 'log2'} or None. Got 'auto' instead.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "71 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\Ghiki\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 729, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\Users\\Ghiki\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 1145, in wrapper\n",
      "    estimator._validate_params()\n",
      "  File \"c:\\Users\\Ghiki\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py\", line 638, in _validate_params\n",
      "    validate_parameter_constraints(\n",
      "  File \"c:\\Users\\Ghiki\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py\", line 95, in validate_parameter_constraints\n",
      "    raise InvalidParameterError(\n",
      "sklearn.utils._param_validation.InvalidParameterError: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\Users\\Ghiki\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:979: UserWarning: One or more of the test scores are non-finite: [       nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan        nan        nan        nan        nan\n",
      "        nan        nan 0.99404892 0.99417414 0.99417414 0.99411147\n",
      " 0.99411156 0.99417418 0.99411156 0.99404896 0.99379845 0.99398635\n",
      " 0.99392367 0.99386098 0.99379845 0.99398635 0.99392367 0.99386098\n",
      " 0.99367276 0.99373545 0.99373544 0.99379806 0.99367276 0.99373545\n",
      " 0.99373544 0.99379806 0.99298394 0.9932974  0.99310933 0.99310951\n",
      " 0.99298394 0.9932974  0.99310933 0.99310951 0.9941741  0.99411134\n",
      " 0.99417389 0.9940486  0.99429938 0.99436193 0.99436193 0.99417403\n",
      " 0.9939862  0.99417404 0.99411136 0.99404875 0.9939862  0.99417404\n",
      " 0.99411136 0.99404875 0.99423672 0.99423672 0.99423666 0.99417404\n",
      " 0.99423672 0.99423672 0.99423666 0.99417404 0.99367288 0.9937981\n",
      " 0.99379809 0.99392339 0.99367288 0.9937981  0.99379809 0.99392339]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters found:  {'max_depth': None, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 3, 'n_estimators': 250}\n",
      "{'f1': 0.9922046775078228, 'confusion_matrix': [[2137, 26], [5, 1832]], 'accuracy': 0.99225, 'precision': 0.9860064585575888, 'recall': 0.9972781709308656, 'auc': 0.992628914406718}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split your data\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the parameter grid\n",
    "\n",
    "# 'max_features': 'log2', 'min_samples_leaf': 2, 'min_samples_split': 2, 'n_estimators': 200}\n",
    "param_grid = {\n",
    "    'n_estimators': [200,250,300,350],\n",
    "    'max_depth': [None],\n",
    "    'min_samples_split': [2, 3],\n",
    "    'min_samples_leaf': [1, 2, 3, 4],\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    # Add more parameters here if you wish\n",
    "}\n",
    "\n",
    "# Create a base model\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# Instantiate the grid search model\n",
    "grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, \n",
    "                           cv=3, n_jobs=-1, verbose=2, scoring='f1_macro')\n",
    "\n",
    "# Fit the grid search to the data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters\n",
    "print(\"Best parameters found: \", grid_search.best_params_)\n",
    "\n",
    "# Use the best estimator to make predictions\n",
    "y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "\n",
    "# Evaluate the best model with your getMetrics function\n",
    "print(getMetrics(y_pred, y_test))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
