{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ghiki\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding, AutoTokenizer, set_seed\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.special import softmax\n",
    "import argparse\n",
    "import logging\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import wandb\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score, confusion_matrix\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "import pandas as pd,os\n",
    "import torch\n",
    "from statistics import mode\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline, Trainer\n",
    "from datasets import Dataset, load_metric\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   text      5000 non-null   object\n",
      " 1   label     5000 non-null   int64 \n",
      " 2   model     5000 non-null   object\n",
      " 3   language  5000 non-null   object\n",
      " 4   id        5000 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 195.4+ KB\n",
      "None\n",
      "\n",
      "label\n",
      "1    2500\n",
      "0    2500\n",
      "Name: count, dtype: int64\n",
      "\n",
      "model\n",
      "bloomz    2500\n",
      "human     2500\n",
      "Name: count, dtype: int64\n",
      "\n",
      "language\n",
      "english    5000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Processing with bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5000/5000 [00:00<00:00, 5462.95 examples/s]\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████| 625/625 [00:24<00:00, 25.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing with Hello-SimpleAI/chatgpt-detector-roberta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5000/5000 [00:01<00:00, 4856.90 examples/s]\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████| 625/625 [00:26<00:00, 23.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing with roberta-base-openai-detector\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5000/5000 [00:00<00:00, 5693.49 examples/s]\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████| 625/625 [00:26<00:00, 23.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing with roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5000/5000 [00:00<00:00, 5380.00 examples/s]\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████| 625/625 [00:26<00:00, 23.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing with distilbert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5000/5000 [00:00<00:00, 5418.15 examples/s]\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████| 625/625 [00:14<00:00, 44.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing with google/electra-base-discriminator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5000/5000 [00:00<00:00, 5022.66 examples/s]\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████| 625/625 [00:26<00:00, 23.51it/s]\n"
     ]
    }
   ],
   "source": [
    "MODEL1 = 'bert-base-uncased'\n",
    "MODEL2 = 'Hello-SimpleAI/chatgpt-detector-roberta'\n",
    "MODEL3 = 'roberta-base-openai-detector'\n",
    "MODEL4 = 'roberta-base'\n",
    "MODEL5 = 'distilbert-base-uncased'\n",
    "MODEL6 = 'google/electra-base-discriminator'\n",
    "\n",
    "MODEL_PATH1='SavedModels/bert-base-uncased12k'\n",
    "MODEL_PATH2='SavedModels/chatgpt-detector-roberta12k'\n",
    "MODEL_PATH3='SavedModels/roberta-base-openai-detector12k'\n",
    "MODEL_PATH4='SavedModels/roberta-base1k'\n",
    "MODEL_PATH5='SavedModels/distilbert-base-uncased15k'\n",
    "MODEL_PATH6='SavedModels/electra-base-discriminator9k'\n",
    "\n",
    "\n",
    "'''Preparing data'''\n",
    "\n",
    "df = pd.read_json('datasets/subtaskA_dev_monolingual.jsonl', lines=True)\n",
    "\n",
    "df = df.rename(columns={'source': 'language'})\n",
    "non_language_sources = ['wikihow', 'wikipedia', 'reddit', 'arxiv', 'peerread']\n",
    "df['language'] = df['language'].replace(non_language_sources, 'english')\n",
    "\n",
    "print(f'Original dataset')\n",
    "print(df.info())\n",
    "\n",
    "print(f'''\\n{df['label'].value_counts()}''')\n",
    "print(f'''\\n{df['model'].value_counts()}''')\n",
    "print(f'''\\n{df['language'].value_counts()}''')\n",
    "\n",
    "df = df[['text', 'label']]\n",
    "test_df=df\n",
    "\n",
    "test_texts = test_df['text'].tolist()\n",
    "\n",
    "def getPrediction(model_path):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    id2label = {0: \"human\", 1: \"machine\"}\n",
    "    label2id = {\"human\": 0, \"machine\": 1}\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_path, num_labels=len(label2id), id2label=id2label, label2id=label2id\n",
    "        )\n",
    "\n",
    "    test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "    def preprocess_function(examples, **fn_kwargs):\n",
    "        return fn_kwargs['tokenizer'](examples[\"text\"], truncation=True)\n",
    "\n",
    "    tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True,  fn_kwargs={'tokenizer': tokenizer})\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    # create Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        # compute_metrics=compute_metrics,\n",
    "    )\n",
    "    # get logits from predictions and evaluate results using classification report\n",
    "    predictions = trainer.predict(tokenized_test_dataset)\n",
    "    preds = np.argmax(predictions.predictions, axis=-1)\n",
    "    probs = softmax(predictions.predictions, axis=-1)\n",
    "    label_specific_probs = probs[:, 1]  # This extracts the probability for label 1\n",
    "    \n",
    "    return list(preds),list(label_specific_probs)\n",
    "\n",
    "print(f'\\nProcessing with {MODEL1}')\n",
    "labels1,scores1=getPrediction(MODEL_PATH1)\n",
    "\n",
    "print(f'\\nProcessing with {MODEL2}')\n",
    "labels2,scores2=getPrediction(MODEL_PATH2)\n",
    "\n",
    "print(f'\\nProcessing with {MODEL3}')\n",
    "labels3,scores3=getPrediction(MODEL_PATH3)\n",
    "\n",
    "print(f'\\nProcessing with {MODEL4}')\n",
    "labels4,scores4=getPrediction(MODEL_PATH4)\n",
    "\n",
    "print(f'\\nProcessing with {MODEL5}')\n",
    "labels5,scores5=getPrediction(MODEL_PATH5)\n",
    "\n",
    "print(f'\\nProcessing with {MODEL6}')\n",
    "labels6,scores6=getPrediction(MODEL_PATH6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-uncased\n",
      "{'f1': 0.6377560090558456, 'confusion_matrix': [[2236, 264], [1446, 1054]], 'accuracy': 0.658, 'precision': 0.7996965098634294, 'recall': 0.4216, 'auc': 0.658}\n",
      "Hello-SimpleAI/chatgpt-detector-roberta\n",
      "{'f1': 0.6664736580980795, 'confusion_matrix': [[2362, 138], [1420, 1080]], 'accuracy': 0.6884, 'precision': 0.8866995073891626, 'recall': 0.432, 'auc': 0.6884}\n",
      "roberta-base-openai-detector\n",
      "{'f1': 0.7271371597426719, 'confusion_matrix': [[2416, 84], [1211, 1289]], 'accuracy': 0.741, 'precision': 0.9388201019664967, 'recall': 0.5156, 'auc': 0.7410000000000001}\n",
      "roberta-base\n",
      "{'f1': 0.8172579207232292, 'confusion_matrix': [[2225, 275], [634, 1866]], 'accuracy': 0.8182, 'precision': 0.8715553479682392, 'recall': 0.7464, 'auc': 0.8181999999999999}\n",
      "distilbert-base-uncased\n",
      "{'f1': 0.7160806014684142, 'confusion_matrix': [[2032, 468], [939, 1561]], 'accuracy': 0.7186, 'precision': 0.7693445046821095, 'recall': 0.6244, 'auc': 0.7186}\n",
      "google/electra-base-discriminator\n",
      "{'f1': 0.7430628080618769, 'confusion_matrix': [[2224, 276], [983, 1517]], 'accuracy': 0.7482, 'precision': 0.8460680423870608, 'recall': 0.6068, 'auc': 0.7482}\n"
     ]
    }
   ],
   "source": [
    "def getMetrics(predicted_labels, true_labels):\n",
    "    # Ensure the labels are numpy arrays\n",
    "    predicted_labels = np.array(predicted_labels)\n",
    "    true_labels = np.array(true_labels)\n",
    "\n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    f1 = precision_recall_fscore_support(true_labels, predicted_labels, average='macro')[2]\n",
    "    precision = precision_score(true_labels, predicted_labels, average='binary')\n",
    "    recall = recall_score(true_labels, predicted_labels, average='binary')\n",
    "    auc = roc_auc_score(true_labels, predicted_labels)\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "    # Create a dictionary of metrics\n",
    "    metrics = {\n",
    "        'f1': f1,\n",
    "        'confusion_matrix': cm.tolist(),\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'auc': auc,\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "print(MODEL1)\n",
    "print(getMetrics(labels1,test_df['label'].tolist()))\n",
    "print(MODEL2)\n",
    "print(getMetrics(labels2,test_df['label'].tolist()))\n",
    "print(MODEL3)\n",
    "print(getMetrics(labels3,test_df['label'].tolist()))\n",
    "print(MODEL4)\n",
    "print(getMetrics(labels4,test_df['label'].tolist()))\n",
    "print(MODEL5)\n",
    "print(getMetrics(labels5,test_df['label'].tolist()))\n",
    "print(MODEL6)\n",
    "print(getMetrics(labels6,test_df['label'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 8 columns):\n",
      " #   Column                                          Non-Null Count  Dtype  \n",
      "---  ------                                          --------------  -----  \n",
      " 0   Labels_bert-base-uncased                        5000 non-null   int64  \n",
      " 1   Scores_bert-base-uncased                        5000 non-null   float32\n",
      " 2   Labels_Hello-SimpleAI/chatgpt-detector-roberta  5000 non-null   int64  \n",
      " 3   Scores_Hello-SimpleAI/chatgpt-detector-roberta  5000 non-null   float32\n",
      " 4   Labels_roberta-base-openai-detector             5000 non-null   int64  \n",
      " 5   Scores_roberta-base-openai-detector             5000 non-null   float32\n",
      " 6   Labels_roberta-base                             5000 non-null   int64  \n",
      " 7   Scores_roberta-base                             5000 non-null   float32\n",
      "dtypes: float32(4), int64(4)\n",
      "memory usage: 234.5 KB\n",
      "Majority Voting: 0.767659177314456\n",
      "Soft Voting: 0.8192682465517362\n",
      "Rank Voting: 0.5547929401046241\n",
      "Borda Count: 0.6695734238403143\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "models = [MODEL1, MODEL2, MODEL3, MODEL4]\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    f'Labels_{MODEL1}': labels1,\n",
    "    f'Scores_{MODEL1}': scores1,\n",
    "    f'Labels_{MODEL2}': labels2,\n",
    "    f'Scores_{MODEL2}': scores2,\n",
    "    f'Labels_{MODEL3}': labels3,\n",
    "    f'Scores_{MODEL3}': scores3,\n",
    "    f'Labels_{MODEL4}': labels4,\n",
    "    f'Scores_{MODEL4}': scores4,\n",
    "})\n",
    "\n",
    "labels = test_df['label'].tolist()\n",
    "\n",
    "df.info()\n",
    "\n",
    "def ensemble_methods(df, models):\n",
    "    \n",
    "    majority_labels = []\n",
    "    score_based_labels = []\n",
    "    rank_voting_labels = []\n",
    "    borda_count_labels = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        label_counts = {0: 0, 1: 0}\n",
    "        score_sums = {0: 0.0, 1: 0.0}\n",
    "        weighted_scores = {0: 0.0, 1: 0.0}\n",
    "\n",
    "        for i, model in enumerate(models):\n",
    "            label = row[f'Labels_{model}']\n",
    "            score = row[f'Scores_{model}']\n",
    "            label_counts[label] += 1\n",
    "            score_sums[label] += score\n",
    "\n",
    "        # Majority Voting\n",
    "        majority_label = 0 if label_counts[0] > label_counts[1] else 1\n",
    "        majority_labels.append(majority_label)\n",
    "\n",
    "        # Soft Voting\n",
    "        avg_score_0 = score_sums[0] / (label_counts[0] if label_counts[0] else 1)\n",
    "        avg_score_1 = score_sums[1] / (label_counts[1] if label_counts[1] else 1)\n",
    "        score_based_label = 0 if avg_score_0 > avg_score_1 else 1\n",
    "        score_based_labels.append(score_based_label)\n",
    "        \n",
    "        # Rank Voting\n",
    "        ranks = [row[f'Scores_{model}'] for model in models]\n",
    "        ranked_labels = [label for _, label in sorted(zip(ranks, [row[f'Labels_{model}'] for model in models]))]\n",
    "        rank_voting_labels.append(ranked_labels[0])  # The label with the lowest rank\n",
    "\n",
    "        # Borda Count\n",
    "        borda_scores = {0: 0, 1: 0}\n",
    "        for rank, label in enumerate(ranked_labels):\n",
    "            borda_scores[label] += (len(models) - rank)\n",
    "        borda_count_labels.append(max(borda_scores, key=borda_scores.get))\n",
    "        \n",
    "    return {\n",
    "        'Majority Voting':majority_labels,\n",
    "        'Soft Voting':score_based_labels,\n",
    "        'Rank Voting':rank_voting_labels,\n",
    "        'Borda Count':borda_count_labels,\n",
    "    }\n",
    "\n",
    "ensemble_results = ensemble_methods(df, models)\n",
    "\n",
    "for voting_method, details in ensemble_results.items():\n",
    "    print(f'''{voting_method}: {getMetrics(details,labels)['f1']}''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best F1 Score: 0.8442979591104827\n",
      "Best Model Combination: ('Hello-SimpleAI/chatgpt-detector-roberta', 'roberta-base-openai-detector', 'roberta-base')\n",
      "Best Voting Method: Soft Voting\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "# Assuming all_models is a list of your model names\n",
    "all_models = [MODEL1, MODEL2, MODEL3, MODEL4]\n",
    "\n",
    "# This will store the best F1 score, the corresponding model combination, and the voting method\n",
    "best_f1_score = 0\n",
    "best_model_combination = None\n",
    "best_voting_method = None\n",
    "\n",
    "# Define the voting methods you want to evaluate\n",
    "voting_methods = ['Majority Voting', 'Soft Voting', 'Rank Voting', 'Borda Count']\n",
    "\n",
    "# Try all possible combinations of the models\n",
    "for r in range(1, len(all_models) + 1):\n",
    "    for model_combination in itertools.combinations(all_models, r):\n",
    "        # Generate the predictions using the ensemble of the current combination of models\n",
    "        ensemble_results = ensemble_methods(df, model_combination)\n",
    "        \n",
    "        # Evaluate each voting method\n",
    "        for method in voting_methods:\n",
    "            f1_score = getMetrics(ensemble_results[method], labels)['f1']\n",
    "            \n",
    "            # Update the best combination if the current one is better\n",
    "            if f1_score > best_f1_score:\n",
    "                best_f1_score = f1_score\n",
    "                best_model_combination = model_combination\n",
    "                best_voting_method = method\n",
    "\n",
    "# Print the best combination, its score, and the voting method\n",
    "print(f\"Best F1 Score: {best_f1_score}\")\n",
    "print(f\"Best Model Combination: {best_model_combination}\")\n",
    "print(f\"Best Voting Method: {best_voting_method}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
