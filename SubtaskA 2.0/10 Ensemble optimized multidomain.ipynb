{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ghiki\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding, AutoTokenizer, set_seed\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.special import softmax\n",
    "import argparse\n",
    "import logging\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import wandb\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score, confusion_matrix\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
    "from transformers import AutoConfig, RobertaModel, RobertaForSequenceClassification\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "import pandas as pd,os\n",
    "import torch\n",
    "from statistics import mode\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline, Trainer\n",
    "from datasets import Dataset, load_metric\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "import pickle\n",
    "from transformers import RobertaConfig, RobertaModel\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoConfig, RobertaModel, RobertaForSequenceClassification\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from typing import Optional, Union, Tuple\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import EarlyStoppingCallback,AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding, AutoTokenizer, set_seed\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.special import softmax\n",
    "import argparse\n",
    "import logging\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import wandb\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score, confusion_matrix\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from textstat.textstat import textstatistics\n",
    "import pandas as pd\n",
    "import language_tool_python\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score, confusion_matrix\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(nn.Module):\n",
    "    \"\"\"Head for sentence-level classification tasks.\"\"\"\n",
    "\n",
    "    def __init__(self, config, num_extra_dims):\n",
    "        super().__init__()\n",
    "        total_dims = config.hidden_size+num_extra_dims\n",
    "        self.dense = nn.Linear(total_dims, total_dims)\n",
    "        classifier_dropout = (\n",
    "            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n",
    "        )\n",
    "        self.dropout = nn.Dropout(classifier_dropout)\n",
    "        self.out_proj = nn.Linear(total_dims, config.num_labels)\n",
    "\n",
    "    def forward(self, features, **kwargs):\n",
    "        x = self.dropout(features)\n",
    "        x = self.dense(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x\n",
    "\n",
    "class CustomSequenceClassification(RobertaForSequenceClassification):\n",
    "\n",
    "    def __init__(self, config, num_extra_dims):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.config = config\n",
    "\n",
    "        # might need to rename this depending on the model\n",
    "        self.roberta =  RobertaModel(config)\n",
    "        self.classifier = ClassificationHead(config, num_extra_dims)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        extra_data: Optional[torch.FloatTensor] = None,\n",
    "        token_type_ids: Optional[torch.LongTensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, SequenceClassifierOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.roberta(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        # sequence_output will be (batch_size, seq_length, hidden_size)\n",
    "        sequence_output = outputs[0]\n",
    "\n",
    "        # additional data should be (batch_size, num_extra_dims)\n",
    "        cls_embedding = sequence_output[:, 0, :]\n",
    "\n",
    "        output = torch.cat((cls_embedding, extra_data), dim=-1)\n",
    "\n",
    "        logits = self.classifier(output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = nn.MSELoss()\n",
    "                if self.num_labels == 1:\n",
    "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
    "                else:\n",
    "                    loss = loss_fct(logits, labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = nn.CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = nn.BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype \n",
      "---  ------    --------------  ----- \n",
      " 0   text      5000 non-null   object\n",
      " 1   label     5000 non-null   int64 \n",
      " 2   model     5000 non-null   object\n",
      " 3   language  5000 non-null   object\n",
      " 4   id        5000 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 195.4+ KB\n",
      "None\n",
      "\n",
      "label\n",
      "1    2500\n",
      "0    2500\n",
      "Name: count, dtype: int64\n",
      "\n",
      "model\n",
      "bloomz    2500\n",
      "human     2500\n",
      "Name: count, dtype: int64\n",
      "\n",
      "language\n",
      "english    5000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Processing with bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5000/5000 [00:01<00:00, 4516.16 examples/s]\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████| 625/625 [00:18<00:00, 34.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing with Hello-SimpleAI/chatgpt-detector-roberta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5000/5000 [00:00<00:00, 5055.61 examples/s]\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████| 625/625 [00:18<00:00, 34.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing with roberta-base-openai-detector\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5000/5000 [00:00<00:00, 5258.24 examples/s]\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████| 625/625 [00:18<00:00, 34.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing with roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5000/5000 [00:01<00:00, 4897.16 examples/s]\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████| 625/625 [00:18<00:00, 34.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing with distilbert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5000/5000 [00:00<00:00, 5149.33 examples/s]\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████| 625/625 [00:09<00:00, 65.74it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing with google/electra-base-discriminator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 5000/5000 [00:01<00:00, 4403.61 examples/s]\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████| 625/625 [00:18<00:00, 34.70it/s]\n",
      "Map: 100%|██████████| 5000/5000 [00:04<00:00, 1101.60 examples/s]\n",
      "Map: 100%|██████████| 5000/5000 [00:00<00:00, 18450.29 examples/s]\n",
      "Map: 100%|██████████| 5000/5000 [00:04<00:00, 1065.64 examples/s]\n",
      "Map: 100%|██████████| 5000/5000 [00:00<00:00, 29585.61 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing with multidomain-roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:23<00:00, 26.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing with multidomain-extended-roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 625/625 [00:23<00:00, 26.16it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "MODEL1 = 'bert-base-uncased'\n",
    "MODEL2 = 'Hello-SimpleAI/chatgpt-detector-roberta'\n",
    "MODEL3 = 'roberta-base-openai-detector'\n",
    "MODEL4 = 'roberta-base'\n",
    "MODEL5 = 'distilbert-base-uncased'\n",
    "MODEL6 = 'google/electra-base-discriminator'\n",
    "\n",
    "MODEL8='multidomain-roberta-base'\n",
    "MODEL9='multidomain-extended-roberta-base'\n",
    "\n",
    "MODEL_PATH1='SavedModels/optimized-bert-base-uncased-22.5k'\n",
    "MODEL_PATH2='SavedModels/optimized-chatgpt-detector-roberta-17.5k'\n",
    "MODEL_PATH3='SavedModels/optimized-roberta-base-openai-detector-12k'\n",
    "MODEL_PATH4='SavedModels/optimized-roberta-base-0.5k'\n",
    "MODEL_PATH5='SavedModels/optimized-distilbert-base-uncased-15k'\n",
    "MODEL_PATH6='SavedModels/optimized-electra-base-discriminator-6k'\n",
    "\n",
    "MODEL_PATH8='SavedModels/optimized-multidomain-roberta-base-1k'\n",
    "MODEL_PATH9='SavedModels/optimized-multidomain-extended-robera-base-0.8k'\n",
    "\n",
    "'''Preparing data'''\n",
    "\n",
    "df = pd.read_json('datasets/subtaskA_dev_monolingual.jsonl', lines=True)\n",
    "\n",
    "df = df.rename(columns={'source': 'language'})\n",
    "non_language_sources = ['wikihow', 'wikipedia', 'reddit', 'arxiv', 'peerread']\n",
    "df['language'] = df['language'].replace(non_language_sources, 'english')\n",
    "\n",
    "print(f'Original dataset')\n",
    "print(df.info())\n",
    "\n",
    "print(f'''\\n{df['label'].value_counts()}''')\n",
    "print(f'''\\n{df['model'].value_counts()}''')\n",
    "print(f'''\\n{df['language'].value_counts()}''')\n",
    "\n",
    "df = df[['text', 'label']]\n",
    "test_df=df\n",
    "\n",
    "test_texts = test_df['text'].tolist()\n",
    "\n",
    "def getPrediction(model_path):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    id2label = {0: \"human\", 1: \"machine\"}\n",
    "    label2id = {\"human\": 0, \"machine\": 1}\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_path, num_labels=len(label2id), id2label=id2label, label2id=label2id\n",
    "        )\n",
    "\n",
    "    test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "    def preprocess_function(examples, **fn_kwargs):\n",
    "        return fn_kwargs['tokenizer'](examples[\"text\"], truncation=True)\n",
    "\n",
    "    tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True,  fn_kwargs={'tokenizer': tokenizer})\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    # create Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        # compute_metrics=compute_metrics,\n",
    "    )\n",
    "    # get logits from predictions and evaluate results using classification report\n",
    "    predictions = trainer.predict(tokenized_test_dataset)\n",
    "    preds = np.argmax(predictions.predictions, axis=-1)\n",
    "    probs = softmax(predictions.predictions, axis=-1)\n",
    "    label_specific_probs = probs[:, 1]  # This extracts the probability for label 1\n",
    "    \n",
    "    return list(preds),list(label_specific_probs)\n",
    "\n",
    "def getPredictionMultidomain(model_path,num_extra_dims,test_data):\n",
    "    config = RobertaConfig.from_pretrained(model_path)\n",
    "    model = CustomSequenceClassification(config, num_extra_dims)\n",
    "    model.load_state_dict(torch.load(model_path+'/pytorch_model.bin'))\n",
    "    trainer = Trainer(model=model)\n",
    "    # get logits from predictions and evaluate results using classification report\n",
    "    predictions = trainer.predict(test_data)\n",
    "    preds = np.argmax(predictions.predictions, axis=-1)\n",
    "    probs = softmax(predictions.predictions, axis=-1)\n",
    "    label_specific_probs = probs[:, 1]  # This extracts the probability for label 1\n",
    "    \n",
    "    return list(preds),list(label_specific_probs)\n",
    "\n",
    "print(f'\\nProcessing with {MODEL1}')\n",
    "labels1,scores1=getPrediction(MODEL_PATH1)\n",
    "\n",
    "print(f'\\nProcessing with {MODEL2}')\n",
    "labels2,scores2=getPrediction(MODEL_PATH2)\n",
    "\n",
    "print(f'\\nProcessing with {MODEL3}')\n",
    "labels3,scores3=getPrediction(MODEL_PATH3)\n",
    "\n",
    "print(f'\\nProcessing with {MODEL4}')\n",
    "labels4,scores4=getPrediction(MODEL_PATH4)\n",
    "\n",
    "print(f'\\nProcessing with {MODEL5}')\n",
    "labels5,scores5=getPrediction(MODEL_PATH5)\n",
    "\n",
    "print(f'\\nProcessing with {MODEL6}')\n",
    "labels6,scores6=getPrediction(MODEL_PATH6)\n",
    "\n",
    "df_multidomain=pd.read_csv('datasets/features_df_test.csv')\n",
    "df_multidomain_extended=pd.read_csv('datasets/features_df_test_extended.csv')\n",
    "\n",
    "df_multidomain_extraData=df_multidomain.drop(['label'],axis=1)\n",
    "df_multidomain_extraData=df_multidomain_extraData.drop(['text'],axis=1)\n",
    "\n",
    "df_multidomain_extended_extraData=df_multidomain_extended.drop(['label'],axis=1)\n",
    "df_multidomain_extended_extraData=df_multidomain_extended_extraData.drop(['text'],axis=1)\n",
    "\n",
    "ds_test = Dataset.from_dict({\n",
    "        \"text\": df_multidomain['text'].tolist(), \n",
    "        \"extra_data\": df_multidomain_extraData.values.tolist(),\n",
    "        \"labels\": df_multidomain['label'].tolist()\n",
    "    })\n",
    "\n",
    "ds_test_extended = Dataset.from_dict({\n",
    "        \"text\": df_multidomain_extended['text'].tolist(), \n",
    "        \"extra_data\": df_multidomain_extended_extraData.values.tolist(),\n",
    "        \"labels\": df_multidomain_extended['label'].tolist()\n",
    "    })\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "tokenized_ds_test = ds_test.map(lambda x: tokenizer(x[\"text\"], padding='max_length', truncation=True))\n",
    "tokenized_ds_test = tokenized_ds_test.map(lambda x: {'extra_data': x['extra_data']})\n",
    "tokenized_ds_test_extended = ds_test_extended.map(lambda x: tokenizer(x[\"text\"], padding='max_length', truncation=True))\n",
    "tokenized_ds_test_extended = tokenized_ds_test_extended.map(lambda x: {'extra_data': x['extra_data']})\n",
    "\n",
    "print(f'\\nProcessing with {MODEL8}')\n",
    "labels8,scores8=getPredictionMultidomain(MODEL_PATH8,5,tokenized_ds_test)\n",
    "\n",
    "print(f'\\nProcessing with {MODEL9}')\n",
    "labels9,scores9=getPredictionMultidomain(MODEL_PATH9,9,tokenized_ds_test_extended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-uncased\n",
      "{'f1': 0.7835800547378446, 'confusion_matrix': [[1935, 565], [517, 1983]], 'accuracy': 0.7836, 'precision': 0.7782574568288854, 'recall': 0.7932, 'auc': 0.7836000000000001}\n",
      "\n",
      "Hello-SimpleAI/chatgpt-detector-roberta\n",
      "{'f1': 0.7197234156820624, 'confusion_matrix': [[2372, 128], [1208, 1292]], 'accuracy': 0.7328, 'precision': 0.9098591549295775, 'recall': 0.5168, 'auc': 0.7328}\n",
      "\n",
      "roberta-base-openai-detector\n",
      "{'f1': 0.8644663083985576, 'confusion_matrix': [[2371, 129], [544, 1956]], 'accuracy': 0.8654, 'precision': 0.9381294964028777, 'recall': 0.7824, 'auc': 0.8654}\n",
      "\n",
      "roberta-base\n",
      "{'f1': 0.8417475804685544, 'confusion_matrix': [[2059, 441], [350, 2150]], 'accuracy': 0.8418, 'precision': 0.8297954457738325, 'recall': 0.86, 'auc': 0.8418}\n",
      "\n",
      "distilbert-base-uncased\n",
      "{'f1': 0.7442622033685506, 'confusion_matrix': [[2189, 311], [947, 1553]], 'accuracy': 0.7484, 'precision': 0.8331545064377682, 'recall': 0.6212, 'auc': 0.7484}\n",
      "\n",
      "google/electra-base-discriminator\n",
      "{'f1': 0.777975985882633, 'confusion_matrix': [[1919, 581], [529, 1971]], 'accuracy': 0.778, 'precision': 0.7723354231974922, 'recall': 0.7884, 'auc': 0.778}\n",
      "\n",
      "Features Model\n",
      "{'f1': 0.6909230472951077, 'confusion_matrix': [[1846, 654], [888, 1612]], 'accuracy': 0.6916, 'precision': 0.7113857016769638, 'recall': 0.6448, 'auc': 0.6916}\n",
      "\n",
      "multidomain-roberta-base\n",
      "{'f1': 0.8279256820457738, 'confusion_matrix': [[2228, 272], [585, 1915]], 'accuracy': 0.8286, 'precision': 0.8756287151348879, 'recall': 0.766, 'auc': 0.8286}\n",
      "\n",
      "multidomain-extended-roberta-base\n",
      "{'f1': 0.848560256153625, 'confusion_matrix': [[2162, 338], [419, 2081]], 'accuracy': 0.8486, 'precision': 0.8602728400165358, 'recall': 0.8324, 'auc': 0.8486}\n"
     ]
    }
   ],
   "source": [
    "def getMetrics(predicted_labels, true_labels):\n",
    "    # Ensure the labels are numpy arrays\n",
    "    predicted_labels = np.array(predicted_labels)\n",
    "    true_labels = np.array(true_labels)\n",
    "\n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    f1 = precision_recall_fscore_support(true_labels, predicted_labels, average='macro')[2]\n",
    "    precision = precision_score(true_labels, predicted_labels, average='binary')\n",
    "    recall = recall_score(true_labels, predicted_labels, average='binary')\n",
    "    auc = roc_auc_score(true_labels, predicted_labels)\n",
    "    cm = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "    # Create a dictionary of metrics\n",
    "    metrics = {\n",
    "        'f1': f1,\n",
    "        'confusion_matrix': cm.tolist(),\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'auc': auc,\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "feature_df=pd.read_csv('datasets/features_df_test.csv')\n",
    "best_features=['word_count', 'avg_sentence_length', 'avg_word_length', 'gunning_fog_index', 'grammatical_errors']\n",
    "feature_df=feature_df[best_features]\n",
    "\n",
    "with open('SavedModels/optimized-rf-features-20k.pkl', 'rb') as file:\n",
    "    randomForest = pickle.load(file)\n",
    "MODEL7='Features Model'\n",
    "labels7=randomForest.predict(feature_df)\n",
    "probabilities = randomForest.predict_proba(feature_df)\n",
    "scores7 = probabilities.max(axis=1)\n",
    "\n",
    "print(MODEL1)\n",
    "print(getMetrics(labels1,test_df['label'].tolist()))\n",
    "print('')\n",
    "print(MODEL2)\n",
    "print(getMetrics(labels2,test_df['label'].tolist()))\n",
    "print('')\n",
    "print(MODEL3)\n",
    "print(getMetrics(labels3,test_df['label'].tolist()))\n",
    "print('')\n",
    "print(MODEL4)\n",
    "print(getMetrics(labels4,test_df['label'].tolist()))\n",
    "print('')\n",
    "print(MODEL5)\n",
    "print(getMetrics(labels5,test_df['label'].tolist()))\n",
    "print('')\n",
    "print(MODEL6)\n",
    "print(getMetrics(labels6,test_df['label'].tolist()))\n",
    "print('')\n",
    "print(MODEL7)\n",
    "print(getMetrics(labels7,test_df['label'].tolist()))\n",
    "print('')\n",
    "print(MODEL8)\n",
    "print(getMetrics(labels8,test_df['label'].tolist()))\n",
    "print('')\n",
    "print(MODEL9)\n",
    "print(getMetrics(labels9,test_df['label'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 18 columns):\n",
      " #   Column                                          Non-Null Count  Dtype  \n",
      "---  ------                                          --------------  -----  \n",
      " 0   Labels_bert-base-uncased                        5000 non-null   int64  \n",
      " 1   Scores_bert-base-uncased                        5000 non-null   float32\n",
      " 2   Labels_Hello-SimpleAI/chatgpt-detector-roberta  5000 non-null   int64  \n",
      " 3   Scores_Hello-SimpleAI/chatgpt-detector-roberta  5000 non-null   float32\n",
      " 4   Labels_roberta-base-openai-detector             5000 non-null   int64  \n",
      " 5   Scores_roberta-base-openai-detector             5000 non-null   float32\n",
      " 6   Labels_roberta-base                             5000 non-null   int64  \n",
      " 7   Scores_roberta-base                             5000 non-null   float32\n",
      " 8   Labels_distilbert-base-uncased                  5000 non-null   int64  \n",
      " 9   Scores_distilbert-base-uncased                  5000 non-null   float32\n",
      " 10  Labels_google/electra-base-discriminator        5000 non-null   int64  \n",
      " 11  Scores_google/electra-base-discriminator        5000 non-null   float32\n",
      " 12  Labels_Features Model                           5000 non-null   int64  \n",
      " 13  Scores_Features Model                           5000 non-null   float64\n",
      " 14  Labels_multidomain-roberta-base                 5000 non-null   int64  \n",
      " 15  Scores_multidomain-roberta-base                 5000 non-null   float32\n",
      " 16  Labels_multidomain-extended-roberta-base        5000 non-null   int64  \n",
      " 17  Scores_multidomain-extended-roberta-base        5000 non-null   float32\n",
      "dtypes: float32(8), float64(1), int64(9)\n",
      "memory usage: 547.0 KB\n",
      "Majority Voting: 0.866096607048386\n",
      "Soft Voting: 0.7194420736479322\n",
      "Rank Voting: 0.5315710061736914\n",
      "Borda Count: 0.7890459123442917\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "models = [MODEL1, MODEL2, MODEL3, MODEL4, MODEL5, MODEL6, MODEL7, MODEL8, MODEL9]\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    f'Labels_{MODEL1}': labels1,\n",
    "    f'Scores_{MODEL1}': scores1,\n",
    "    f'Labels_{MODEL2}': labels2,\n",
    "    f'Scores_{MODEL2}': scores2,\n",
    "    f'Labels_{MODEL3}': labels3,\n",
    "    f'Scores_{MODEL3}': scores3,\n",
    "    f'Labels_{MODEL4}': labels4,\n",
    "    f'Scores_{MODEL4}': scores4,\n",
    "    f'Labels_{MODEL5}': labels5,\n",
    "    f'Scores_{MODEL5}': scores5,\n",
    "    f'Labels_{MODEL6}': labels6,\n",
    "    f'Scores_{MODEL6}': scores6,\n",
    "    f'Labels_{MODEL7}': labels7,\n",
    "    f'Scores_{MODEL7}': scores7,\n",
    "    f'Labels_{MODEL8}': labels8,\n",
    "    f'Scores_{MODEL8}': scores8,\n",
    "    f'Labels_{MODEL9}': labels9,\n",
    "    f'Scores_{MODEL9}': scores9,\n",
    "})\n",
    "\n",
    "labels = test_df['label'].tolist()\n",
    "\n",
    "df.info()\n",
    "\n",
    "def ensemble_methods(df, models):\n",
    "    \n",
    "    majority_labels = []\n",
    "    score_based_labels = []\n",
    "    rank_voting_labels = []\n",
    "    borda_count_labels = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        label_counts = {0: 0, 1: 0}\n",
    "        score_sums = {0: 0.0, 1: 0.0}\n",
    "        weighted_scores = {0: 0.0, 1: 0.0}\n",
    "\n",
    "        for i, model in enumerate(models):\n",
    "            label = row[f'Labels_{model}']\n",
    "            score = row[f'Scores_{model}']\n",
    "            label_counts[label] += 1\n",
    "            score_sums[label] += score\n",
    "\n",
    "        # Majority Voting\n",
    "        majority_label = 0 if label_counts[0] > label_counts[1] else 1\n",
    "        majority_labels.append(majority_label)\n",
    "\n",
    "        # Soft Voting\n",
    "        avg_score_0 = score_sums[0] / (label_counts[0] if label_counts[0] else 1)\n",
    "        avg_score_1 = score_sums[1] / (label_counts[1] if label_counts[1] else 1)\n",
    "        score_based_label = 0 if avg_score_0 > avg_score_1 else 1\n",
    "        score_based_labels.append(score_based_label)\n",
    "        \n",
    "        # Rank Voting\n",
    "        ranks = [row[f'Scores_{model}'] for model in models]\n",
    "        ranked_labels = [label for _, label in sorted(zip(ranks, [row[f'Labels_{model}'] for model in models]))]\n",
    "        rank_voting_labels.append(ranked_labels[0])  # The label with the lowest rank\n",
    "\n",
    "        # Borda Count\n",
    "        borda_scores = {0: 0, 1: 0}\n",
    "        for rank, label in enumerate(ranked_labels):\n",
    "            borda_scores[label] += (len(models) - rank)\n",
    "        borda_count_labels.append(max(borda_scores, key=borda_scores.get))\n",
    "        \n",
    "    return {\n",
    "        'Majority Voting':majority_labels,\n",
    "        'Soft Voting':score_based_labels,\n",
    "        'Rank Voting':rank_voting_labels,\n",
    "        'Borda Count':borda_count_labels,\n",
    "    }\n",
    "\n",
    "ensemble_results = ensemble_methods(df, models)\n",
    "\n",
    "for voting_method, details in ensemble_results.items():\n",
    "    print(f'''{voting_method}: {getMetrics(details,labels)['f1']}''')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Combinations: 100%|██████████| 9/9 [01:36<00:00, 10.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy Score: 0.8868\n",
      "Best Model Combination: ('roberta-base-openai-detector', 'roberta-base', 'google/electra-base-discriminator', 'Features Model', 'multidomain-roberta-base', 'multidomain-extended-roberta-base')\n",
      "Best Voting Method: Majority Voting\n",
      "Best Metrics: {'f1': 0.8867599764460752, 'confusion_matrix': [[2170, 330], [236, 2264]], 'accuracy': 0.8868, 'precision': 0.8727833461835004, 'recall': 0.9056, 'auc': 0.8867999999999999}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "# Assuming all_models is a list of your model names\n",
    "all_models = [MODEL1, MODEL2, MODEL3, MODEL4, MODEL5, MODEL6, MODEL7, MODEL8, MODEL9]\n",
    "\n",
    "# This will store the best F1 score, the corresponding model combination, and the voting method\n",
    "best_f1_score = 0\n",
    "best_model_combination = None\n",
    "best_voting_method = None\n",
    "\n",
    "# Define the voting methods you want to evaluate\n",
    "voting_methods = ['Majority Voting', 'Soft Voting', 'Rank Voting', 'Borda Count']\n",
    "\n",
    "# Try all possible combinations of the models\n",
    "# for r in tqdm(range(1, len(all_models) + 1), desc='Model Combinations'):\n",
    "#     for model_combination in itertools.combinations(all_models, r):\n",
    "#         # Generate the predictions using the ensemble of the current combination of models\n",
    "#         ensemble_results = ensemble_methods(df, model_combination)\n",
    "        \n",
    "#         # Evaluate each voting method\n",
    "#         for method in voting_methods:\n",
    "#             f1_score = getMetrics(ensemble_results[method], labels)['f1']\n",
    "            \n",
    "#             # Update the best combination if the current one is better\n",
    "#             if f1_score > best_f1_score:\n",
    "#                 best_f1_score = f1_score\n",
    "#                 best_model_combination = model_combination\n",
    "#                 best_voting_method = method\n",
    "\n",
    "# # Print the best combination, its score, and the voting method\n",
    "# print(f\"Best F1 Score: {best_f1_score}\")\n",
    "# print(f\"Best Model Combination: {best_model_combination}\")\n",
    "# print(f\"Best Voting Method: {best_voting_method}\")\n",
    "\n",
    "for r in tqdm(range(1, len(all_models) + 1), desc='Model Combinations'):\n",
    "    for model_combination in itertools.combinations(all_models, r):\n",
    "        # Generate the predictions using the ensemble of the current combination of models\n",
    "        ensemble_results = ensemble_methods(df, model_combination)\n",
    "        \n",
    "        # Evaluate each voting method\n",
    "        for method in voting_methods:\n",
    "            metrics=getMetrics(ensemble_results[method], labels)\n",
    "            f1_score=metrics['accuracy']\n",
    "            \n",
    "            # Update the best combination if the current one is better\n",
    "            if f1_score > best_f1_score:\n",
    "                best_f1_score = f1_score\n",
    "                best_model_combination = model_combination\n",
    "                best_voting_method = method\n",
    "                best_metrics=metrics\n",
    "\n",
    "# Print the best combination, its score, and the voting method\n",
    "print(f\"Best Accuracy Score: {best_f1_score}\")\n",
    "print(f\"Best Model Combination: {best_model_combination}\")\n",
    "print(f\"Best Voting Method: {best_voting_method}\")\n",
    "print(f\"Best Metrics: {best_metrics}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
