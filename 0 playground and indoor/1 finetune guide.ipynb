{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (846495780.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[17], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    to be tested\u001b[0m\n\u001b[1;37m       ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "to be tested\n",
    "\n",
    "from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# 1. Load the Pretrained Model and Tokenizer\n",
    "model_name = \"roberta-base-openai-detector\"\n",
    "model = RobertaForSequenceClassification.from_pretrained(model_name)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# 2. Prepare Your Dataset\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
    "        encoding['labels'] = torch.tensor([label])\n",
    "        return encoding\n",
    "\n",
    "texts = [\"This is an example text.\", \"Another example.\"]\n",
    "labels = [0, 1]  # Use your actual labels here\n",
    "\n",
    "# 3. Tokenize Your Dataset\n",
    "dataset = MyDataset(texts, labels, tokenizer)\n",
    "\n",
    "# 4. Train the Model\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',          # output directory\n",
    "    num_train_epochs=3,              # total number of training epochs\n",
    "    per_device_train_batch_size=8,   # batch size per device during training\n",
    "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
    "    weight_decay=0.01,               # strength of weight decay\n",
    "    logging_dir='./logs',            # directory for storing logs\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,                         # the instantiated ðŸ¤— Transformers model to be trained\n",
    "    args=training_args,                  # training arguments, defined above\n",
    "    train_dataset=dataset,               # training dataset\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "# 5. Evaluate and Use the Model\n",
    "# Now the model is fine-tuned, you can evaluate it on a validation set, or use it for predictions\n",
    "print(trainer.predict(dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example of dataframe (text|label)\n",
      "\n",
      "                                                     text  label\n",
      "114918    We derive masses and radii for both componen...      0\n",
      "116652    Photon-induced reactions play a key role in ...      0\n",
      "119223  This work finds a connection between Bourgain'...      0\n",
      "31412   Well, um, eggs are really yummy, but some peop...      1\n",
      "106375    Properties of planetary atmospheres, ionosph...      0\n",
      "\n",
      "Size(20000, 2)\n",
      "\n",
      "Value count \n",
      "\n",
      "label\n",
      "0    10642\n",
      "1     9358\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "'''dataset'''\n",
    "\n",
    "\n",
    "# Import all the needed libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "import functools\n",
    "import wandb\n",
    "import random\n",
    "import os\n",
    "\n",
    "from datasets import Dataset, DatasetDict, load_metric\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, \\\n",
    " TrainingArguments, Trainer, pipeline, EarlyStoppingCallback, \\\n",
    " EncoderDecoderModel, RobertaTokenizerFast\n",
    "\n",
    "'''Loading data'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "df = pd.read_json(os.getcwd()+'/SubtaskA/subtaskA_train_monolingual.jsonl', lines=True)\n",
    "# Just interested so far in text and label\n",
    "df = df[['text', 'label']]\n",
    "\n",
    "'So testing doesnt takes too much time processing, 10k seems ok'\n",
    "df=df.sample(20000)\n",
    "\n",
    "print('\\nExample of dataframe (text|label)\\n')\n",
    "print(df.sample(5))\n",
    "print(f'\\nSize{df.shape}')\n",
    "print(f'\\nValue count \\n')\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "# Splitting the DataFrame into train, dev, and test sets\n",
    "train_df, temp_df = train_test_split(df, test_size=0.4, random_state=42)  # 60% training, 40% temporary\n",
    "dev_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)  # 20% development, 20% test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_TensorSliceDataset element_spec=({'input_ids': TensorSpec(shape=(512,), dtype=tf.int32, name=None), 'attention_mask': TensorSpec(shape=(512,), dtype=tf.int32, name=None)}, TensorSpec(shape=(), dtype=tf.int32, name=None))>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "train_text=train_df['text'].tolist()\n",
    "train_label=train_df['label'].tolist()\n",
    "\n",
    "val_text=dev_df['text'].tolist()\n",
    "val_label=dev_df['label'].tolist()\n",
    "\n",
    "test_text=test_df['text'].tolist()\n",
    "test_label=test_df['label'].tolist()\n",
    "\n",
    "MODEL_NAME = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME )\n",
    "\n",
    "train_encodings = tokenizer(train_text, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_text, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_text, truncation=True, padding=True)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    train_label\n",
    "))\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(val_encodings),\n",
    "    val_label\n",
    "))\n",
    "\n",
    "train_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading model.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 268M/268M [00:04<00:00, 65.5MB/s] \n"
     ]
    }
   ],
   "source": [
    "'''model'''\n",
    "\n",
    "from transformers import AutoModel\n",
    "\n",
    "model=AutoModel.from_pretrained(MODEL_NAME)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ghiki\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer_tf.py:118: FutureWarning: The class `TFTrainer` is deprecated and will be removed in version 5 of Transformers. We recommend using native Keras instead, by calling methods like `fit()` and `predict()` directly on the model object. Detailed examples of the Keras style can be found in our examples at https://github.com/huggingface/transformers/tree/main/examples/tensorflow\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Ghiki\\Documents\\GitHub\\Multidomain--Multimodal-and-Multilingual-Machine-Generated-Text-Detection\\wandb\\run-20230928_132455-6df1xya9</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alberto-rodero557/huggingface/runs/6df1xya9' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/alberto-rodero557/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alberto-rodero557/huggingface' target=\"_blank\">https://wandb.ai/alberto-rodero557/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alberto-rodero557/huggingface/runs/6df1xya9' target=\"_blank\">https://wandb.ai/alberto-rodero557/huggingface/runs/6df1xya9</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "'''training'''\n",
    "from transformers import TFTrainingArguments, TFTrainer\n",
    "\n",
    "training_args = TFTrainingArguments(\n",
    "    output_dir='./results',          \n",
    "    num_train_epochs=3,              \n",
    "    per_device_train_batch_size=16,  \n",
    "    per_device_eval_batch_size=64,   \n",
    "    warmup_steps=500,                \n",
    "    weight_decay=0.01,              \n",
    "    logging_dir='./logs',            \n",
    "    logging_steps=10,\n",
    ")\n",
    "\n",
    "\n",
    "trainer = TFTrainer(\n",
    "    model=model,                         \n",
    "    args=training_args,                  \n",
    "    train_dataset=train_dataset,         \n",
    "    eval_dataset=val_dataset             \n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`Checkpoint` was expecting model to be a trackable object (an object derived from `Trackable`), got DistilBertModel(\n  (embeddings): Embeddings(\n    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n    (position_embeddings): Embedding(512, 768)\n    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (transformer): Transformer(\n    (layer): ModuleList(\n      (0-5): 6 x TransformerBlock(\n        (attention): MultiHeadSelfAttention(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n        )\n        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (ffn): FFN(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n          (activation): GELUActivation()\n        )\n        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      )\n    )\n  )\n). If you believe this object should be trackable (i.e. it is part of the TensorFlow Python API and manages state), please open an issue.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[1;32mc:\\Users\\Ghiki\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer_tf.py:514\u001b[0m, in \u001b[0;36mTFTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_optimizer_and_scheduler(num_training_steps\u001b[39m=\u001b[39mt_total)\n\u001b[0;32m    513\u001b[0m folder \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39moutput_dir, PREFIX_CHECKPOINT_DIR)\n\u001b[1;32m--> 514\u001b[0m ckpt \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39;49mtrain\u001b[39m.\u001b[39;49mCheckpoint(optimizer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptimizer, model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel)\n\u001b[0;32m    515\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mckpt_manager \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mtrain\u001b[39m.\u001b[39mCheckpointManager(ckpt, folder, max_to_keep\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39msave_total_limit)\n\u001b[0;32m    517\u001b[0m iterations \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39miterations\n",
      "File \u001b[1;32mc:\\Users\\Ghiki\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\checkpoint\\checkpoint.py:2177\u001b[0m, in \u001b[0;36mCheckpoint.__init__\u001b[1;34m(self, root, **kwargs)\u001b[0m\n\u001b[0;32m   2175\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(converted_v, weakref\u001b[39m.\u001b[39mref):\n\u001b[0;32m   2176\u001b[0m   converted_v \u001b[39m=\u001b[39m converted_v()\n\u001b[1;32m-> 2177\u001b[0m _assert_trackable(converted_v, k)\n\u001b[0;32m   2179\u001b[0m \u001b[39mif\u001b[39;00m root:\n\u001b[0;32m   2180\u001b[0m   \u001b[39m# Make sure that root doesn't already have dependencies with these names\u001b[39;00m\n\u001b[0;32m   2181\u001b[0m   child \u001b[39m=\u001b[39m trackable_root\u001b[39m.\u001b[39m_lookup_dependency(k)\n",
      "File \u001b[1;32mc:\\Users\\Ghiki\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\checkpoint\\checkpoint.py:1531\u001b[0m, in \u001b[0;36m_assert_trackable\u001b[1;34m(obj, name)\u001b[0m\n\u001b[0;32m   1528\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_assert_trackable\u001b[39m(obj, name):\n\u001b[0;32m   1529\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\n\u001b[0;32m   1530\u001b[0m       obj, (base\u001b[39m.\u001b[39mTrackable, def_function\u001b[39m.\u001b[39mFunction)):\n\u001b[1;32m-> 1531\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1532\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m`Checkpoint` was expecting \u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m to be a trackable object (an \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1533\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mobject derived from `Trackable`), got \u001b[39m\u001b[39m{\u001b[39;00mobj\u001b[39m}\u001b[39;00m\u001b[39m. If you believe this \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1534\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mobject should be trackable (i.e. it is part of the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1535\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mTensorFlow Python API and manages state), please open an issue.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: `Checkpoint` was expecting model to be a trackable object (an object derived from `Trackable`), got DistilBertModel(\n  (embeddings): Embeddings(\n    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n    (position_embeddings): Embedding(512, 768)\n    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n  )\n  (transformer): Transformer(\n    (layer): ModuleList(\n      (0-5): 6 x TransformerBlock(\n        (attention): MultiHeadSelfAttention(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (q_lin): Linear(in_features=768, out_features=768, bias=True)\n          (k_lin): Linear(in_features=768, out_features=768, bias=True)\n          (v_lin): Linear(in_features=768, out_features=768, bias=True)\n          (out_lin): Linear(in_features=768, out_features=768, bias=True)\n        )\n        (sa_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n        (ffn): FFN(\n          (dropout): Dropout(p=0.1, inplace=False)\n          (lin1): Linear(in_features=768, out_features=3072, bias=True)\n          (lin2): Linear(in_features=3072, out_features=768, bias=True)\n          (activation): GELUActivation()\n        )\n        (output_layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      )\n    )\n  )\n). If you believe this object should be trackable (i.e. it is part of the TensorFlow Python API and manages state), please open an issue."
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
