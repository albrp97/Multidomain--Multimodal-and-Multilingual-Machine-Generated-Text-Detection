{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\ghiki\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.33.3)\n",
      "Requirement already satisfied: datasets in c:\\users\\ghiki\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.14.5)\n",
      "Requirement already satisfied: filelock in c:\\users\\ghiki\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (3.12.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in c:\\users\\ghiki\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.17.3)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\ghiki\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (1.24.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\ghiki\\appdata\\roaming\\python\\python311\\site-packages (from transformers) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\ghiki\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\ghiki\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2023.8.8)\n",
      "Requirement already satisfied: requests in c:\\users\\ghiki\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in c:\\users\\ghiki\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in c:\\users\\ghiki\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (0.3.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\ghiki\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in c:\\users\\ghiki\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (13.0.0)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in c:\\users\\ghiki\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in c:\\users\\ghiki\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (2.1.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\ghiki\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (3.3.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\ghiki\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.70.15)\n",
      "Requirement already satisfied: fsspec[http]<2023.9.0,>=2023.1.0 in c:\\users\\ghiki\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (2023.6.0)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\ghiki\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (3.8.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\ghiki\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\users\\ghiki\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (3.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\ghiki\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\ghiki\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\ghiki\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (1.9.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\ghiki\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\ghiki\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\ghiki\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ghiki\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ghiki\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2.0.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ghiki\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\ghiki\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\ghiki\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\ghiki\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\ghiki\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\ghiki\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\ghiki\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.1.99)\n",
      "Requirement already satisfied: wandb in c:\\users\\ghiki\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.15.11)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in c:\\users\\ghiki\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from wandb) (8.1.7)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in c:\\users\\ghiki\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from wandb) (3.1.37)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in c:\\users\\ghiki\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from wandb) (2.31.0)\n",
      "Requirement already satisfied: psutil>=5.0.0 in c:\\users\\ghiki\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from wandb) (5.9.4)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in c:\\users\\ghiki\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from wandb) (1.31.0)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in c:\\users\\ghiki\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: PyYAML in c:\\users\\ghiki\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from wandb) (6.0.1)\n",
      "Requirement already satisfied: pathtools in c:\\users\\ghiki\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: setproctitle in c:\\users\\ghiki\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from wandb) (1.3.2)\n",
      "Requirement already satisfied: setuptools in c:\\users\\ghiki\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from wandb) (65.5.0)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in c:\\users\\ghiki\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in c:\\users\\ghiki\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from wandb) (4.24.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\ghiki\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from Click!=8.0.0,>=7.1->wandb) (0.4.6)\n",
      "Requirement already satisfied: six>=1.4.0 in c:\\users\\ghiki\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from docker-pycreds>=0.4.0->wandb) (1.16.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in c:\\users\\ghiki\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\ghiki\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ghiki\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\ghiki\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2.0.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ghiki\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests<3,>=2.0.0->wandb) (2022.12.7)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in c:\\users\\ghiki\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.1)\n"
     ]
    }
   ],
   "source": [
    "# Install needed libraries\n",
    "!pip install transformers datasets\n",
    "!pip install sentencepiece\n",
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import all the needed libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import functools\n",
    "import wandb\n",
    "import random\n",
    "import os\n",
    "\n",
    "from datasets import Dataset, DatasetDict, load_metric\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, \\\n",
    " TrainingArguments, Trainer, pipeline, EarlyStoppingCallback, \\\n",
    " EncoderDecoderModel, RobertaTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU detected. Currently using: \"NVIDIA GeForce RTX 4090\"\n"
     ]
    }
   ],
   "source": [
    "# Check that pyTorch is identifying the GPU\n",
    "if torch.cuda.device_count() > 0:\n",
    "  print(f'GPU detected. Currently using: \"{torch.cuda.get_device_name(0)}\"')\n",
    "else:\n",
    "  raise Exception('Currently using CPU, change the type of the runtime in the \\'runtime\\' tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example of dataframe (text|label)\n",
      "\n",
      "                                                     text  label\n",
      "115179    Recent applications have proved that the Sur...      0\n",
      "88021   You've misread the article that you linked. \\n...      0\n",
      "63746    Knowing your own strengths and what you care ...      0\n",
      "77921   The Roman Catholic Diocese of Jaipur (Dioecesi...      0\n",
      "756     How to Make Crepe Paper Peonies\\n\\nIf you're l...      1\n",
      "\n",
      "Size(20000, 2)\n",
      "\n",
      "Value count \n",
      "\n",
      "label\n",
      "0    10406\n",
      "1     9594\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', '__index_level_0__'],\n",
       "        num_rows: 12000\n",
       "    })\n",
       "    valid: Dataset({\n",
       "        features: ['text', 'label', '__index_level_0__'],\n",
       "        num_rows: 4000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', '__index_level_0__'],\n",
       "        num_rows: 4000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Loading data'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "df = pd.read_json(os.getcwd()+'/SubtaskA/subtaskA_train_monolingual.jsonl', lines=True)\n",
    "# Just interested so far in text and label\n",
    "df = df[['text', 'label']]\n",
    "\n",
    "'So testing doesnt takes too much time processing, 10k seems ok'\n",
    "df=df.sample(20000)\n",
    "\n",
    "print('\\nExample of dataframe (text|label)\\n')\n",
    "print(df.sample(5))\n",
    "print(f'\\nSize{df.shape}')\n",
    "print(f'\\nValue count \\n')\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "# Splitting the DataFrame into train, dev, and test sets\n",
    "train_df, temp_df = train_test_split(df, test_size=0.4, random_state=42)  # 60% training, 40% temporary\n",
    "dev_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)  # 20% development, 20% test\n",
    "\n",
    "# Convert dataframes to datasets objects\n",
    "train_dataset = Dataset.from_pandas(train_df, split='train')\n",
    "valid_dataset = Dataset.from_pandas(dev_df, split='valid')\n",
    "test_dataset = Dataset.from_pandas(test_df, split='test')\n",
    "\n",
    "dataset = DatasetDict({'train': train_dataset, 'valid': valid_dataset, 'test': test_dataset})\n",
    "dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = load_metric('accuracy')\n",
    "f1 = load_metric('f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metric(eval_pred):\n",
    "  predictions, labels = eval_pred\n",
    "\n",
    "  predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "  result_acc = accuracy.compute(predictions=predictions, references=labels)['accuracy']\n",
    "  result_f1 = f1.compute(predictions=predictions, references=labels)['f1']\n",
    "\n",
    "  return {'accuracy': result_acc, 'f1-score': result_f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bert-base-uncased'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_checkpoint=\"bert-base-uncased\"\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 570/570 [00:00<00:00, 570kB/s]\n",
      "Downloading model.safetensors: 100%|██████████| 440M/440M [00:05<00:00, 75.8MB/s]\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "n_labels=2\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=n_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<?, ?B/s]\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 1.12MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 4.29MB/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42\n",
    "BATCH_SIZE = 6\n",
    "EPOCHS = 10\n",
    "METRIC_FOR_BEST_MODEL = \"eval_loss\"\n",
    "GENERATE_GRAPHIC = True\n",
    "LEARNING_RATE = 2e-05\n",
    "WEIGHT_DECAY = 0.01\n",
    "\n",
    "# Define the training parameters\n",
    "num_train_samples = dataset['train'].num_rows\n",
    "train_dataset = dataset['train'].shuffle(seed=RANDOM_SEED).select(range(num_train_samples))\n",
    "logging_steps = len(train_dataset) // (2 * BATCH_SIZE * EPOCHS)\n",
    "\n",
    "# If the chosen metric for best model is 'eval_loss' we have to adjust\n",
    "# a parameter so it chooses the smallest value.\n",
    "if METRIC_FOR_BEST_MODEL == 'eval_loss':\n",
    "  metric_condition = False\n",
    "else:\n",
    "  metric_condition = True\n",
    "\n",
    "# Tell to the trainer wether it needs to generate the graphic or not\n",
    "if GENERATE_GRAPHIC:\n",
    "  report_option = 'wandb'\n",
    "else:\n",
    "  report_option = None\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='results',\n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    per_device_train_batch_size=BATCH_SIZE,\n",
    "    per_device_eval_batch_size=BATCH_SIZE,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=METRIC_FOR_BEST_MODEL,\n",
    "    greater_is_better=metric_condition,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    logging_steps=logging_steps,\n",
    "    save_total_limit=3,\n",
    "    report_to=report_option,\n",
    "    push_to_hub=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "train_text=train_df['text'].tolist()\n",
    "train_label=train_df['label'].tolist()\n",
    "\n",
    "val_text=dev_df['text'].tolist()\n",
    "val_label=dev_df['label'].tolist()\n",
    "\n",
    "test_text=test_df['text'].tolist()\n",
    "test_label=test_df['label'].tolist()\n",
    "\n",
    "MODEL_NAME = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME )\n",
    "\n",
    "train_encodings = tokenizer(train_text, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_text, truncation=True, padding=True)\n",
    "test_encodings = tokenizer(test_text, truncation=True, padding=True)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(train_encodings),\n",
    "    train_label\n",
    "))\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    dict(val_encodings),\n",
    "    val_label\n",
    "))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ES_PATIENCE = 3\n",
    "# Create a Trainer object that will do the work for us\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metric,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=ES_PATIENCE)],\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      | 428786 KiB |    895 MiB |    895 MiB | 488176 KiB |\n",
      "|       from large pool | 428288 KiB |    894 MiB |    894 MiB | 487680 KiB |\n",
      "|       from small pool |    498 KiB |      0 MiB |      0 MiB |    496 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         | 428786 KiB |    895 MiB |    895 MiB | 488176 KiB |\n",
      "|       from large pool | 428288 KiB |    894 MiB |    894 MiB | 487680 KiB |\n",
      "|       from small pool |    498 KiB |      0 MiB |      0 MiB |    496 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      | 427679 KiB |    893 MiB |    893 MiB | 486911 KiB |\n",
      "|       from large pool | 427182 KiB |    892 MiB |    892 MiB | 486417 KiB |\n",
      "|       from small pool |    497 KiB |      0 MiB |      0 MiB |    494 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   | 503808 KiB |    980 MiB |    980 MiB | 499712 KiB |\n",
      "|       from large pool | 501760 KiB |    978 MiB |    978 MiB | 499712 KiB |\n",
      "|       from small pool |   2048 KiB |      2 MiB |      2 MiB |      0 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  75021 KiB | 101387 KiB | 731617 KiB | 656595 KiB |\n",
      "|       from large pool |  73472 KiB |  99840 KiB | 729076 KiB | 655604 KiB |\n",
      "|       from small pool |   1549 KiB |   2045 KiB |   2541 KiB |    991 KiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     205    |     408    |     409    |     204    |\n",
      "|       from large pool |      75    |     150    |     150    |      75    |\n",
      "|       from small pool |     130    |     258    |     259    |     129    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     205    |     408    |     409    |     204    |\n",
      "|       from large pool |      75    |     150    |     150    |      75    |\n",
      "|       from small pool |     130    |     258    |     259    |     129    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      22    |      40    |      40    |      18    |\n",
      "|       from large pool |      21    |      39    |      39    |      18    |\n",
      "|       from small pool |       1    |       1    |       1    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      23    |      39    |      44    |      21    |\n",
      "|       from large pool |      21    |      36    |      40    |      19    |\n",
      "|       from small pool |       2    |       4    |       4    |       2    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(model.parameters()).is_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "TypeError",
     "evalue": "'_TensorSliceDataset' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[35], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[1;32mc:\\Users\\Ghiki\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:1556\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1554\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1555\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1556\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1557\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1558\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1559\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1560\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1561\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Ghiki\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:1816\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1813\u001b[0m     rng_to_sync \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m   1815\u001b[0m step \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m\n\u001b[1;32m-> 1816\u001b[0m \u001b[39mfor\u001b[39;00m step, inputs \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(epoch_iterator):\n\u001b[0;32m   1817\u001b[0m     total_batched_samples \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m   1818\u001b[0m     \u001b[39mif\u001b[39;00m rng_to_sync:\n",
      "File \u001b[1;32mc:\\Users\\Ghiki\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\accelerate\\data_loader.py:384\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[39m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[0;32m    383\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 384\u001b[0m     current_batch \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(dataloader_iter)\n\u001b[0;32m    385\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mStopIteration\u001b[39;00m:\n\u001b[0;32m    386\u001b[0m     \u001b[39myield\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Ghiki\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:633\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    631\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    632\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 633\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    634\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    636\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    637\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Ghiki\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:677\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    675\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    676\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 677\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    678\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    679\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Ghiki\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;49;00m idx \u001b[39min\u001b[39;49;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Ghiki\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:51\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     49\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 51\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[1;31mTypeError\u001b[0m: '_TensorSliceDataset' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
