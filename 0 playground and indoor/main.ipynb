{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example of dataframe (text|label)\n",
      "\n",
      "                                                     text  label\n",
      "58515    When you fall asleep, your body temperature n...      0\n",
      "114540    A key challenge to deploying reinforcement l...      0\n",
      "43617   \\n\\nThis paper addresses the question of how h...      1\n",
      "29586   Young Dan’l Boone is a television series that ...      1\n",
      "74753   Conal Holmes O'Connell O'Riordan (pseudonym No...      0\n",
      "\n",
      "Size(10000, 2)\n",
      "\n",
      "Value count\n",
      "\n",
      "label\n",
      "0    5204\n",
      "1    4796\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "'''Subtask 1'''\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "df = pd.read_json('./SubtaskA/datasets/subtaskA_train_monolingual.jsonl', lines=True)\n",
    "# Just interested so far in text and label\n",
    "df = df[['text', 'label']]\n",
    "\n",
    "'So testing doesnt takes too much time processing, 10k seems ok'\n",
    "df=df.sample(10000)\n",
    "\n",
    "print('\\nExample of dataframe (text|label)\\n')\n",
    "print(df.sample(5))\n",
    "print(f'\\nSize{df.shape}')\n",
    "print('\\nValue count\\n')\n",
    "print(df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "'''Quick model to test dataset with glove and RF/Dense/RNN'''\n",
    "\n",
    "#Glove Embedding, we will try first with just 200 dimensions\n",
    "\"\"\"Load the Glove vectors in a dictionay\"\"\"\n",
    "# Download from here https://nlp.stanford.edu/projects/glove/\n",
    "embeddings_index={}\n",
    "with open(os.getcwd()+'/OtherData/glove.6B.200d.txt','r',encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values=line.split()\n",
    "        word=values[0]\n",
    "        vectors=np.asarray(values[1:],'float32')\n",
    "        embeddings_index[word]=vectors\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size before balancing: (10000, 2)\n",
      "Dataset size after balancing: (9592, 1)\n",
      "Entried dropped: 408\n",
      "\n",
      "Balanced DataFrame:\n",
      "label\n",
      "0    4796\n",
      "1    4796\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "'''Balance the dataset so its easier to run'''\n",
    "\n",
    "print(f'Dataset size before balancing: {df.shape}')\n",
    "counts = df['label'].value_counts()\n",
    "\n",
    "sampler = RandomUnderSampler(random_state=42)\n",
    "x_text, y = sampler.fit_resample(df[['text']], df['label'])\n",
    "\n",
    "print(f'Dataset size after balancing: {x_text.shape}')\n",
    "print(f'Entried dropped: {df.shape[0]-x_text.shape[0]}')\n",
    "\n",
    "# Create a new balanced DataFrame\n",
    "balanced_df = pd.DataFrame({'text': x_text['text'], 'label': y})\n",
    "\n",
    "# Print the balanced DataFrame\n",
    "print(\"\\nBalanced DataFrame:\")\n",
    "print(balanced_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9592/9592 [00:14<00:00, 663.67it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9592, 200)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Creating embedings'''\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from tqdm import tqdm \n",
    "\n",
    "def sent2vec(s):\n",
    "    \"\"\" Function Creates a normalized vector for the whole sentence\"\"\"\n",
    "    words = str(s).lower()\n",
    "    words = word_tokenize(words)\n",
    "    words = [w for w in words if w.isalpha()]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            M.append(embeddings_index[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(200)\n",
    "    return v / np.sqrt((v ** 2).sum())\n",
    "\n",
    "x_glove = np.array([sent2vec(x) for x in tqdm(x_text['text'])])\n",
    "x_glove.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Preparing for training'''\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_glove, y, test_size=0.2, random_state=42)\n",
    "# Initialize the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# Fit the scaler to the training data and transform the data\n",
    "x_train_scaled = scaler.fit_transform(x_train)\n",
    "x_test_scaled = scaler.transform(x_test)  # Note that we use the same scaler to transform the test data\n",
    "# If this scaler is going to be used later on for prediction it must be saved, for example with pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([7673, 200]) torch.Size([7673]) torch.Size([1919, 200]) torch.Size([1919])\n",
      "7673 1919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Ghiki\\AppData\\Local\\Temp\\ipykernel_6880\\1058304169.py:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_train_scaled = torch.tensor(x_train_scaled, dtype=torch.float32)\n",
      "C:\\Users\\Ghiki\\AppData\\Local\\Temp\\ipykernel_6880\\1058304169.py:30: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_test_scaled = torch.tensor(x_test_scaled, dtype=torch.float32)\n",
      "C:\\Users\\Ghiki\\AppData\\Local\\Temp\\ipykernel_6880\\1058304169.py:32: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_train = torch.tensor(y_train.to_numpy(), dtype=torch.float32) if isinstance(y_train, pd.Series) else torch.tensor(y_train, dtype=torch.float32)\n",
      "C:\\Users\\Ghiki\\AppData\\Local\\Temp\\ipykernel_6880\\1058304169.py:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y_test = torch.tensor(y_test.to_numpy(), dtype=torch.float32) if isinstance(y_test, pd.Series) else torch.tensor(y_test, dtype=torch.float32)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, auc, f1_score\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "# Sample Data, replace these with actual numpy arrays of your data\n",
    "import numpy as np\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, features, labels):\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'x': self.features[idx],  # changed 'features' to 'x'\n",
    "            'label': self.labels[idx]  # changed 'labels' to 'label'\n",
    "        }\n",
    "\n",
    "# Verify the shape and type of the data\n",
    "print(x_train_scaled.shape, y_train.shape, x_test_scaled.shape, y_test.shape)\n",
    "\n",
    "# Convert numpy arrays to PyTorch tensors\n",
    "x_train_scaled = torch.tensor(x_train_scaled, dtype=torch.float32)\n",
    "x_test_scaled = torch.tensor(x_test_scaled, dtype=torch.float32)\n",
    "\n",
    "y_train = torch.tensor(y_train.to_numpy(), dtype=torch.float32) if isinstance(y_train, pd.Series) else torch.tensor(y_train, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test.to_numpy(), dtype=torch.float32) if isinstance(y_test, pd.Series) else torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# Create dataset and dataloaders\n",
    "train_dataset = MyDataset(x_train_scaled, y_train)\n",
    "test_dataset = MyDataset(x_test_scaled, y_test)\n",
    "\n",
    "# Add prints to check the length of datasets\n",
    "print(len(train_dataset), len(test_dataset))\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# PyTorch model\n",
    "class MyModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(MyModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 200)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.batchnorm1 = nn.BatchNorm1d(200)\n",
    "        self.fc2 = nn.Linear(200, 100)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.batchnorm2 = nn.BatchNorm1d(100)\n",
    "        self.fc3 = nn.Linear(100, 100)\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "        self.batchnorm3 = nn.BatchNorm1d(100)\n",
    "        self.fc4 = nn.Linear(100, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x, labels=None):\n",
    "        x = F.relu(self.batchnorm1(self.dropout1(self.fc1(x))))\n",
    "        x = F.relu(self.batchnorm2(self.dropout2(self.fc2(x))))\n",
    "        x = F.relu(self.batchnorm3(self.dropout3(self.fc3(x))))\n",
    "        logits = self.fc4(x)\n",
    "        outputs = self.sigmoid(logits)\n",
    "\n",
    "        if labels is not None:\n",
    "            loss = F.binary_cross_entropy(outputs, labels.unsqueeze(-1))\n",
    "            return loss, outputs\n",
    "        return outputs\n",
    "\n",
    "# Instantiate the model\n",
    "model = MyModel(input_dim=x_train_scaled.shape[1])\n",
    "\n",
    "# Create dataset and dataloaders\n",
    "train_dataset = MyDataset(x_train_scaled, y_train)\n",
    "test_dataset = MyDataset(x_test_scaled, y_test)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=1000,\n",
    "    per_device_train_batch_size=64,\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    logging_dir='./logs',\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                          \n",
      "  0%|          | 0/120000 [00:43<?, ?it/s]           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.6201, 'learning_rate': 4.979166666666667e-05, 'epoch': 4.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                          376.55it/s]\u001b[A\n",
      "  0%|          | 0/120000 [00:44<?, ?it/s]           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.5279, 'learning_rate': 4.958333333333334e-05, 'epoch': 8.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                          \n",
      "  0%|          | 0/120000 [00:45<?, ?it/s]            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4745, 'learning_rate': 4.937500000000001e-05, 'epoch': 12.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                          \n",
      "  0%|          | 0/120000 [00:47<?, ?it/s]            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4459, 'learning_rate': 4.9166666666666665e-05, 'epoch': 16.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                          \n",
      "  0%|          | 0/120000 [00:48<?, ?it/s]            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4264, 'learning_rate': 4.8958333333333335e-05, 'epoch': 20.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                          \n",
      "  0%|          | 0/120000 [00:50<?, ?it/s]            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.4038, 'learning_rate': 4.875e-05, 'epoch': 25.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                          \n",
      "  0%|          | 0/120000 [00:51<?, ?it/s]            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3885, 'learning_rate': 4.854166666666667e-05, 'epoch': 29.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                          \n",
      "  0%|          | 0/120000 [00:52<?, ?it/s]            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.376, 'learning_rate': 4.8333333333333334e-05, 'epoch': 33.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                          \n",
      "  0%|          | 0/120000 [00:54<?, ?it/s]            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3606, 'learning_rate': 4.8125000000000004e-05, 'epoch': 37.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                          \n",
      "  0%|          | 0/120000 [00:55<?, ?it/s]            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3512, 'learning_rate': 4.791666666666667e-05, 'epoch': 41.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                          \n",
      "  0%|          | 0/120000 [00:57<?, ?it/s]            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3341, 'learning_rate': 4.770833333333334e-05, 'epoch': 45.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                           357.32it/s]\u001b[A\n",
      "  0%|          | 0/120000 [00:58<?, ?it/s]            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.327, 'learning_rate': 4.75e-05, 'epoch': 50.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                          \n",
      "  0%|          | 0/120000 [00:59<?, ?it/s]            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3162, 'learning_rate': 4.7291666666666666e-05, 'epoch': 54.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                          \n",
      "  0%|          | 0/120000 [01:01<?, ?it/s]            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.3054, 'learning_rate': 4.708333333333334e-05, 'epoch': 58.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                          \n",
      "  0%|          | 0/120000 [01:02<?, ?it/s]            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2991, 'learning_rate': 4.6875e-05, 'epoch': 62.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                          \n",
      "  0%|          | 0/120000 [01:03<?, ?it/s]            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2899, 'learning_rate': 4.666666666666667e-05, 'epoch': 66.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                          \n",
      "  0%|          | 0/120000 [01:05<?, ?it/s]            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.281, 'learning_rate': 4.6458333333333335e-05, 'epoch': 70.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                          \n",
      "  0%|          | 0/120000 [01:06<?, ?it/s]            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.271, 'learning_rate': 4.6250000000000006e-05, 'epoch': 75.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                          \n",
      "  0%|          | 0/120000 [01:07<?, ?it/s]            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2636, 'learning_rate': 4.604166666666666e-05, 'epoch': 79.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                          \n",
      "  0%|          | 0/120000 [01:09<?, ?it/s]            "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2588, 'learning_rate': 4.5833333333333334e-05, 'epoch': 83.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                          \n",
      "  0%|          | 0/120000 [01:10<?, ?it/s]             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2526, 'learning_rate': 4.5625e-05, 'epoch': 87.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                          \n",
      "  0%|          | 0/120000 [01:12<?, ?it/s]             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2451, 'learning_rate': 4.541666666666667e-05, 'epoch': 91.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                          \n",
      "  0%|          | 0/120000 [01:13<?, ?it/s]             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2399, 'learning_rate': 4.520833333333334e-05, 'epoch': 95.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                          , 369.50it/s]\u001b[A\n",
      "  0%|          | 0/120000 [01:15<?, ?it/s]             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2272, 'learning_rate': 4.5e-05, 'epoch': 100.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                          , 380.27it/s]\u001b[A\n",
      "  0%|          | 0/120000 [01:16<?, ?it/s]             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2263, 'learning_rate': 4.4791666666666673e-05, 'epoch': 104.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                          \n",
      "  0%|          | 0/120000 [01:17<?, ?it/s]             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2209, 'learning_rate': 4.458333333333334e-05, 'epoch': 108.33}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                          \n",
      "  0%|          | 0/120000 [01:19<?, ?it/s]             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2146, 'learning_rate': 4.4375e-05, 'epoch': 112.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                          \n",
      "  0%|          | 0/120000 [01:20<?, ?it/s]             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2084, 'learning_rate': 4.4166666666666665e-05, 'epoch': 116.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                          \n",
      "  0%|          | 0/120000 [01:22<?, ?it/s]             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.2025, 'learning_rate': 4.3958333333333336e-05, 'epoch': 120.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                          \n",
      "  0%|          | 0/120000 [01:23<?, ?it/s]             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1911, 'learning_rate': 4.375e-05, 'epoch': 125.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                          \n",
      "  0%|          | 0/120000 [01:25<?, ?it/s]             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1882, 'learning_rate': 4.354166666666667e-05, 'epoch': 129.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain()\n",
      "File \u001b[1;32mc:\\Users\\Ghiki\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:1556\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1554\u001b[0m         hf_hub_utils\u001b[39m.\u001b[39menable_progress_bars()\n\u001b[0;32m   1555\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1556\u001b[0m     \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1557\u001b[0m         args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1558\u001b[0m         resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1559\u001b[0m         trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1560\u001b[0m         ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1561\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Ghiki\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\trainer.py:1896\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1891\u001b[0m         nn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(\n\u001b[0;32m   1892\u001b[0m             amp\u001b[39m.\u001b[39mmaster_params(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer),\n\u001b[0;32m   1893\u001b[0m             args\u001b[39m.\u001b[39mmax_grad_norm,\n\u001b[0;32m   1894\u001b[0m         )\n\u001b[0;32m   1895\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1896\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maccelerator\u001b[39m.\u001b[39;49mclip_grad_norm_(\n\u001b[0;32m   1897\u001b[0m             model\u001b[39m.\u001b[39;49mparameters(),\n\u001b[0;32m   1898\u001b[0m             args\u001b[39m.\u001b[39;49mmax_grad_norm,\n\u001b[0;32m   1899\u001b[0m         )\n\u001b[0;32m   1901\u001b[0m \u001b[39m# Optimizer step\u001b[39;00m\n\u001b[0;32m   1902\u001b[0m optimizer_was_run \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Ghiki\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\accelerate\\accelerator.py:2121\u001b[0m, in \u001b[0;36mAccelerator.clip_grad_norm_\u001b[1;34m(self, parameters, max_norm, norm_type)\u001b[0m\n\u001b[0;32m   2119\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   2120\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munscale_gradients()\n\u001b[1;32m-> 2121\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mutils\u001b[39m.\u001b[39;49mclip_grad_norm_(parameters, max_norm, norm_type\u001b[39m=\u001b[39;49mnorm_type)\n",
      "File \u001b[1;32mc:\\Users\\Ghiki\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\utils\\clip_grad.py:46\u001b[0m, in \u001b[0;36mclip_grad_norm_\u001b[1;34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mtensor(\u001b[39m0.\u001b[39m)\n\u001b[0;32m     44\u001b[0m first_device \u001b[39m=\u001b[39m grads[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdevice\n\u001b[0;32m     45\u001b[0m grouped_grads: Dict[Tuple[torch\u001b[39m.\u001b[39mdevice, torch\u001b[39m.\u001b[39mdtype], List[List[Tensor]]] \\\n\u001b[1;32m---> 46\u001b[0m     \u001b[39m=\u001b[39m _group_tensors_by_device_and_dtype([[g\u001b[39m.\u001b[39;49mdetach() \u001b[39mfor\u001b[39;49;00m g \u001b[39min\u001b[39;49;00m grads]])  \u001b[39m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[39mif\u001b[39;00m norm_type \u001b[39m==\u001b[39m inf:\n\u001b[0;32m     49\u001b[0m     norms \u001b[39m=\u001b[39m [g\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mabs()\u001b[39m.\u001b[39mmax()\u001b[39m.\u001b[39mto(first_device) \u001b[39mfor\u001b[39;00m g \u001b[39min\u001b[39;00m grads]\n",
      "File \u001b[1;32mc:\\Users\\Ghiki\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\utils\\clip_grad.py:46\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     43\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mtensor(\u001b[39m0.\u001b[39m)\n\u001b[0;32m     44\u001b[0m first_device \u001b[39m=\u001b[39m grads[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdevice\n\u001b[0;32m     45\u001b[0m grouped_grads: Dict[Tuple[torch\u001b[39m.\u001b[39mdevice, torch\u001b[39m.\u001b[39mdtype], List[List[Tensor]]] \\\n\u001b[1;32m---> 46\u001b[0m     \u001b[39m=\u001b[39m _group_tensors_by_device_and_dtype([[g\u001b[39m.\u001b[39;49mdetach() \u001b[39mfor\u001b[39;00m g \u001b[39min\u001b[39;00m grads]])  \u001b[39m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[39mif\u001b[39;00m norm_type \u001b[39m==\u001b[39m inf:\n\u001b[0;32m     49\u001b[0m     norms \u001b[39m=\u001b[39m [g\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mabs()\u001b[39m.\u001b[39mmax()\u001b[39m.\u001b[39mto(first_device) \u001b[39mfor\u001b[39;00m g \u001b[39min\u001b[39;00m grads]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_5 (Dense)             (None, 200)               40200     \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 200)               0         \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 200)               800       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 100)               20100     \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, 100)               400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 100)               0         \n",
      "                                                                 \n",
      " batch_normalization_7 (Bat  (None, 100)               400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 1)                 0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 72101 (281.64 KB)\n",
      "Trainable params: 71301 (278.52 KB)\n",
      "Non-trainable params: 800 (3.12 KB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/1000\n",
      "116/116 [==============================] - 1s 5ms/step - loss: 0.6459 - accuracy: 0.6559 - recall: 0.6384 - precision: 0.6642 - auc: 0.7139 - f1_score: 0.6500 - val_loss: 0.5131 - val_accuracy: 0.7632 - val_recall: 0.7174 - val_precision: 0.7803 - val_auc: 0.8331 - val_f1_score: 0.7459\n",
      "Epoch 2/1000\n",
      "116/116 [==============================] - 0s 4ms/step - loss: 0.5455 - accuracy: 0.7284 - recall: 0.7073 - precision: 0.7407 - auc: 0.8025 - f1_score: 0.7223 - val_loss: 0.4601 - val_accuracy: 0.7869 - val_recall: 0.7208 - val_precision: 0.8214 - val_auc: 0.8668 - val_f1_score: 0.7662\n",
      "Epoch 3/1000\n",
      "116/116 [==============================] - 0s 4ms/step - loss: 0.4827 - accuracy: 0.7668 - recall: 0.7371 - precision: 0.7858 - auc: 0.8491 - f1_score: 0.7599 - val_loss: 0.4261 - val_accuracy: 0.8074 - val_recall: 0.7693 - val_precision: 0.8249 - val_auc: 0.8877 - val_f1_score: 0.7945\n",
      "Epoch 4/1000\n",
      "116/116 [==============================] - 0s 3ms/step - loss: 0.4632 - accuracy: 0.7828 - recall: 0.7680 - precision: 0.7935 - auc: 0.8623 - f1_score: 0.7790 - val_loss: 0.4092 - val_accuracy: 0.8242 - val_recall: 0.7826 - val_precision: 0.8461 - val_auc: 0.8970 - val_f1_score: 0.8117\n",
      "Epoch 5/1000\n",
      "116/116 [==============================] - 0s 4ms/step - loss: 0.4293 - accuracy: 0.8040 - recall: 0.7817 - precision: 0.8202 - auc: 0.8826 - f1_score: 0.7999 - val_loss: 0.4067 - val_accuracy: 0.8193 - val_recall: 0.7616 - val_precision: 0.8529 - val_auc: 0.8987 - val_f1_score: 0.8026\n",
      "Epoch 6/1000\n",
      "116/116 [==============================] - 0s 4ms/step - loss: 0.4100 - accuracy: 0.8164 - recall: 0.7894 - precision: 0.8363 - auc: 0.8935 - f1_score: 0.8110 - val_loss: 0.3932 - val_accuracy: 0.8306 - val_recall: 0.7837 - val_precision: 0.8575 - val_auc: 0.9040 - val_f1_score: 0.8164\n",
      "Epoch 7/1000\n",
      "116/116 [==============================] - 0s 3ms/step - loss: 0.3919 - accuracy: 0.8237 - recall: 0.8063 - precision: 0.8371 - auc: 0.9039 - f1_score: 0.8208 - val_loss: 0.4026 - val_accuracy: 0.8193 - val_recall: 0.7815 - val_precision: 0.8379 - val_auc: 0.8994 - val_f1_score: 0.8065\n",
      "Epoch 8/1000\n",
      "116/116 [==============================] - 0s 3ms/step - loss: 0.3844 - accuracy: 0.8216 - recall: 0.8098 - precision: 0.8310 - auc: 0.9072 - f1_score: 0.8198 - val_loss: 0.3905 - val_accuracy: 0.8258 - val_recall: 0.7837 - val_precision: 0.8483 - val_auc: 0.9058 - val_f1_score: 0.8135\n",
      "Epoch 9/1000\n",
      "116/116 [==============================] - 0s 3ms/step - loss: 0.3566 - accuracy: 0.8423 - recall: 0.8197 - precision: 0.8601 - auc: 0.9205 - f1_score: 0.8392 - val_loss: 0.3918 - val_accuracy: 0.8279 - val_recall: 0.7605 - val_precision: 0.8710 - val_auc: 0.9077 - val_f1_score: 0.8105\n",
      "Epoch 10/1000\n",
      "116/116 [==============================] - 0s 3ms/step - loss: 0.3451 - accuracy: 0.8503 - recall: 0.8383 - precision: 0.8604 - auc: 0.9259 - f1_score: 0.8488 - val_loss: 0.3900 - val_accuracy: 0.8263 - val_recall: 0.7848 - val_precision: 0.8484 - val_auc: 0.9063 - val_f1_score: 0.8144\n",
      "Epoch 11/1000\n",
      "116/116 [==============================] - 0s 4ms/step - loss: 0.3336 - accuracy: 0.8508 - recall: 0.8323 - precision: 0.8658 - auc: 0.9309 - f1_score: 0.8479 - val_loss: 0.3970 - val_accuracy: 0.8279 - val_recall: 0.7837 - val_precision: 0.8523 - val_auc: 0.9077 - val_f1_score: 0.8149\n",
      "Epoch 12/1000\n",
      "116/116 [==============================] - 0s 4ms/step - loss: 0.3165 - accuracy: 0.8595 - recall: 0.8479 - precision: 0.8694 - auc: 0.9379 - f1_score: 0.8576 - val_loss: 0.3932 - val_accuracy: 0.8296 - val_recall: 0.8002 - val_precision: 0.8430 - val_auc: 0.9064 - val_f1_score: 0.8187\n",
      "Epoch 13/1000\n",
      "116/116 [==============================] - 0s 3ms/step - loss: 0.3119 - accuracy: 0.8620 - recall: 0.8533 - precision: 0.8698 - auc: 0.9398 - f1_score: 0.8610 - val_loss: 0.3854 - val_accuracy: 0.8360 - val_recall: 0.8035 - val_precision: 0.8525 - val_auc: 0.9112 - val_f1_score: 0.8244\n",
      "Epoch 14/1000\n",
      "116/116 [==============================] - 0s 4ms/step - loss: 0.2978 - accuracy: 0.8744 - recall: 0.8659 - precision: 0.8822 - auc: 0.9454 - f1_score: 0.8743 - val_loss: 0.4044 - val_accuracy: 0.8306 - val_recall: 0.8002 - val_precision: 0.8450 - val_auc: 0.9047 - val_f1_score: 0.8208\n",
      "Epoch 15/1000\n",
      "116/116 [==============================] - 0s 3ms/step - loss: 0.2891 - accuracy: 0.8785 - recall: 0.8694 - precision: 0.8867 - auc: 0.9483 - f1_score: 0.8774 - val_loss: 0.3965 - val_accuracy: 0.8403 - val_recall: 0.8113 - val_precision: 0.8547 - val_auc: 0.9106 - val_f1_score: 0.8307\n",
      "Epoch 16/1000\n",
      "116/116 [==============================] - 0s 3ms/step - loss: 0.2818 - accuracy: 0.8789 - recall: 0.8686 - precision: 0.8881 - auc: 0.9511 - f1_score: 0.8771 - val_loss: 0.3919 - val_accuracy: 0.8350 - val_recall: 0.7936 - val_precision: 0.8580 - val_auc: 0.9129 - val_f1_score: 0.8230\n",
      "Epoch 17/1000\n",
      "116/116 [==============================] - 0s 3ms/step - loss: 0.2698 - accuracy: 0.8831 - recall: 0.8723 - precision: 0.8927 - auc: 0.9553 - f1_score: 0.8819 - val_loss: 0.4078 - val_accuracy: 0.8366 - val_recall: 0.8157 - val_precision: 0.8446 - val_auc: 0.9066 - val_f1_score: 0.8282\n",
      "Epoch 18/1000\n",
      "116/116 [==============================] - 0s 3ms/step - loss: 0.2656 - accuracy: 0.8833 - recall: 0.8828 - precision: 0.8849 - auc: 0.9567 - f1_score: 0.8827 - val_loss: 0.4023 - val_accuracy: 0.8285 - val_recall: 0.7870 - val_precision: 0.8508 - val_auc: 0.9089 - val_f1_score: 0.8151\n",
      "Epoch 19/1000\n",
      "116/116 [==============================] - 0s 3ms/step - loss: 0.2453 - accuracy: 0.8957 - recall: 0.8871 - precision: 0.9038 - auc: 0.9632 - f1_score: 0.8948 - val_loss: 0.4022 - val_accuracy: 0.8425 - val_recall: 0.8212 - val_precision: 0.8513 - val_auc: 0.9133 - val_f1_score: 0.8342\n",
      "Epoch 20/1000\n",
      "116/116 [==============================] - 0s 3ms/step - loss: 0.2501 - accuracy: 0.8965 - recall: 0.8871 - precision: 0.9053 - auc: 0.9615 - f1_score: 0.8957 - val_loss: 0.4084 - val_accuracy: 0.8420 - val_recall: 0.8013 - val_precision: 0.8653 - val_auc: 0.9105 - val_f1_score: 0.8305\n",
      "Epoch 21/1000\n",
      "116/116 [==============================] - 0s 3ms/step - loss: 0.2378 - accuracy: 0.9029 - recall: 0.8983 - precision: 0.9076 - auc: 0.9654 - f1_score: 0.9013 - val_loss: 0.4310 - val_accuracy: 0.8231 - val_recall: 0.7594 - val_precision: 0.8622 - val_auc: 0.9030 - val_f1_score: 0.8046\n",
      "Epoch 22/1000\n",
      "116/116 [==============================] - 0s 4ms/step - loss: 0.2290 - accuracy: 0.9076 - recall: 0.8999 - precision: 0.9149 - auc: 0.9678 - f1_score: 0.9063 - val_loss: 0.4195 - val_accuracy: 0.8425 - val_recall: 0.8135 - val_precision: 0.8570 - val_auc: 0.9120 - val_f1_score: 0.8347\n",
      "Epoch 23/1000\n",
      "116/116 [==============================] - 0s 4ms/step - loss: 0.2300 - accuracy: 0.9015 - recall: 0.8951 - precision: 0.9078 - auc: 0.9676 - f1_score: 0.9002 - val_loss: 0.4129 - val_accuracy: 0.8398 - val_recall: 0.8190 - val_precision: 0.8480 - val_auc: 0.9119 - val_f1_score: 0.8321\n",
      "Epoch 24/1000\n",
      "116/116 [==============================] - 0s 4ms/step - loss: 0.2130 - accuracy: 0.9127 - recall: 0.9123 - precision: 0.9140 - auc: 0.9722 - f1_score: 0.9128 - val_loss: 0.4276 - val_accuracy: 0.8403 - val_recall: 0.7870 - val_precision: 0.8738 - val_auc: 0.9127 - val_f1_score: 0.8267\n",
      "Epoch 25/1000\n",
      "116/116 [==============================] - 0s 4ms/step - loss: 0.2149 - accuracy: 0.9133 - recall: 0.9088 - precision: 0.9179 - auc: 0.9716 - f1_score: 0.9129 - val_loss: 0.4202 - val_accuracy: 0.8403 - val_recall: 0.8124 - val_precision: 0.8538 - val_auc: 0.9122 - val_f1_score: 0.8325\n",
      "Epoch 26/1000\n",
      "116/116 [==============================] - 0s 3ms/step - loss: 0.1984 - accuracy: 0.9166 - recall: 0.9088 - precision: 0.9242 - auc: 0.9759 - f1_score: 0.9160 - val_loss: 0.4496 - val_accuracy: 0.8376 - val_recall: 0.8234 - val_precision: 0.8410 - val_auc: 0.9086 - val_f1_score: 0.8315\n",
      "Epoch 27/1000\n",
      "116/116 [==============================] - 0s 3ms/step - loss: 0.1923 - accuracy: 0.9220 - recall: 0.9238 - precision: 0.9213 - auc: 0.9772 - f1_score: 0.9216 - val_loss: 0.4540 - val_accuracy: 0.8376 - val_recall: 0.7748 - val_precision: 0.8786 - val_auc: 0.9102 - val_f1_score: 0.8225\n",
      "Epoch 28/1000\n",
      "116/116 [==============================] - 0s 3ms/step - loss: 0.2026 - accuracy: 0.9168 - recall: 0.9093 - precision: 0.9240 - auc: 0.9746 - f1_score: 0.9168 - val_loss: 0.4288 - val_accuracy: 0.8339 - val_recall: 0.7969 - val_precision: 0.8534 - val_auc: 0.9134 - val_f1_score: 0.8240\n",
      "Epoch 29/1000\n",
      "116/116 [==============================] - 0s 3ms/step - loss: 0.1861 - accuracy: 0.9265 - recall: 0.9219 - precision: 0.9312 - auc: 0.9787 - f1_score: 0.9260 - val_loss: 0.4466 - val_accuracy: 0.8387 - val_recall: 0.8510 - val_precision: 0.8246 - val_auc: 0.9111 - val_f1_score: 0.8361\n",
      "Epoch 30/1000\n",
      "116/116 [==============================] - 0s 3ms/step - loss: 0.1832 - accuracy: 0.9270 - recall: 0.9241 - precision: 0.9303 - auc: 0.9792 - f1_score: 0.9268 - val_loss: 0.4261 - val_accuracy: 0.8414 - val_recall: 0.8190 - val_precision: 0.8509 - val_auc: 0.9154 - val_f1_score: 0.8341\n",
      "Epoch 31/1000\n",
      "116/116 [==============================] - 0s 4ms/step - loss: 0.1692 - accuracy: 0.9322 - recall: 0.9284 - precision: 0.9362 - auc: 0.9823 - f1_score: 0.9316 - val_loss: 0.4734 - val_accuracy: 0.8274 - val_recall: 0.7693 - val_precision: 0.8626 - val_auc: 0.9067 - val_f1_score: 0.8118\n",
      "Epoch 32/1000\n",
      "116/116 [==============================] - 0s 4ms/step - loss: 0.1732 - accuracy: 0.9318 - recall: 0.9284 - precision: 0.9354 - auc: 0.9813 - f1_score: 0.9319 - val_loss: 0.4695 - val_accuracy: 0.8355 - val_recall: 0.8168 - val_precision: 0.8419 - val_auc: 0.9055 - val_f1_score: 0.8288\n",
      "Epoch 33/1000\n",
      "116/116 [==============================] - 0s 4ms/step - loss: 0.1670 - accuracy: 0.9319 - recall: 0.9305 - precision: 0.9338 - auc: 0.9829 - f1_score: 0.9315 - val_loss: 0.4464 - val_accuracy: 0.8387 - val_recall: 0.8322 - val_precision: 0.8368 - val_auc: 0.9118 - val_f1_score: 0.8334\n",
      "58/58 [==============================] - 0s 1ms/step - loss: 0.4464 - accuracy: 0.8387 - recall: 0.8322 - precision: 0.8368 - auc: 0.9118 - f1_score: 0.8299\n",
      "Accuracy: 0.8387270569801331\n",
      "Recall: 0.8322295546531677\n",
      "Precision: 0.8368479609489441\n",
      "AUC: 0.9117580056190491\n",
      "F1 Score: 0.829898476600647\n"
     ]
    }
   ],
   "source": [
    "'''Dense Network'''\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dense, Activation, Dropout, BatchNormalization, SimpleRNN\n",
    "from tensorflow.keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from tensorflow.keras.preprocessing import sequence, text\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2*(precision*recall)/(precision+recall+K.epsilon())\n",
    "    return f1_val\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(200, input_dim=x_train_scaled.shape[1], activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "# compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy','Recall','Precision','AUC',f1_score])\n",
    "model.summary()\n",
    "\n",
    "stop_early = EarlyStopping(monitor='val_loss', patience=20)\n",
    "model.fit(x_train_scaled, y=y_train, batch_size=64, \n",
    "              epochs=1000, verbose=1, \n",
    "              validation_data=(x_test_scaled, y_test),\n",
    "              callbacks=[stop_early])\n",
    "\n",
    "metrics=model.evaluate(x_test_scaled, y_test)\n",
    "\n",
    "loss, accuracy, recall, precision, auc, f1 = metrics\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"AUC: {auc}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7535059331175836\n",
      "Recall: 0.7406181015452539\n",
      "Precision: 0.7513997760358343\n",
      "AUC: 0.8421592803718296\n",
      "F1 Score: 0.7459699833240689\n"
     ]
    }
   ],
   "source": [
    "'''Random Forest'''\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, roc_auc_score, f1_score\n",
    "\n",
    "# Train Random Forest model\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(x_train_scaled, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = model.predict(x_test_scaled)\n",
    "y_proba = model.predict_proba(x_test_scaled)[:, 1]  # Get the probability of class 1\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_proba)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"AUC: {auc}\")\n",
    "print(f\"F1 Score: {f1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
