{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ghiki\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding, AutoTokenizer, set_seed\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.special import softmax\n",
    "import argparse\n",
    "import logging\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import wandb\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score, confusion_matrix\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
    "from transformers import AutoConfig, RobertaModel, RobertaForSequenceClassification\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "import pandas as pd,os\n",
    "import torch\n",
    "from statistics import mode\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline, Trainer\n",
    "from datasets import Dataset, load_metric\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "import pickle\n",
    "from transformers import RobertaConfig, RobertaModel\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoConfig, RobertaModel, RobertaForSequenceClassification\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from typing import Optional, Union, Tuple\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import EarlyStoppingCallback,AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding, AutoTokenizer, set_seed\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.special import softmax\n",
    "import argparse\n",
    "import logging\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import wandb\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score, confusion_matrix\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from textstat.textstat import textstatistics\n",
    "import pandas as pd\n",
    "import language_tool_python\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score, confusion_matrix\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(nn.Module):\n",
    "    \"\"\"Head for sentence-level classification tasks.\"\"\"\n",
    "\n",
    "    def __init__(self, config, num_extra_dims):\n",
    "        super().__init__()\n",
    "        total_dims = config.hidden_size+num_extra_dims\n",
    "        self.dense = nn.Linear(total_dims, total_dims)\n",
    "        classifier_dropout = (\n",
    "            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n",
    "        )\n",
    "        self.dropout = nn.Dropout(classifier_dropout)\n",
    "        self.out_proj = nn.Linear(total_dims, config.num_labels)\n",
    "\n",
    "    def forward(self, features, **kwargs):\n",
    "        x = self.dropout(features)\n",
    "        x = self.dense(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x\n",
    "\n",
    "class CustomSequenceClassification(RobertaForSequenceClassification):\n",
    "\n",
    "    def __init__(self, config, num_extra_dims):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.config = config\n",
    "\n",
    "        # might need to rename this depending on the model\n",
    "        self.roberta =  RobertaModel(config)\n",
    "        self.classifier = ClassificationHead(config, num_extra_dims)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        extra_data: Optional[torch.FloatTensor] = None,\n",
    "        token_type_ids: Optional[torch.LongTensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, SequenceClassifierOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.roberta(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        # sequence_output will be (batch_size, seq_length, hidden_size)\n",
    "        sequence_output = outputs[0]\n",
    "\n",
    "        # additional data should be (batch_size, num_extra_dims)\n",
    "        cls_embedding = sequence_output[:, 0, :]\n",
    "\n",
    "        output = torch.cat((cls_embedding, extra_data), dim=-1)\n",
    "\n",
    "        logits = self.classifier(output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = nn.MSELoss()\n",
    "                if self.num_labels == 1:\n",
    "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
    "                else:\n",
    "                    loss = loss_fct(logits, labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = nn.CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = nn.BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 34272 entries, 0 to 34271\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    34272 non-null  object\n",
      " 1   id      34272 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 535.6+ KB\n",
      "None\n",
      "\n",
      "Processing with bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 34272/34272 [00:09<00:00, 3704.91 examples/s]\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████| 4284/4284 [02:49<00:00, 25.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing with Hello-SimpleAI/chatgpt-detector-roberta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 34272/34272 [00:08<00:00, 4148.24 examples/s]\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████| 4284/4284 [02:47<00:00, 25.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing with roberta-base-openai-detector\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 34272/34272 [00:08<00:00, 3889.33 examples/s]\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████| 4284/4284 [02:45<00:00, 25.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing with roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 34272/34272 [00:08<00:00, 3987.55 examples/s]\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████| 4284/4284 [02:46<00:00, 25.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing with distilbert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 34272/34272 [00:11<00:00, 2974.55 examples/s]\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████| 4284/4284 [01:29<00:00, 47.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing with google/electra-base-discriminator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 34272/34272 [00:13<00:00, 2590.25 examples/s]\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████| 4284/4284 [02:49<00:00, 25.29it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['text', 'label'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Ghiki\\Documents\\GitHub\\Multidomain--Multimodal-and-Multilingual-Machine-Generated-Text-Detection\\09 test\\1 prediction.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Ghiki/Documents/GitHub/Multidomain--Multimodal-and-Multilingual-Machine-Generated-Text-Detection/09%20test/1%20prediction.ipynb#W2sZmlsZQ%3D%3D?line=102'>103</a>\u001b[0m df_multidomain_extended_extraData\u001b[39m=\u001b[39mpd\u001b[39m.\u001b[39mread_csv(\u001b[39m'\u001b[39m\u001b[39mdatasets/features_df_test_A.csv\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Ghiki/Documents/GitHub/Multidomain--Multimodal-and-Multilingual-Machine-Generated-Text-Detection/09%20test/1%20prediction.ipynb#W2sZmlsZQ%3D%3D?line=103'>104</a>\u001b[0m reduced_columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mword_count\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mavg_sentence_length\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mavg_word_length\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mgunning_fog_index\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mgrammatical_errors\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Ghiki/Documents/GitHub/Multidomain--Multimodal-and-Multilingual-Machine-Generated-Text-Detection/09%20test/1%20prediction.ipynb#W2sZmlsZQ%3D%3D?line=104'>105</a>\u001b[0m df_multidomain_extraData\u001b[39m=\u001b[39mdf_multidomain_extended_extraData[reduced_columns]\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Ghiki/Documents/GitHub/Multidomain--Multimodal-and-Multilingual-Machine-Generated-Text-Detection/09%20test/1%20prediction.ipynb#W2sZmlsZQ%3D%3D?line=106'>107</a>\u001b[0m \u001b[39m# df_multidomain_extraData=df_multidomain.drop(['label'],axis=1)\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Ghiki/Documents/GitHub/Multidomain--Multimodal-and-Multilingual-Machine-Generated-Text-Detection/09%20test/1%20prediction.ipynb#W2sZmlsZQ%3D%3D?line=107'>108</a>\u001b[0m \u001b[39m# df_multidomain_extraData=df_multidomain_extraData.drop(['text'],axis=1)\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Ghiki/Documents/GitHub/Multidomain--Multimodal-and-Multilingual-Machine-Generated-Text-Detection/09%20test/1%20prediction.ipynb#W2sZmlsZQ%3D%3D?line=108'>109</a>\u001b[0m \n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Ghiki/Documents/GitHub/Multidomain--Multimodal-and-Multilingual-Machine-Generated-Text-Detection/09%20test/1%20prediction.ipynb#W2sZmlsZQ%3D%3D?line=109'>110</a>\u001b[0m \u001b[39m# df_multidomain_extended_extraData=df_multidomain_extended.drop(['label'],axis=1)\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Ghiki/Documents/GitHub/Multidomain--Multimodal-and-Multilingual-Machine-Generated-Text-Detection/09%20test/1%20prediction.ipynb#W2sZmlsZQ%3D%3D?line=110'>111</a>\u001b[0m \u001b[39m# df_multidomain_extended_extraData=df_multidomain_extended_extraData.drop(['text'],axis=1)\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Ghiki/Documents/GitHub/Multidomain--Multimodal-and-Multilingual-Machine-Generated-Text-Detection/09%20test/1%20prediction.ipynb#W2sZmlsZQ%3D%3D?line=112'>113</a>\u001b[0m ds_test \u001b[39m=\u001b[39m Dataset\u001b[39m.\u001b[39mfrom_dict({\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Ghiki/Documents/GitHub/Multidomain--Multimodal-and-Multilingual-Machine-Generated-Text-Detection/09%20test/1%20prediction.ipynb#W2sZmlsZQ%3D%3D?line=113'>114</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m\"\u001b[39m: test_texts[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mtolist(), \n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Ghiki/Documents/GitHub/Multidomain--Multimodal-and-Multilingual-Machine-Generated-Text-Detection/09%20test/1%20prediction.ipynb#W2sZmlsZQ%3D%3D?line=114'>115</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mextra_data\u001b[39m\u001b[39m\"\u001b[39m: df_multidomain_extraData\u001b[39m.\u001b[39mvalues\u001b[39m.\u001b[39mtolist(),\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Ghiki/Documents/GitHub/Multidomain--Multimodal-and-Multilingual-Machine-Generated-Text-Detection/09%20test/1%20prediction.ipynb#W2sZmlsZQ%3D%3D?line=115'>116</a>\u001b[0m         \u001b[39m# \"labels\": df_multidomain['label'].tolist()\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Ghiki/Documents/GitHub/Multidomain--Multimodal-and-Multilingual-Machine-Generated-Text-Detection/09%20test/1%20prediction.ipynb#W2sZmlsZQ%3D%3D?line=116'>117</a>\u001b[0m     })\n",
      "File \u001b[1;32mc:\\Users\\Ghiki\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\frame.py:3902\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3900\u001b[0m     \u001b[39mif\u001b[39;00m is_iterator(key):\n\u001b[0;32m   3901\u001b[0m         key \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(key)\n\u001b[1;32m-> 3902\u001b[0m     indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49m_get_indexer_strict(key, \u001b[39m\"\u001b[39;49m\u001b[39mcolumns\u001b[39;49m\u001b[39m\"\u001b[39;49m)[\u001b[39m1\u001b[39m]\n\u001b[0;32m   3904\u001b[0m \u001b[39m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[0;32m   3905\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(indexer, \u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m) \u001b[39m==\u001b[39m \u001b[39mbool\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Ghiki\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6114\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[1;34m(self, key, axis_name)\u001b[0m\n\u001b[0;32m   6111\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   6112\u001b[0m     keyarr, indexer, new_indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[1;32m-> 6114\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_raise_if_missing(keyarr, indexer, axis_name)\n\u001b[0;32m   6116\u001b[0m keyarr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtake(indexer)\n\u001b[0;32m   6117\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(key, Index):\n\u001b[0;32m   6118\u001b[0m     \u001b[39m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Ghiki\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:6178\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[1;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[0;32m   6175\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNone of [\u001b[39m\u001b[39m{\u001b[39;00mkey\u001b[39m}\u001b[39;00m\u001b[39m] are in the [\u001b[39m\u001b[39m{\u001b[39;00maxis_name\u001b[39m}\u001b[39;00m\u001b[39m]\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   6177\u001b[0m not_found \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[39m.\u001b[39mnonzero()[\u001b[39m0\u001b[39m]]\u001b[39m.\u001b[39munique())\n\u001b[1;32m-> 6178\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mnot_found\u001b[39m}\u001b[39;00m\u001b[39m not in index\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['text', 'label'] not in index\""
     ]
    }
   ],
   "source": [
    "\n",
    "MODEL1 = 'bert-base-uncased'\n",
    "MODEL2 = 'Hello-SimpleAI/chatgpt-detector-roberta'\n",
    "MODEL3 = 'roberta-base-openai-detector'\n",
    "MODEL4 = 'roberta-base'\n",
    "MODEL5 = 'distilbert-base-uncased'\n",
    "MODEL6 = 'google/electra-base-discriminator'\n",
    "\n",
    "MODEL8='multidomain-roberta-base'\n",
    "MODEL9='multidomain-extended-roberta-base'\n",
    "\n",
    "MODEL_PATH1='SavedModels_A/optimized-bert-base-uncased-22.5k'\n",
    "MODEL_PATH2='SavedModels_A/optimized-chatgpt-detector-roberta-17.5k'\n",
    "MODEL_PATH3='SavedModels_A/optimized-roberta-base-openai-detector-12k'\n",
    "MODEL_PATH4='SavedModels_A/optimized-roberta-base-0.5k'\n",
    "MODEL_PATH5='SavedModels_A/optimized-distilbert-base-uncased-15k'\n",
    "MODEL_PATH6='SavedModels_A/optimized-electra-base-discriminator-6k'\n",
    "\n",
    "MODEL_PATH8='SavedModels_A/optimized-multidomain-roberta-base-1k'\n",
    "MODEL_PATH9='SavedModels_A/optimized-multidomain-extended-robera-base-0.8k'\n",
    "\n",
    "'''Preparing data'''\n",
    "\n",
    "df = pd.read_json('datasets/subtaskA_test_monolingual.jsonl', lines=True)\n",
    "\n",
    "# df = df.rename(columns={'source': 'language'})\n",
    "# non_language_sources = ['wikihow', 'wikipedia', 'reddit', 'arxiv', 'peerread']\n",
    "# df['language'] = df['language'].replace(non_language_sources, 'english')\n",
    "\n",
    "print(f'Original dataset')\n",
    "print(df.info())\n",
    "\n",
    "# print(f'''\\n{df['label'].value_counts()}''')\n",
    "# print(f'''\\n{df['model'].value_counts()}''')\n",
    "# print(f'''\\n{df['language'].value_counts()}''')\n",
    "\n",
    "# df = df[['text', 'label']]\n",
    "test_df=df\n",
    "\n",
    "test_texts = test_df['text'].tolist()\n",
    "\n",
    "def getPrediction(model_path):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    id2label = {0: \"human\", 1: \"machine\"}\n",
    "    label2id = {\"human\": 0, \"machine\": 1}\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_path, num_labels=len(label2id), id2label=id2label, label2id=label2id\n",
    "        )\n",
    "\n",
    "    test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "    def preprocess_function(examples, **fn_kwargs):\n",
    "        return fn_kwargs['tokenizer'](examples[\"text\"], truncation=True)\n",
    "\n",
    "    tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True,  fn_kwargs={'tokenizer': tokenizer})\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    # create Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        # compute_metrics=compute_metrics,\n",
    "    )\n",
    "    # get logits from predictions and evaluate results using classification report\n",
    "    predictions = trainer.predict(tokenized_test_dataset)\n",
    "    preds = np.argmax(predictions.predictions, axis=-1)\n",
    "    probs = softmax(predictions.predictions, axis=-1)\n",
    "    label_specific_probs = probs[:, 1]  # This extracts the probability for label 1\n",
    "    \n",
    "    return list(preds),list(label_specific_probs)\n",
    "\n",
    "def getPredictionMultidomain(model_path,num_extra_dims,test_data):\n",
    "    config = RobertaConfig.from_pretrained(model_path)\n",
    "    model = CustomSequenceClassification(config, num_extra_dims)\n",
    "    model.load_state_dict(torch.load(model_path+'/pytorch_model.bin'))\n",
    "    trainer = Trainer(model=model)\n",
    "    # get logits from predictions and evaluate results using classification report\n",
    "    predictions = trainer.predict(test_data)\n",
    "    preds = np.argmax(predictions.predictions, axis=-1)\n",
    "    probs = softmax(predictions.predictions, axis=-1)\n",
    "    label_specific_probs = probs[:, 1]  # This extracts the probability for label 1\n",
    "    \n",
    "    return list(preds),list(label_specific_probs)\n",
    "\n",
    "print(f'\\nProcessing with {MODEL1}')\n",
    "labels1,scores1=getPrediction(MODEL_PATH1)\n",
    "\n",
    "print(f'\\nProcessing with {MODEL2}')\n",
    "labels2,scores2=getPrediction(MODEL_PATH2)\n",
    "\n",
    "print(f'\\nProcessing with {MODEL3}')\n",
    "labels3,scores3=getPrediction(MODEL_PATH3)\n",
    "\n",
    "print(f'\\nProcessing with {MODEL4}')\n",
    "labels4,scores4=getPrediction(MODEL_PATH4)\n",
    "\n",
    "print(f'\\nProcessing with {MODEL5}')\n",
    "labels5,scores5=getPrediction(MODEL_PATH5)\n",
    "\n",
    "print(f'\\nProcessing with {MODEL6}')\n",
    "labels6,scores6=getPrediction(MODEL_PATH6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 34272/34272 [00:33<00:00, 1027.61 examples/s]\n",
      "Map: 100%|██████████| 34272/34272 [00:01<00:00, 30512.07 examples/s]\n",
      "Map: 100%|██████████| 34272/34272 [00:33<00:00, 1031.94 examples/s]\n",
      "Map: 100%|██████████| 34272/34272 [00:01<00:00, 29362.81 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing with multidomain-roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4284/4284 [02:49<00:00, 25.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing with multidomain-extended-roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4284/4284 [02:47<00:00, 25.61it/s]\n"
     ]
    }
   ],
   "source": [
    "df_multidomain_extended_extraData=pd.read_csv('datasets/features_df_test_A.csv')\n",
    "reduced_columns=['word_count','avg_sentence_length','avg_word_length','gunning_fog_index','grammatical_errors']\n",
    "df_multidomain_extraData=df_multidomain_extended_extraData[reduced_columns]\n",
    "\n",
    "# df_multidomain_extraData=df_multidomain.drop(['label'],axis=1)\n",
    "# df_multidomain_extraData=df_multidomain_extraData.drop(['text'],axis=1)\n",
    "\n",
    "# df_multidomain_extended_extraData=df_multidomain_extended.drop(['label'],axis=1)\n",
    "# df_multidomain_extended_extraData=df_multidomain_extended_extraData.drop(['text'],axis=1)\n",
    "\n",
    "ds_test = Dataset.from_dict({\n",
    "        \"text\": test_df['text'].tolist(), \n",
    "        \"extra_data\": df_multidomain_extraData.values.tolist(),\n",
    "        # \"labels\": df_multidomain['label'].tolist()\n",
    "    })\n",
    "\n",
    "ds_test_extended = Dataset.from_dict({\n",
    "        \"text\": test_df['text'].tolist(), \n",
    "        \"extra_data\": df_multidomain_extended_extraData.values.tolist(),\n",
    "        # \"labels\": df_multidomain_extended['label'].tolist()\n",
    "    })\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "tokenized_ds_test = ds_test.map(lambda x: tokenizer(x[\"text\"], padding='max_length', truncation=True))\n",
    "tokenized_ds_test = tokenized_ds_test.map(lambda x: {'extra_data': x['extra_data']})\n",
    "tokenized_ds_test_extended = ds_test_extended.map(lambda x: tokenizer(x[\"text\"], padding='max_length', truncation=True))\n",
    "tokenized_ds_test_extended = tokenized_ds_test_extended.map(lambda x: {'extra_data': x['extra_data']})\n",
    "\n",
    "print(f'\\nProcessing with {MODEL8}')\n",
    "labels8,scores8=getPredictionMultidomain(MODEL_PATH8,5,tokenized_ds_test)\n",
    "\n",
    "print(f'\\nProcessing with {MODEL9}')\n",
    "labels9,scores9=getPredictionMultidomain(MODEL_PATH9,9,tokenized_ds_test_extended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df=pd.read_csv('datasets/features_df_test_A.csv')\n",
    "best_features=['word_count', 'avg_sentence_length', 'avg_word_length', 'gunning_fog_index', 'grammatical_errors']\n",
    "feature_df=feature_df[best_features]\n",
    "\n",
    "with open('SavedModels_A/optimized-rf-features-20k.pkl', 'rb') as file:\n",
    "    randomForest = pickle.load(file)\n",
    "MODEL7='Features Model'\n",
    "labels7=randomForest.predict(feature_df)\n",
    "probabilities = randomForest.predict_proba(feature_df)\n",
    "scores7 = probabilities.max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_methods(df, models):\n",
    "    \n",
    "    majority_labels = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        label_counts = {0: 0, 1: 0}\n",
    "        score_sums = {0: 0.0, 1: 0.0}\n",
    "\n",
    "        for i, model in enumerate(models):\n",
    "            label = row[f'Labels_{model}']\n",
    "            score = row[f'Scores_{model}']\n",
    "            label_counts[label] += 1\n",
    "            score_sums[label] += score\n",
    "\n",
    "        # Majority Voting\n",
    "        majority_label = 0 if label_counts[0] > label_counts[1] else 1\n",
    "        majority_labels.append(majority_label)\n",
    "        \n",
    "    return majority_labels\n",
    "\n",
    "models = [MODEL3, MODEL4, MODEL6, MODEL7, MODEL8, MODEL9]\n",
    "\n",
    "df_reduced = pd.DataFrame({\n",
    "    f'Labels_{MODEL3}': labels3,\n",
    "    f'Scores_{MODEL3}': scores3,\n",
    "    f'Labels_{MODEL4}': labels4,\n",
    "    f'Scores_{MODEL4}': scores4,\n",
    "    f'Labels_{MODEL6}': labels6,\n",
    "    f'Scores_{MODEL6}': scores6,\n",
    "    f'Labels_{MODEL7}': labels7,\n",
    "    f'Scores_{MODEL7}': scores7,\n",
    "    f'Labels_{MODEL8}': labels8,\n",
    "    f'Scores_{MODEL8}': scores8,\n",
    "    f'Labels_{MODEL9}': labels9,\n",
    "    f'Scores_{MODEL9}': scores9,\n",
    "})\n",
    "\n",
    "ensemble_results = ensemble_methods(df_reduced, models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ensemble_results),len(df['id'])\n",
    "\n",
    "new_df = pd.DataFrame({\n",
    "    'id': df['id'],\n",
    "    'label': ensemble_results\n",
    "})\n",
    "\n",
    "# Exporting to a jsonl file\n",
    "new_df.to_json('datasets/subtaskA_predictions.jsonl', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34267</th>\n",
       "      <td>34267</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34268</th>\n",
       "      <td>34268</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34269</th>\n",
       "      <td>34269</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34270</th>\n",
       "      <td>34270</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34271</th>\n",
       "      <td>34271</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34272 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id  label\n",
       "0          0      1\n",
       "1          1      1\n",
       "2          2      1\n",
       "3          3      1\n",
       "4          4      1\n",
       "...      ...    ...\n",
       "34267  34267      1\n",
       "34268  34268      1\n",
       "34269  34269      1\n",
       "34270  34270      0\n",
       "34271  34271      1\n",
       "\n",
       "[34272 rows x 2 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
