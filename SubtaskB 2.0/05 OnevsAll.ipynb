{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ghiki\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding, AutoTokenizer, set_seed\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.special import softmax\n",
    "import argparse\n",
    "import logging\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import wandb\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score, confusion_matrix\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
    "\n",
    "import pandas as pd,os\n",
    "import torch\n",
    "from statistics import mode\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline, Trainer\n",
    "from datasets import Dataset, load_metric\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples, **fn_kwargs):\n",
    "    return fn_kwargs['tokenizer'](examples[\"text\"], truncation=True)\n",
    "\n",
    "def get_data(train_path, test_path, random_seed):\n",
    "    \"\"\"\n",
    "    function to read dataframe with columns\n",
    "    \"\"\"\n",
    "\n",
    "    train_df = pd.read_json(train_path, lines=True)\n",
    "    test_df = pd.read_json(test_path, lines=True)\n",
    "   \n",
    "    return train_df, test_df\n",
    "\n",
    "def compute_metrics(p):\n",
    "    preds = np.argmax(p.predictions, axis=1)\n",
    "    labels = p.label_ids\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='binary')\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    auc = roc_auc_score(labels, preds)\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "\n",
    "    return {\n",
    "        'accuracy': acc,\n",
    "        'f1': f1,\n",
    "        'auc': auc,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'confusion_matrix': cm.tolist()\n",
    "    }\n",
    "\n",
    "def fine_tune(train_df, valid_df, checkpoints_path, id2label, label2id, model):\n",
    "\n",
    "    # pandas dataframe to huggingface Dataset\n",
    "    train_dataset = Dataset.from_pandas(train_df)\n",
    "    valid_dataset = Dataset.from_pandas(valid_df)\n",
    "    \n",
    "    # get tokenizer and model from huggingface\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model)     # put your model here\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "       model, num_labels=len(label2id), id2label=id2label, label2id=label2id    # put your model here\n",
    "    )\n",
    "    \n",
    "    # tokenize data for train/valid\n",
    "    tokenized_train_dataset = train_dataset.map(preprocess_function, batched=True, fn_kwargs={'tokenizer': tokenizer})\n",
    "    tokenized_valid_dataset = valid_dataset.map(preprocess_function, batched=True,  fn_kwargs={'tokenizer': tokenizer})\n",
    "    \n",
    "\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "\n",
    "    # create Trainer \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=checkpoints_path,\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=tokenized_train_dataset,\n",
    "        eval_dataset=tokenized_valid_dataset,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    \n",
    "    pokemonRojoFuego=trainer.predict(tokenized_valid_dataset)\n",
    "    print(pokemonRojoFuego)\n",
    "\n",
    "    # save best model\n",
    "    best_model_path = checkpoints_path+'/best/'\n",
    "    \n",
    "    if not os.path.exists(best_model_path):\n",
    "        os.makedirs(best_model_path)\n",
    "    \n",
    "\n",
    "    trainer.save_model(best_model_path)\n",
    "\n",
    "def test(test_df, model_path, id2label, label2id):\n",
    "    \n",
    "    # load tokenizer from saved model \n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "\n",
    "    # load best model\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "       model_path, num_labels=len(label2id), id2label=id2label, label2id=label2id\n",
    "    )\n",
    "            \n",
    "    test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "    tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True,  fn_kwargs={'tokenizer': tokenizer})\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    # create Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    # get logits from predictions and evaluate results using classification report\n",
    "    predictions = trainer.predict(tokenized_test_dataset)\n",
    "    prob_pred = softmax(predictions.predictions, axis=-1)\n",
    "    preds = np.argmax(predictions.predictions, axis=-1)\n",
    "    metric = evaluate.load(\"bstrai/classification_report\")\n",
    "    results = metric.compute(predictions=preds, references=predictions.label_ids)\n",
    "    \n",
    "    # return dictionary of classification report\n",
    "    return results, preds\n",
    "\n",
    "def process_and_balance_dataframes(df):\n",
    "    if 'label' not in df.columns:\n",
    "        raise ValueError(\"DataFrame must contain a 'label' column\")\n",
    "\n",
    "    unique_labels = df['label'].unique()\n",
    "    balanced_dfs = []\n",
    "\n",
    "    for label in np.sort(unique_labels):\n",
    "        \n",
    "        # Create a binary label DataFrame\n",
    "        binary_df = df.copy()\n",
    "        binary_df['label'] = binary_df['label'].apply(lambda x: 1 if x == label else 0)\n",
    "\n",
    "        # Balancing the dataset\n",
    "        rus = RandomUnderSampler(random_state=42)\n",
    "        X = binary_df.drop('label', axis=1)\n",
    "        y = binary_df['label']\n",
    "        X_res, y_res = rus.fit_resample(X, y)\n",
    "\n",
    "        # Creating a balanced DataFrame\n",
    "        balanced_df = pd.DataFrame(X_res, columns=X.columns)\n",
    "        balanced_df['label'] = y_res\n",
    "        balanced_dfs.append(balanced_df)\n",
    "        print(f'label: {label} - {balanced_df.shape}')\n",
    "\n",
    "    return balanced_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n",
      "3    178\n",
      "4    174\n",
      "1    172\n",
      "2    169\n",
      "0    159\n",
      "5    148\n",
      "Name: count, dtype: int64\n",
      "label: 0 - (318, 5)\n",
      "label: 1 - (344, 5)\n",
      "label: 2 - (338, 5)\n",
      "label: 3 - (356, 5)\n",
      "label: 4 - (348, 5)\n",
      "label: 5 - (296, 5)\n",
      "\n",
      "\n",
      "Training for label 0\n",
      "label\n",
      "0    159\n",
      "1    159\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 286/286 [00:00<00:00, 4494.27 examples/s]\n",
      "Map: 100%|██████████| 32/32 [00:00<00:00, 2282.54 examples/s]\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33malberto-rodero557\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.16.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.15.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\Ghiki\\Documents\\GitHub\\Multidomain--Multimodal-and-Multilingual-Machine-Generated-Text-Detection\\SubtaskB 2.0\\wandb\\run-20240120_093330-lzktccmz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/alberto-rodero557/huggingface/runs/lzktccmz' target=\"_blank\">eager-lake-158</a></strong> to <a href='https://wandb.ai/alberto-rodero557/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/alberto-rodero557/huggingface' target=\"_blank\">https://wandb.ai/alberto-rodero557/huggingface</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/alberto-rodero557/huggingface/runs/lzktccmz' target=\"_blank\">https://wandb.ai/alberto-rodero557/huggingface/runs/lzktccmz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/54 [00:00<?, ?it/s]You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      " 33%|███▎      | 18/54 [00:04<00:07,  4.65it/s]Trainer is attempting to log a value of \"[[11, 5], [1, 15]]\" of type <class 'list'> for key \"eval/confusion_matrix\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "                                               \n",
      " 33%|███▎      | 18/54 [00:04<00:07,  4.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5817766189575195, 'eval_accuracy': 0.8125, 'eval_f1': 0.8333333333333334, 'eval_auc': 0.8125, 'eval_precision': 0.75, 'eval_recall': 0.9375, 'eval_confusion_matrix': [[11, 5], [1, 15]], 'eval_runtime': 0.1643, 'eval_samples_per_second': 194.738, 'eval_steps_per_second': 12.171, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 36/54 [00:10<00:03,  4.65it/s]Trainer is attempting to log a value of \"[[16, 0], [2, 14]]\" of type <class 'list'> for key \"eval/confusion_matrix\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "                                               \n",
      " 67%|██████▋   | 36/54 [00:10<00:03,  4.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.2547818422317505, 'eval_accuracy': 0.9375, 'eval_f1': 0.9333333333333333, 'eval_auc': 0.9375, 'eval_precision': 1.0, 'eval_recall': 0.875, 'eval_confusion_matrix': [[16, 0], [2, 14]], 'eval_runtime': 0.1648, 'eval_samples_per_second': 194.124, 'eval_steps_per_second': 12.133, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54/54 [00:15<00:00,  4.67it/s]Trainer is attempting to log a value of \"[[16, 0], [2, 14]]\" of type <class 'list'> for key \"eval/confusion_matrix\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "                                               \n",
      "100%|██████████| 54/54 [00:15<00:00,  4.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.21345257759094238, 'eval_accuracy': 0.9375, 'eval_f1': 0.9333333333333333, 'eval_auc': 0.9375, 'eval_precision': 1.0, 'eval_recall': 0.875, 'eval_confusion_matrix': [[16, 0], [2, 14]], 'eval_runtime': 0.1629, 'eval_samples_per_second': 196.421, 'eval_steps_per_second': 12.276, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54/54 [00:17<00:00,  3.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 18.7682, 'train_samples_per_second': 45.716, 'train_steps_per_second': 2.877, 'train_loss': 0.3860274420844184, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 26.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PredictionOutput(predictions=array([[ 1.3591527 , -1.367556  ],\n",
      "       [-2.080223  ,  2.0065427 ],\n",
      "       [ 1.1978284 , -1.2842716 ],\n",
      "       [-2.1423118 ,  2.0436175 ],\n",
      "       [ 1.1823822 , -1.2691592 ],\n",
      "       [ 1.368689  , -1.437959  ],\n",
      "       [-2.0632112 ,  1.9494604 ],\n",
      "       [-2.0471585 ,  2.093024  ],\n",
      "       [ 1.283997  , -1.3507924 ],\n",
      "       [ 1.3503917 , -1.4255153 ],\n",
      "       [ 1.295598  , -1.3111292 ],\n",
      "       [ 1.2889085 , -1.4252031 ],\n",
      "       [-2.0277724 ,  2.1149943 ],\n",
      "       [ 0.93307614, -0.9568824 ],\n",
      "       [-2.0161204 ,  2.0337534 ],\n",
      "       [ 1.1350982 , -1.1812134 ],\n",
      "       [-1.6758199 ,  1.7885774 ],\n",
      "       [ 1.190765  , -1.2457278 ],\n",
      "       [-2.1109395 ,  2.099482  ],\n",
      "       [-2.009266  ,  1.9808086 ],\n",
      "       [-1.9287232 ,  1.9721607 ],\n",
      "       [-1.754269  ,  1.6867911 ],\n",
      "       [-2.0530899 ,  2.0136352 ],\n",
      "       [ 1.2567849 , -1.3016502 ],\n",
      "       [ 0.4637889 , -0.61355335],\n",
      "       [ 0.8646828 , -0.92552537],\n",
      "       [-2.2327447 ,  2.12434   ],\n",
      "       [ 0.8192887 , -0.8938779 ],\n",
      "       [ 1.244318  , -1.2370352 ],\n",
      "       [-0.5391083 ,  0.5496468 ],\n",
      "       [ 1.2860186 , -1.3098695 ],\n",
      "       [ 1.1289288 , -1.217649  ]], dtype=float32), label_ids=array([0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1,\n",
      "       1, 0, 0, 0, 1, 0, 0, 1, 0, 0], dtype=int64), metrics={'test_loss': 0.21345257759094238, 'test_accuracy': 0.9375, 'test_f1': 0.9333333333333333, 'test_auc': 0.9375, 'test_precision': 1.0, 'test_recall': 0.875, 'test_confusion_matrix': [[16, 0], [2, 14]], 'test_runtime': 0.1581, 'test_samples_per_second': 202.41, 'test_steps_per_second': 12.651})\n",
      "\n",
      "\n",
      "Training for label 1\n",
      "label\n",
      "0    172\n",
      "1    172\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 309/309 [00:00<00:00, 5060.58 examples/s]\n",
      "Map: 100%|██████████| 35/35 [00:00<00:00, 3330.25 examples/s]\n",
      "  0%|          | 0/60 [00:00<?, ?it/s]You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      " 32%|███▏      | 19/60 [00:04<00:09,  4.51it/s]Trainer is attempting to log a value of \"[[12, 7], [4, 12]]\" of type <class 'list'> for key \"eval/confusion_matrix\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "                                               \n",
      " 33%|███▎      | 20/60 [00:04<00:08,  4.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6276017427444458, 'eval_accuracy': 0.6857142857142857, 'eval_f1': 0.6857142857142857, 'eval_auc': 0.6907894736842104, 'eval_precision': 0.631578947368421, 'eval_recall': 0.75, 'eval_confusion_matrix': [[12, 7], [4, 12]], 'eval_runtime': 0.1775, 'eval_samples_per_second': 197.218, 'eval_steps_per_second': 16.904, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 39/60 [00:10<00:04,  4.36it/s]Trainer is attempting to log a value of \"[[12, 7], [0, 16]]\" of type <class 'list'> for key \"eval/confusion_matrix\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "                                               \n",
      " 67%|██████▋   | 40/60 [00:10<00:04,  4.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5574249625205994, 'eval_accuracy': 0.8, 'eval_f1': 0.8205128205128205, 'eval_auc': 0.8157894736842105, 'eval_precision': 0.6956521739130435, 'eval_recall': 1.0, 'eval_confusion_matrix': [[12, 7], [0, 16]], 'eval_runtime': 0.1768, 'eval_samples_per_second': 197.942, 'eval_steps_per_second': 16.966, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 59/60 [00:15<00:00,  4.48it/s]Trainer is attempting to log a value of \"[[16, 3], [1, 15]]\" of type <class 'list'> for key \"eval/confusion_matrix\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "                                               \n",
      "100%|██████████| 60/60 [00:16<00:00,  4.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.359159380197525, 'eval_accuracy': 0.8857142857142857, 'eval_f1': 0.8823529411764706, 'eval_auc': 0.8898026315789473, 'eval_precision': 0.8333333333333334, 'eval_recall': 0.9375, 'eval_confusion_matrix': [[16, 3], [1, 15]], 'eval_runtime': 0.1829, 'eval_samples_per_second': 191.315, 'eval_steps_per_second': 16.398, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:17<00:00,  3.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 17.6252, 'train_samples_per_second': 52.595, 'train_steps_per_second': 3.404, 'train_loss': 0.5245036443074544, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 35.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PredictionOutput(predictions=array([[-0.20435828,  0.15633969],\n",
      "       [-0.7768489 ,  0.250968  ],\n",
      "       [-1.8090919 ,  1.6144699 ],\n",
      "       [-1.9305148 ,  1.6774125 ],\n",
      "       [-0.22724682,  0.06194476],\n",
      "       [-1.4356672 ,  1.087782  ],\n",
      "       [ 0.684137  , -0.9207112 ],\n",
      "       [-1.5524727 ,  1.2944418 ],\n",
      "       [ 1.5878452 , -1.4768381 ],\n",
      "       [-1.7457817 ,  1.4313656 ],\n",
      "       [-1.7523235 ,  1.5552132 ],\n",
      "       [ 1.5410403 , -1.4765381 ],\n",
      "       [ 1.4940193 , -1.5232819 ],\n",
      "       [-0.924099  ,  0.40566328],\n",
      "       [-1.7242949 ,  1.4463938 ],\n",
      "       [ 0.8200089 , -0.87617326],\n",
      "       [ 0.5484975 , -0.66233194],\n",
      "       [-1.8922565 ,  1.693135  ],\n",
      "       [-1.800823  ,  1.6714047 ],\n",
      "       [-0.593021  ,  0.17281239],\n",
      "       [ 0.46311155, -0.6121122 ],\n",
      "       [ 0.20105484, -0.38297418],\n",
      "       [-1.5593336 ,  1.2955502 ],\n",
      "       [ 0.16226296, -0.35238618],\n",
      "       [ 0.80712175, -0.892857  ],\n",
      "       [-1.7144881 ,  1.4537184 ],\n",
      "       [ 0.19975235, -0.24556567],\n",
      "       [-1.6151897 ,  1.3581653 ],\n",
      "       [-0.01627994, -0.14408325],\n",
      "       [ 0.39420196, -0.59429324],\n",
      "       [ 0.6724771 , -0.9233096 ],\n",
      "       [ 0.5137394 , -0.6098867 ],\n",
      "       [-0.15659422,  0.10552355],\n",
      "       [ 0.28462446, -0.29873157],\n",
      "       [ 0.56331503, -0.6193418 ]], dtype=float32), label_ids=array([1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 0, 0,\n",
      "       1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0], dtype=int64), metrics={'test_loss': 0.359159380197525, 'test_accuracy': 0.8857142857142857, 'test_f1': 0.8823529411764706, 'test_auc': 0.8898026315789473, 'test_precision': 0.8333333333333334, 'test_recall': 0.9375, 'test_confusion_matrix': [[16, 3], [1, 15]], 'test_runtime': 0.1693, 'test_samples_per_second': 206.747, 'test_steps_per_second': 17.721})\n",
      "\n",
      "\n",
      "Training for label 2\n",
      "label\n",
      "0    169\n",
      "1    169\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 304/304 [00:00<00:00, 6531.38 examples/s]\n",
      "Map: 100%|██████████| 34/34 [00:00<00:00, 3364.63 examples/s]\n",
      "  0%|          | 0/57 [00:00<?, ?it/s]You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      " 33%|███▎      | 19/57 [00:04<00:08,  4.51it/s]Trainer is attempting to log a value of \"[[2, 18], [0, 14]]\" of type <class 'list'> for key \"eval/confusion_matrix\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "\n",
      " 33%|███▎      | 19/57 [00:04<00:08,  4.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6751080751419067, 'eval_accuracy': 0.47058823529411764, 'eval_f1': 0.6086956521739131, 'eval_auc': 0.55, 'eval_precision': 0.4375, 'eval_recall': 1.0, 'eval_confusion_matrix': [[2, 18], [0, 14]], 'eval_runtime': 0.1716, 'eval_samples_per_second': 198.114, 'eval_steps_per_second': 17.481, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 38/57 [00:10<00:04,  4.52it/s]Trainer is attempting to log a value of \"[[8, 12], [0, 14]]\" of type <class 'list'> for key \"eval/confusion_matrix\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "                                               \n",
      " 67%|██████▋   | 38/57 [00:10<00:04,  4.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.635382354259491, 'eval_accuracy': 0.6470588235294118, 'eval_f1': 0.7000000000000001, 'eval_auc': 0.7, 'eval_precision': 0.5384615384615384, 'eval_recall': 1.0, 'eval_confusion_matrix': [[8, 12], [0, 14]], 'eval_runtime': 0.1738, 'eval_samples_per_second': 195.584, 'eval_steps_per_second': 17.257, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:15<00:00,  4.67it/s]Trainer is attempting to log a value of \"[[13, 7], [0, 14]]\" of type <class 'list'> for key \"eval/confusion_matrix\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "\n",
      "100%|██████████| 57/57 [00:15<00:00,  4.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.5640297532081604, 'eval_accuracy': 0.7941176470588235, 'eval_f1': 0.8, 'eval_auc': 0.825, 'eval_precision': 0.6666666666666666, 'eval_recall': 1.0, 'eval_confusion_matrix': [[13, 7], [0, 14]], 'eval_runtime': 0.1852, 'eval_samples_per_second': 183.593, 'eval_steps_per_second': 16.199, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 57/57 [00:17<00:00,  3.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 17.0708, 'train_samples_per_second': 53.424, 'train_steps_per_second': 3.339, 'train_loss': 0.6081049065840872, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 34.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PredictionOutput(predictions=array([[ 0.08637703, -0.08790345],\n",
      "       [-0.6629512 ,  0.9621987 ],\n",
      "       [-0.06899299,  0.05285936],\n",
      "       [ 0.5083303 , -0.42344975],\n",
      "       [-0.92346716,  1.2016282 ],\n",
      "       [-0.8169539 ,  0.9896983 ],\n",
      "       [ 0.17533414, -0.18819079],\n",
      "       [-0.39926964,  0.52405214],\n",
      "       [-0.20780736,  0.3423396 ],\n",
      "       [ 0.1667374 , -0.12906155],\n",
      "       [ 0.02122703,  0.02909597],\n",
      "       [-0.85111547,  1.1006504 ],\n",
      "       [-0.74926096,  1.0284059 ],\n",
      "       [-0.02114723,  0.1544601 ],\n",
      "       [-0.04533643,  0.10110208],\n",
      "       [ 0.1436456 , -0.12348482],\n",
      "       [-0.10289803,  0.16720676],\n",
      "       [-0.33224073,  0.53215694],\n",
      "       [-0.5862348 ,  0.85472804],\n",
      "       [ 0.10874398, -0.0365673 ],\n",
      "       [ 0.03913756,  0.11373958],\n",
      "       [-0.09728011,  0.15510148],\n",
      "       [-0.7860497 ,  1.0730821 ],\n",
      "       [ 0.1402562 , -0.13215916],\n",
      "       [ 0.09247153, -0.09143364],\n",
      "       [ 0.14592904, -0.14396968],\n",
      "       [-0.36664566,  0.48463708],\n",
      "       [-0.19152954,  0.3061733 ],\n",
      "       [ 0.19726667, -0.17616513],\n",
      "       [ 0.37720108, -0.35514876],\n",
      "       [-0.48152956,  0.59944975],\n",
      "       [ 0.06251698,  0.01235195],\n",
      "       [-0.8045627 ,  1.0719496 ],\n",
      "       [ 0.14582166, -0.09578348]], dtype=float32), label_ids=array([0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n",
      "       1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0], dtype=int64), metrics={'test_loss': 0.5640297532081604, 'test_accuracy': 0.7941176470588235, 'test_f1': 0.8, 'test_auc': 0.825, 'test_precision': 0.6666666666666666, 'test_recall': 1.0, 'test_confusion_matrix': [[13, 7], [0, 14]], 'test_runtime': 0.1678, 'test_samples_per_second': 202.652, 'test_steps_per_second': 17.881})\n",
      "\n",
      "\n",
      "Training for label 3\n",
      "label\n",
      "0    178\n",
      "1    178\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 320/320 [00:00<00:00, 5920.84 examples/s]\n",
      "Map: 100%|██████████| 36/36 [00:00<00:00, 3424.93 examples/s]\n",
      "  0%|          | 0/60 [00:00<?, ?it/s]You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      " 33%|███▎      | 20/60 [00:04<00:08,  4.47it/s]Trainer is attempting to log a value of \"[[17, 1], [13, 5]]\" of type <class 'list'> for key \"eval/confusion_matrix\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "\n",
      " 33%|███▎      | 20/60 [00:04<00:08,  4.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6897968649864197, 'eval_accuracy': 0.6111111111111112, 'eval_f1': 0.4166666666666667, 'eval_auc': 0.611111111111111, 'eval_precision': 0.8333333333333334, 'eval_recall': 0.2777777777777778, 'eval_confusion_matrix': [[17, 1], [13, 5]], 'eval_runtime': 0.1869, 'eval_samples_per_second': 192.651, 'eval_steps_per_second': 16.054, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 40/60 [00:10<00:04,  4.45it/s]Trainer is attempting to log a value of \"[[9, 9], [6, 12]]\" of type <class 'list'> for key \"eval/confusion_matrix\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "\n",
      " 67%|██████▋   | 40/60 [00:10<00:04,  4.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.672661304473877, 'eval_accuracy': 0.5833333333333334, 'eval_f1': 0.6153846153846153, 'eval_auc': 0.5833333333333333, 'eval_precision': 0.5714285714285714, 'eval_recall': 0.6666666666666666, 'eval_confusion_matrix': [[9, 9], [6, 12]], 'eval_runtime': 0.1823, 'eval_samples_per_second': 197.447, 'eval_steps_per_second': 16.454, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:16<00:00,  4.47it/s]Trainer is attempting to log a value of \"[[6, 12], [3, 15]]\" of type <class 'list'> for key \"eval/confusion_matrix\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "\n",
      "100%|██████████| 60/60 [00:16<00:00,  4.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6580802798271179, 'eval_accuracy': 0.5833333333333334, 'eval_f1': 0.6666666666666667, 'eval_auc': 0.5833333333333335, 'eval_precision': 0.5555555555555556, 'eval_recall': 0.8333333333333334, 'eval_confusion_matrix': [[6, 12], [3, 15]], 'eval_runtime': 0.179, 'eval_samples_per_second': 201.097, 'eval_steps_per_second': 16.758, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:18<00:00,  3.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 18.1552, 'train_samples_per_second': 52.877, 'train_steps_per_second': 3.305, 'train_loss': 0.6723592122395833, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 33.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PredictionOutput(predictions=array([[ 0.00891186,  0.03057183],\n",
      "       [-0.16328064,  0.16737331],\n",
      "       [-0.06584411,  0.11730452],\n",
      "       [-0.13337278,  0.14698479],\n",
      "       [ 0.1968729 , -0.17205904],\n",
      "       [ 0.04135403, -0.0206851 ],\n",
      "       [ 0.11520058, -0.08736792],\n",
      "       [-0.00839227,  0.03754065],\n",
      "       [ 0.09774699, -0.07137944],\n",
      "       [-0.24098292,  0.27013865],\n",
      "       [-0.09760526,  0.09771747],\n",
      "       [-0.10875738,  0.12566149],\n",
      "       [-0.05143604,  0.06832202],\n",
      "       [-0.22114326,  0.22992933],\n",
      "       [-0.11453638,  0.15631332],\n",
      "       [-0.26102144,  0.30380854],\n",
      "       [ 0.04067517,  0.00239846],\n",
      "       [ 0.06739908, -0.03980219],\n",
      "       [-0.02638169,  0.06701081],\n",
      "       [-0.19351128,  0.22124003],\n",
      "       [ 0.01260625,  0.02673431],\n",
      "       [ 0.00066426,  0.03971876],\n",
      "       [-0.22268729,  0.22999252],\n",
      "       [-0.01811118,  0.00090532],\n",
      "       [-0.18239923,  0.21142544],\n",
      "       [-0.20199145,  0.19984998],\n",
      "       [-0.18713191,  0.223202  ],\n",
      "       [-0.21647123,  0.21897845],\n",
      "       [-0.16607533,  0.17659077],\n",
      "       [ 0.15649028, -0.1143577 ],\n",
      "       [ 0.0430107 ,  0.01428841],\n",
      "       [-0.09177543,  0.1468856 ],\n",
      "       [-0.00880718,  0.06146397],\n",
      "       [ 0.00054587,  0.04519537],\n",
      "       [ 0.06443869, -0.01355996],\n",
      "       [-0.06771778,  0.10804763]], dtype=float32), label_ids=array([1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1,\n",
      "       1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0], dtype=int64), metrics={'test_loss': 0.6580802798271179, 'test_accuracy': 0.5833333333333334, 'test_f1': 0.6666666666666667, 'test_auc': 0.5833333333333335, 'test_precision': 0.5555555555555556, 'test_recall': 0.8333333333333334, 'test_confusion_matrix': [[6, 12], [3, 15]], 'test_runtime': 0.1725, 'test_samples_per_second': 208.676, 'test_steps_per_second': 17.39})\n",
      "\n",
      "\n",
      "Training for label 4\n",
      "label\n",
      "0    174\n",
      "1    174\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 313/313 [00:00<00:00, 6021.41 examples/s]\n",
      "Map: 100%|██████████| 35/35 [00:00<00:00, 3685.22 examples/s]\n",
      "  0%|          | 0/60 [00:00<?, ?it/s]You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      " 33%|███▎      | 20/60 [00:04<00:07,  5.04it/s]Trainer is attempting to log a value of \"[[14, 7], [0, 14]]\" of type <class 'list'> for key \"eval/confusion_matrix\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "                                               \n",
      " 33%|███▎      | 20/60 [00:04<00:07,  5.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6063339114189148, 'eval_accuracy': 0.8, 'eval_f1': 0.8, 'eval_auc': 0.8333333333333334, 'eval_precision': 0.6666666666666666, 'eval_recall': 1.0, 'eval_confusion_matrix': [[14, 7], [0, 14]], 'eval_runtime': 0.1691, 'eval_samples_per_second': 207.024, 'eval_steps_per_second': 17.745, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 40/60 [00:10<00:03,  5.07it/s]Trainer is attempting to log a value of \"[[20, 1], [3, 11]]\" of type <class 'list'> for key \"eval/confusion_matrix\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "\n",
      " 67%|██████▋   | 40/60 [00:10<00:03,  5.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.29131242632865906, 'eval_accuracy': 0.8857142857142857, 'eval_f1': 0.8461538461538461, 'eval_auc': 0.869047619047619, 'eval_precision': 0.9166666666666666, 'eval_recall': 0.7857142857142857, 'eval_confusion_matrix': [[20, 1], [3, 11]], 'eval_runtime': 0.1691, 'eval_samples_per_second': 207.006, 'eval_steps_per_second': 17.743, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:16<00:00,  5.15it/s]Trainer is attempting to log a value of \"[[20, 1], [1, 13]]\" of type <class 'list'> for key \"eval/confusion_matrix\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "\n",
      "100%|██████████| 60/60 [00:16<00:00,  5.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.16950753331184387, 'eval_accuracy': 0.9428571428571428, 'eval_f1': 0.9285714285714286, 'eval_auc': 0.9404761904761904, 'eval_precision': 0.9285714285714286, 'eval_recall': 0.9285714285714286, 'eval_confusion_matrix': [[20, 1], [1, 13]], 'eval_runtime': 0.1686, 'eval_samples_per_second': 207.594, 'eval_steps_per_second': 17.794, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:17<00:00,  3.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 17.6862, 'train_samples_per_second': 53.092, 'train_steps_per_second': 3.392, 'train_loss': 0.4370721181233724, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 37.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PredictionOutput(predictions=array([[-1.9490178 ,  2.0316265 ],\n",
      "       [ 1.5227078 , -1.3635459 ],\n",
      "       [-1.9340832 ,  2.0830195 ],\n",
      "       [-1.3084121 ,  1.4560884 ],\n",
      "       [ 1.9918504 , -1.7708483 ],\n",
      "       [ 1.0270512 , -0.68448704],\n",
      "       [-1.8431525 ,  2.0472884 ],\n",
      "       [-1.3297827 ,  1.342704  ],\n",
      "       [-0.24730961,  0.25219274],\n",
      "       [ 1.2789073 , -1.1467907 ],\n",
      "       [ 1.226227  , -1.1811454 ],\n",
      "       [ 1.343309  , -1.2062926 ],\n",
      "       [-1.7623217 ,  2.0356493 ],\n",
      "       [ 1.9279035 , -1.6674129 ],\n",
      "       [-0.26075602,  0.5075524 ],\n",
      "       [ 1.3189132 , -1.0602943 ],\n",
      "       [-1.464678  ,  1.6872567 ],\n",
      "       [ 1.1104039 , -0.88766855],\n",
      "       [ 1.2304451 , -1.0232836 ],\n",
      "       [-0.16647765,  0.32270843],\n",
      "       [-1.9029602 ,  2.0538309 ],\n",
      "       [ 1.1386906 , -1.035981  ],\n",
      "       [ 1.1395421 , -1.185952  ],\n",
      "       [-0.10335716,  0.30097386],\n",
      "       [ 1.2232317 , -1.0556604 ],\n",
      "       [ 1.248432  , -1.2002201 ],\n",
      "       [ 1.2324513 , -0.9984324 ],\n",
      "       [ 0.49405032, -0.34273264],\n",
      "       [ 1.5440437 , -1.3610896 ],\n",
      "       [-1.3208013 ,  1.4995325 ],\n",
      "       [ 1.6566001 , -1.4407666 ],\n",
      "       [ 0.49655223, -0.47384462],\n",
      "       [ 1.1001278 , -1.0504248 ],\n",
      "       [-1.7223814 ,  1.7652504 ],\n",
      "       [ 0.15599474,  0.02628539]], dtype=float32), label_ids=array([1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 1, 0,\n",
      "       0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1], dtype=int64), metrics={'test_loss': 0.16950753331184387, 'test_accuracy': 0.9428571428571428, 'test_f1': 0.9285714285714286, 'test_auc': 0.9404761904761904, 'test_precision': 0.9285714285714286, 'test_recall': 0.9285714285714286, 'test_confusion_matrix': [[20, 1], [1, 13]], 'test_runtime': 0.16, 'test_samples_per_second': 218.708, 'test_steps_per_second': 18.746})\n",
      "\n",
      "\n",
      "Training for label 5\n",
      "label\n",
      "0    148\n",
      "1    148\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Map: 100%|██████████| 266/266 [00:00<00:00, 5012.11 examples/s]\n",
      "Map: 100%|██████████| 30/30 [00:00<00:00, 3526.30 examples/s]\n",
      "  0%|          | 0/51 [00:00<?, ?it/s]You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      " 33%|███▎      | 17/51 [00:03<00:06,  4.98it/s]Trainer is attempting to log a value of \"[[11, 2], [2, 15]]\" of type <class 'list'> for key \"eval/confusion_matrix\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "                                               \n",
      " 33%|███▎      | 17/51 [00:04<00:06,  4.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.6476290225982666, 'eval_accuracy': 0.8666666666666667, 'eval_f1': 0.8823529411764706, 'eval_auc': 0.8642533936651583, 'eval_precision': 0.8823529411764706, 'eval_recall': 0.8823529411764706, 'eval_confusion_matrix': [[11, 2], [2, 15]], 'eval_runtime': 0.1551, 'eval_samples_per_second': 193.422, 'eval_steps_per_second': 12.895, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 34/51 [00:09<00:03,  4.96it/s]Trainer is attempting to log a value of \"[[11, 2], [0, 17]]\" of type <class 'list'> for key \"eval/confusion_matrix\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "\n",
      " 67%|██████▋   | 34/51 [00:09<00:03,  4.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.28423231840133667, 'eval_accuracy': 0.9333333333333333, 'eval_f1': 0.9444444444444444, 'eval_auc': 0.9230769230769231, 'eval_precision': 0.8947368421052632, 'eval_recall': 1.0, 'eval_confusion_matrix': [[11, 2], [0, 17]], 'eval_runtime': 0.1549, 'eval_samples_per_second': 193.711, 'eval_steps_per_second': 12.914, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:14<00:00,  4.97it/s]Trainer is attempting to log a value of \"[[11, 2], [0, 17]]\" of type <class 'list'> for key \"eval/confusion_matrix\" as a scalar. This invocation of Tensorboard's writer.add_scalar() is incorrect so we dropped this attribute.\n",
      "\n",
      "100%|██████████| 51/51 [00:14<00:00,  4.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.21711711585521698, 'eval_accuracy': 0.9333333333333333, 'eval_f1': 0.9444444444444444, 'eval_auc': 0.9230769230769231, 'eval_precision': 0.8947368421052632, 'eval_recall': 1.0, 'eval_confusion_matrix': [[11, 2], [0, 17]], 'eval_runtime': 0.1549, 'eval_samples_per_second': 193.685, 'eval_steps_per_second': 12.912, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 51/51 [00:15<00:00,  3.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 15.697, 'train_samples_per_second': 50.838, 'train_steps_per_second': 3.249, 'train_loss': 0.4747910219080308, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 30.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PredictionOutput(predictions=array([[-1.1500847 ,  1.1474271 ],\n",
      "       [-0.7840526 ,  0.6739131 ],\n",
      "       [ 1.8492463 , -1.9941918 ],\n",
      "       [ 1.813197  , -1.9462798 ],\n",
      "       [-0.8954138 ,  0.77748954],\n",
      "       [ 1.7625248 , -1.8589479 ],\n",
      "       [ 1.6846746 , -1.9103608 ],\n",
      "       [-1.2534664 ,  1.3212765 ],\n",
      "       [-1.0698038 ,  1.0213008 ],\n",
      "       [-0.9071823 ,  0.8377258 ],\n",
      "       [-1.0452782 ,  1.0803397 ],\n",
      "       [-1.0825732 ,  1.1000346 ],\n",
      "       [ 1.7938638 , -1.825214  ],\n",
      "       [-1.1138895 ,  1.1410542 ],\n",
      "       [-1.2842449 ,  1.287307  ],\n",
      "       [ 1.6619502 , -1.5945144 ],\n",
      "       [-0.9776328 ,  0.87743306],\n",
      "       [-1.1470191 ,  1.2442986 ],\n",
      "       [-0.92999357,  0.9439759 ],\n",
      "       [ 1.7240598 , -1.9179192 ],\n",
      "       [ 1.7239888 , -1.7073314 ],\n",
      "       [-0.67987186,  0.7178457 ],\n",
      "       [-1.2051665 ,  1.1256298 ],\n",
      "       [ 1.753485  , -1.8150482 ],\n",
      "       [ 1.8350328 , -1.8648995 ],\n",
      "       [ 1.221591  , -1.1349193 ],\n",
      "       [-0.86628574,  0.94437146],\n",
      "       [-0.9059453 ,  1.0501299 ],\n",
      "       [-1.1256418 ,  1.2152121 ],\n",
      "       [-0.55744886,  0.633975  ]], dtype=float32), label_ids=array([1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1,\n",
      "       1, 0, 0, 0, 1, 1, 1, 1], dtype=int64), metrics={'test_loss': 0.21711711585521698, 'test_accuracy': 0.9333333333333333, 'test_f1': 0.9444444444444444, 'test_auc': 0.9230769230769231, 'test_precision': 0.8947368421052632, 'test_recall': 1.0, 'test_confusion_matrix': [[11, 2], [0, 17]], 'test_runtime': 0.1468, 'test_samples_per_second': 204.366, 'test_steps_per_second': 13.624})\n"
     ]
    }
   ],
   "source": [
    "random_seed = 0\n",
    "\n",
    "df= pd.read_json('datasets/subtaskB_train.jsonl', lines=True)\n",
    "df=df.sample(1000)\n",
    "\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "onevsall_dfs=process_and_balance_dataframes(df)\n",
    "\n",
    "i=0\n",
    "\n",
    "for current_df in onevsall_dfs:\n",
    "    print(f'\\n\\nTraining for label {i}')\n",
    "    print(current_df['label'].value_counts())\n",
    "    \n",
    "    train_df, dev_df = train_test_split(current_df, test_size=0.1, random_state=42)\n",
    "\n",
    "    train_df.to_json('reducedTrainDataFrame.jsonl', orient='records', lines=True)\n",
    "    dev_df.to_json('reducedTrainDataFrame_dev.jsonl', orient='records', lines=True)\n",
    "\n",
    "    train_path =  'reducedTrainDataFrame.jsonl'\n",
    "    test_path =  'reducedTrainDataFrame_dev.jsonl'\n",
    "\n",
    "    model = 'roberta-base'\n",
    "\n",
    "    subtask =  'A'\n",
    "    prediction_path = 'reducedPredictedDataFrame.jsonl'\n",
    "\n",
    "    if not os.path.exists(train_path):\n",
    "        logging.error(\"File doesnt exists: {}\".format(train_path))\n",
    "        raise ValueError(\"File doesnt exists: {}\".format(train_path))\n",
    "\n",
    "    if not os.path.exists(test_path):\n",
    "        logging.error(\"File doesnt exists: {}\".format(train_path))\n",
    "        raise ValueError(\"File doesnt exists: {}\".format(train_path))\n",
    "\n",
    "    id2label = {0: \"not_current_model\", 1: \"current_model\"}\n",
    "    label2id = {\"not_current_model\": 0, \"current_model\": 1}\n",
    "\n",
    "    set_seed(random_seed)\n",
    "\n",
    "    train_df, test_df = get_data(train_path, test_path, random_seed)\n",
    "\n",
    "    fine_tune(train_df, test_df, f\"testing_models/{model.split('/')[-1]}_label{i}_0k\", id2label, label2id, model)\n",
    "\n",
    "    i+=1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
