{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ghiki\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding, AutoTokenizer, set_seed\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.special import softmax\n",
    "import argparse\n",
    "import logging\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import wandb\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score, confusion_matrix\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score, confusion_matrix\n",
    "from transformers import AutoConfig, RobertaModel, RobertaForSequenceClassification\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "import pandas as pd,os\n",
    "import torch\n",
    "from statistics import mode\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline, Trainer\n",
    "from datasets import Dataset, load_metric\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "import pickle\n",
    "from transformers import RobertaConfig, RobertaModel\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch\n",
    "from torch import nn\n",
    "from transformers import AutoConfig, RobertaModel, RobertaForSequenceClassification\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from typing import Optional, Union, Tuple\n",
    "from datasets import Dataset\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from transformers import EarlyStoppingCallback,AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding, AutoTokenizer, set_seed\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.special import softmax\n",
    "import argparse\n",
    "import logging\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "import wandb\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "import torch\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, roc_auc_score, confusion_matrix\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from textstat.textstat import textstatistics\n",
    "import pandas as pd\n",
    "import language_tool_python\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from sklearn.metrics import precision_score, recall_score, roc_auc_score, confusion_matrix\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassificationHead(nn.Module):\n",
    "    \"\"\"Head for sentence-level classification tasks.\"\"\"\n",
    "\n",
    "    def __init__(self, config, num_extra_dims):\n",
    "        super().__init__()\n",
    "        total_dims = config.hidden_size+num_extra_dims\n",
    "        self.dense = nn.Linear(total_dims, total_dims)\n",
    "        classifier_dropout = (\n",
    "            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n",
    "        )\n",
    "        self.dropout = nn.Dropout(classifier_dropout)\n",
    "        self.out_proj = nn.Linear(total_dims, config.num_labels)\n",
    "\n",
    "    def forward(self, features, **kwargs):\n",
    "        x = self.dropout(features)\n",
    "        x = self.dense(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "        return x\n",
    "\n",
    "class CustomSequenceClassification(RobertaForSequenceClassification):\n",
    "\n",
    "    def __init__(self, config, num_extra_dims):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.config = config\n",
    "\n",
    "        # might need to rename this depending on the model\n",
    "        self.roberta =  RobertaModel(config)\n",
    "        self.classifier = ClassificationHead(config, num_extra_dims)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()\n",
    "\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: Optional[torch.LongTensor] = None,\n",
    "        attention_mask: Optional[torch.FloatTensor] = None,\n",
    "        extra_data: Optional[torch.FloatTensor] = None,\n",
    "        token_type_ids: Optional[torch.LongTensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        head_mask: Optional[torch.FloatTensor] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ) -> Union[Tuple, SequenceClassifierOutput]:\n",
    "        r\"\"\"\n",
    "        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n",
    "            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n",
    "            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n",
    "            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n",
    "        \"\"\"\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        outputs = self.roberta(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "\n",
    "        # sequence_output will be (batch_size, seq_length, hidden_size)\n",
    "        sequence_output = outputs[0]\n",
    "\n",
    "        # additional data should be (batch_size, num_extra_dims)\n",
    "        cls_embedding = sequence_output[:, 0, :]\n",
    "\n",
    "        output = torch.cat((cls_embedding, extra_data), dim=-1)\n",
    "\n",
    "        logits = self.classifier(output)\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            if self.config.problem_type is None:\n",
    "                if self.num_labels == 1:\n",
    "                    self.config.problem_type = \"regression\"\n",
    "                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n",
    "                    self.config.problem_type = \"single_label_classification\"\n",
    "                else:\n",
    "                    self.config.problem_type = \"multi_label_classification\"\n",
    "\n",
    "            if self.config.problem_type == \"regression\":\n",
    "                loss_fct = nn.MSELoss()\n",
    "                if self.num_labels == 1:\n",
    "                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n",
    "                else:\n",
    "                    loss = loss_fct(logits, labels)\n",
    "            elif self.config.problem_type == \"single_label_classification\":\n",
    "                loss_fct = nn.CrossEntropyLoss()\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "            elif self.config.problem_type == \"multi_label_classification\":\n",
    "                loss_fct = nn.BCEWithLogitsLoss()\n",
    "                loss = loss_fct(logits, labels)\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "\n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3000 entries, 0 to 2999\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    3000 non-null   object\n",
      " 1   model   3000 non-null   object\n",
      " 2   source  3000 non-null   object\n",
      " 3   label   3000 non-null   int64 \n",
      " 4   id      3000 non-null   int64 \n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 117.3+ KB\n",
      "None\n",
      "\n",
      "label\n",
      "1    500\n",
      "0    500\n",
      "3    500\n",
      "2    500\n",
      "4    500\n",
      "5    500\n",
      "Name: count, dtype: int64\n",
      "\n",
      "model\n",
      "chatGPT    500\n",
      "human      500\n",
      "davinci    500\n",
      "cohere     500\n",
      "bloomz     500\n",
      "dolly      500\n",
      "Name: count, dtype: int64\n",
      "\n",
      "source\n",
      "peerread    3000\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Processing with bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3000/3000 [00:00<00:00, 5037.39 examples/s]\n",
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████| 375/375 [00:11<00:00, 33.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing with roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3000/3000 [00:00<00:00, 6494.51 examples/s]\n",
      "You're using a RobertaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████| 375/375 [00:11<00:00, 34.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing with distilbert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3000/3000 [00:00<00:00, 6208.46 examples/s]\n",
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████| 375/375 [00:05<00:00, 63.82it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing with google/electra-base-discriminator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3000/3000 [00:00<00:00, 5371.47 examples/s]\n",
      "You're using a ElectraTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████| 375/375 [00:10<00:00, 34.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing with albert-base-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 3000/3000 [00:00<00:00, 5066.77 examples/s]\n",
      "You're using a AlbertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "100%|██████████| 375/375 [00:13<00:00, 28.05it/s]\n",
      "Map: 100%|██████████| 3000/3000 [00:02<00:00, 1436.21 examples/s]\n",
      "Map: 100%|██████████| 3000/3000 [00:00<00:00, 28284.34 examples/s]\n",
      "Map: 100%|██████████| 3000/3000 [00:02<00:00, 1462.67 examples/s]\n",
      "Map: 100%|██████████| 3000/3000 [00:00<00:00, 29214.42 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing with multimodal-roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [00:14<00:00, 25.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing with multimodal-extended-roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 375/375 [00:15<00:00, 24.88it/s]\n"
     ]
    }
   ],
   "source": [
    "MODEL1 = 'bert-base-uncased'\n",
    "MODEL2 = 'roberta-base'\n",
    "MODEL3 = 'distilbert-base-uncased'\n",
    "MODEL4 = 'google/electra-base-discriminator'\n",
    "MODEL5 = 'albert-base-v2'\n",
    "MODEL6 = 'multimodal-roberta-base'\n",
    "MODEL7 = 'multimodal-extended-roberta-base'\n",
    "\n",
    "MODEL_PATH1='SavedModels/optimized-bert-base-uncased-50k'\n",
    "MODEL_PATH2='SavedModels/optimized-roberta-base-30k'\n",
    "MODEL_PATH3='SavedModels/optimized-distilbert-base-uncased-50k'\n",
    "MODEL_PATH4='SavedModels/optimized-electra-base-discriminator-50k'\n",
    "MODEL_PATH5='SavedModels/optimized-albert-base-v2-15k'\n",
    "MODEL_PATH6='SavedModels/optimized-roberta-base-multimodal-70k'\n",
    "MODEL_PATH7='SavedModels/optimized-roberta-base-multimodal-extended-70k'\n",
    "\n",
    "'''Preparing data'''\n",
    "\n",
    "df = pd.read_json('datasets/subtaskB_dev.jsonl', lines=True)\n",
    "\n",
    "print(f'Original dataset')\n",
    "print(df.info())\n",
    "\n",
    "print(f'''\\n{df['label'].value_counts()}''')\n",
    "print(f'''\\n{df['model'].value_counts()}''')\n",
    "print(f'''\\n{df['source'].value_counts()}''')\n",
    "\n",
    "df = df[['text', 'label']]\n",
    "test_df=df\n",
    "\n",
    "test_texts = test_df['text'].tolist()\n",
    "\n",
    "from scipy.special import expit\n",
    "\n",
    "def getPrediction(model_path):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "    id2label = {0: 'human', 1: 'chatGPT', 2: 'cohere', 3: 'davinci', 4: 'bloomz', 5: 'dolly'}\n",
    "    label2id = {'human': 0, 'chatGPT': 1,'cohere': 2, 'davinci': 3, 'bloomz': 4, 'dolly': 5}\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        model_path, num_labels=len(label2id), id2label=id2label, label2id=label2id\n",
    "        )\n",
    "\n",
    "    test_dataset = Dataset.from_pandas(test_df)\n",
    "\n",
    "    def preprocess_function(examples, **fn_kwargs):\n",
    "        return fn_kwargs['tokenizer'](examples[\"text\"], truncation=True)\n",
    "\n",
    "    tokenized_test_dataset = test_dataset.map(preprocess_function, batched=True,  fn_kwargs={'tokenizer': tokenizer})\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "    # create Trainer\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        # compute_metrics=compute_metrics,\n",
    "    )\n",
    "    # get logits from predictions and evaluate results using classification report\n",
    "    predictions = trainer.predict(tokenized_test_dataset)\n",
    "    probs = expit(predictions.predictions)  # Use sigmoid instead of softmax\n",
    "    preds = np.argmax(predictions.predictions, axis=-1)\n",
    "\n",
    "    return preds, probs\n",
    "\n",
    "def getPredictionMultidomain(model_path,num_extra_dims,test_data):\n",
    "    config = RobertaConfig.from_pretrained(model_path)\n",
    "    model = CustomSequenceClassification(config, num_extra_dims)\n",
    "    model.load_state_dict(torch.load(model_path+'/pytorch_model.bin'))\n",
    "    trainer = Trainer(model=model)\n",
    "    # get logits from predictions and evaluate results using classification report\n",
    "    predictions = trainer.predict(test_data)\n",
    "    probs = expit(predictions.predictions)  # Use sigmoid instead of softmax\n",
    "    preds = np.argmax(predictions.predictions, axis=-1)\n",
    "\n",
    "    return preds, probs\n",
    "    \n",
    "    preds = np.argmax(predictions.predictions, axis=-1)\n",
    "    probs = softmax(predictions.predictions, axis=-1)\n",
    "    label_specific_probs = probs[:, 1]  # This extracts the probability for label 1\n",
    "    \n",
    "    return list(preds),list(label_specific_probs)\n",
    "\n",
    "print(f'\\nProcessing with {MODEL1}')\n",
    "labels1,scores1=getPrediction(MODEL_PATH1)\n",
    "\n",
    "print(f'\\nProcessing with {MODEL2}')\n",
    "labels2,scores2=getPrediction(MODEL_PATH2)\n",
    "\n",
    "print(f'\\nProcessing with {MODEL3}')\n",
    "labels3,scores3=getPrediction(MODEL_PATH3)\n",
    "\n",
    "print(f'\\nProcessing with {MODEL4}')\n",
    "labels4,scores4=getPrediction(MODEL_PATH4)\n",
    "\n",
    "print(f'\\nProcessing with {MODEL5}')\n",
    "labels5,scores5=getPrediction(MODEL_PATH5)\n",
    "\n",
    "df_multidomain_extended=pd.read_csv('datasets/features_df_test_extended.csv')\n",
    "\n",
    "reduced_columns=['word_count','avg_sentence_length','avg_word_length','gunning_fog_index','grammatical_errors','text','label']\n",
    "df_multidomain=df_multidomain_extended[reduced_columns]\n",
    "\n",
    "df_multidomain_extraData=df_multidomain.drop(['label'],axis=1)\n",
    "df_multidomain_extraData=df_multidomain_extraData.drop(['text'],axis=1)\n",
    "\n",
    "df_multidomain_extended_extraData=df_multidomain_extended.drop(['label'],axis=1)\n",
    "df_multidomain_extended_extraData=df_multidomain_extended_extraData.drop(['text'],axis=1)\n",
    "\n",
    "ds_test_extended = Dataset.from_dict({\n",
    "        \"text\": df_multidomain_extended['text'].tolist(), \n",
    "        \"extra_data\": df_multidomain_extended_extraData.values.tolist(),\n",
    "        \"labels\": df_multidomain_extended['label'].tolist()\n",
    "    })\n",
    "\n",
    "ds_test = Dataset.from_dict({\n",
    "        \"text\": df_multidomain['text'].tolist(), \n",
    "        \"extra_data\": df_multidomain_extraData.values.tolist(),\n",
    "        \"labels\": df_multidomain['label'].tolist()\n",
    "    })\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "tokenized_ds_test_extended = ds_test_extended.map(lambda x: tokenizer(x[\"text\"], padding='max_length', truncation=True))\n",
    "tokenized_ds_test_extended = tokenized_ds_test_extended.map(lambda x: {'extra_data': x['extra_data']})\n",
    "\n",
    "tokenized_ds_test = ds_test.map(lambda x: tokenizer(x[\"text\"], padding='max_length', truncation=True))\n",
    "tokenized_ds_test = tokenized_ds_test.map(lambda x: {'extra_data': x['extra_data']})\n",
    "\n",
    "print(f'\\nProcessing with {MODEL6}')\n",
    "labels6,scores6=getPredictionMultidomain(MODEL_PATH6,5,tokenized_ds_test)\n",
    "\n",
    "print(f'\\nProcessing with {MODEL7}')\n",
    "labels7,scores7=getPredictionMultidomain(MODEL_PATH7,9,tokenized_ds_test_extended)\n",
    "\n",
    "scores1 = [scores1[i][labels1[i]] for i in range(len(labels1))]\n",
    "scores2 = [scores2[i][labels2[i]] for i in range(len(labels2))]\n",
    "scores3 = [scores3[i][labels3[i]] for i in range(len(labels3))]\n",
    "scores4 = [scores4[i][labels4[i]] for i in range(len(labels4))]\n",
    "scores5 = [scores5[i][labels5[i]] for i in range(len(labels5))]\n",
    "scores6 = [scores6[i][labels6[i]] for i in range(len(labels6))]\n",
    "scores7 = [scores7[i][labels7[i]] for i in range(len(labels7))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-uncased\n",
      "{'accuracy': 0.6583333333333333, 'f1': 0.6185394311264053, 'precision': 0.6192174884481815, 'recall': 0.6583333333333333}\n",
      "\n",
      "roberta-base\n",
      "{'accuracy': 0.7473333333333333, 'f1': 0.7167924853853949, 'precision': 0.7252336532426344, 'recall': 0.7473333333333333}\n",
      "\n",
      "distilbert-base-uncased\n",
      "{'accuracy': 0.654, 'f1': 0.6295894012802586, 'precision': 0.6580674916276797, 'recall': 0.6539999999999999}\n",
      "\n",
      "google/electra-base-discriminator\n",
      "{'accuracy': 0.684, 'f1': 0.6546814757852748, 'precision': 0.6841279878821579, 'recall': 0.684}\n",
      "\n",
      "albert-base-v2\n",
      "{'accuracy': 0.674, 'f1': 0.654733141514451, 'precision': 0.6594659303977864, 'recall': 0.6739999999999999}\n",
      "\n",
      "multimodal-roberta-base\n",
      "{'accuracy': 0.759, 'f1': 0.7186877887437483, 'precision': 0.7309157154173311, 'recall': 0.759}\n",
      "\n",
      "multimodal-extended-roberta-base\n",
      "{'accuracy': 0.742, 'f1': 0.7095807039739542, 'precision': 0.7254381710357943, 'recall': 0.742}\n"
     ]
    }
   ],
   "source": [
    "def getMetrics(predicted_labels, true_labels):\n",
    "    # Ensure the labels are numpy arrays\n",
    "    predicted_labels = np.array(predicted_labels)\n",
    "    true_labels = np.array(true_labels)\n",
    "\n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "\n",
    "    # Calculate precision, recall, and F1-score\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predicted_labels, average='macro', zero_division=0)\n",
    "\n",
    "    # Create a dictionary of metrics\n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'f1': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "    }\n",
    "\n",
    "    return metrics\n",
    "\n",
    "print(MODEL1)\n",
    "print(getMetrics(labels1,test_df['label'].tolist()))\n",
    "print('')\n",
    "print(MODEL2)\n",
    "print(getMetrics(labels2,test_df['label'].tolist()))\n",
    "print('')\n",
    "print(MODEL3)\n",
    "print(getMetrics(labels3,test_df['label'].tolist()))\n",
    "print('')\n",
    "print(MODEL4)\n",
    "print(getMetrics(labels4,test_df['label'].tolist()))\n",
    "print('')\n",
    "print(MODEL5)\n",
    "print(getMetrics(labels5,test_df['label'].tolist()))\n",
    "print('')\n",
    "print(MODEL6)\n",
    "print(getMetrics(labels6,test_df['label'].tolist()))\n",
    "print('')\n",
    "print(MODEL7)\n",
    "print(getMetrics(labels7,test_df['label'].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3000 entries, 0 to 2999\n",
      "Data columns (total 14 columns):\n",
      " #   Column                                    Non-Null Count  Dtype  \n",
      "---  ------                                    --------------  -----  \n",
      " 0   Labels_bert-base-uncased                  3000 non-null   int64  \n",
      " 1   Scores_bert-base-uncased                  3000 non-null   float32\n",
      " 2   Labels_roberta-base                       3000 non-null   int64  \n",
      " 3   Scores_roberta-base                       3000 non-null   float32\n",
      " 4   Labels_distilbert-base-uncased            3000 non-null   int64  \n",
      " 5   Scores_distilbert-base-uncased            3000 non-null   float32\n",
      " 6   Labels_google/electra-base-discriminator  3000 non-null   int64  \n",
      " 7   Scores_google/electra-base-discriminator  3000 non-null   float32\n",
      " 8   Labels_albert-base-v2                     3000 non-null   int64  \n",
      " 9   Scores_albert-base-v2                     3000 non-null   float32\n",
      " 10  Labels_multimodal-roberta-base            3000 non-null   int64  \n",
      " 11  Scores_multimodal-roberta-base            3000 non-null   float32\n",
      " 12  Labels_multimodal-extended-roberta-base   3000 non-null   int64  \n",
      " 13  Scores_multimodal-extended-roberta-base   3000 non-null   float32\n",
      "dtypes: float32(7), int64(7)\n",
      "memory usage: 246.2 KB\n",
      "Majority Voting: 0.7413333333333333\n",
      "Majority Score Tie Break Voting: 0.7426666666666667\n",
      "Rank Voting: 0.7426666666666667\n",
      "Borda Count Voting: 0.7596666666666667\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "models = [MODEL1, MODEL2, MODEL3, MODEL4, MODEL5, MODEL6, MODEL7]\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    f'Labels_{MODEL1}': labels1,\n",
    "    f'Scores_{MODEL1}': scores1,\n",
    "    f'Labels_{MODEL2}': labels2,\n",
    "    f'Scores_{MODEL2}': scores2,\n",
    "    f'Labels_{MODEL3}': labels3,\n",
    "    f'Scores_{MODEL3}': scores3,\n",
    "    f'Labels_{MODEL4}': labels4,\n",
    "    f'Scores_{MODEL4}': scores4,\n",
    "    f'Labels_{MODEL5}': labels5,\n",
    "    f'Scores_{MODEL5}': scores5,\n",
    "    f'Labels_{MODEL6}': labels6,\n",
    "    f'Scores_{MODEL6}': scores6,\n",
    "    f'Labels_{MODEL7}': labels7,\n",
    "    f'Scores_{MODEL7}': scores7\n",
    "})\n",
    "\n",
    "labels = test_df['label'].tolist()\n",
    "\n",
    "df.info()\n",
    "\n",
    "def ensemble_methods(df, models):\n",
    "    def majority_voting(df, models):\n",
    "        \"\"\" Majority Voting Ensemble \"\"\"\n",
    "        votes = df[[f'Labels_{m}' for m in models]].mode(axis=1)\n",
    "        return votes[0]\n",
    "\n",
    "    def averaging_ensemble(df, models, threshold=0.5):\n",
    "        \"\"\" Averaging Ensemble with threshold for class labels \"\"\"\n",
    "        avg_scores = df[[f'Scores_{m}' for m in models]].mean(axis=1)\n",
    "        return (avg_scores >= threshold).astype(int)\n",
    "    \n",
    "    def majority_voting_with_tie_breaking(df, models):\n",
    "        \"\"\" Enhanced Majority Voting with Tie-Breaking by Score Mean \"\"\"\n",
    "        votes = df[[f'Labels_{m}' for m in models]].mode(axis=1)\n",
    "        final_labels = []\n",
    "\n",
    "        for row in range(len(df)):\n",
    "            if len(votes.columns) == 1:\n",
    "                final_labels.append(votes.iloc[row, 0])  # No tie\n",
    "            else:\n",
    "                # In case of a tie, break it by mean score\n",
    "                tied_labels = votes.iloc[row]\n",
    "                tied_scores = {}\n",
    "                for label in tied_labels:\n",
    "                    if pd.notna(label):\n",
    "                        tied_scores[label] = df.loc[row, [f'Scores_{m}' for m in models if df.loc[row, f'Labels_{m}'] == label]].mean()\n",
    "                if tied_scores:\n",
    "                    final_labels.append(max(tied_scores, key=tied_scores.get))\n",
    "                else:\n",
    "                    final_labels.append(np.nan)  # Handle case with all NaNs\n",
    "\n",
    "        return final_labels\n",
    "\n",
    "    def mean_score_voting(df, models):\n",
    "        \"\"\" Mean Score Voting with Tie-Breaking based on Vote Count \"\"\"\n",
    "        # Create a DataFrame to hold the mean scores for each label\n",
    "        mean_scores_df = pd.DataFrame()\n",
    "\n",
    "        # Calculate mean scores for each label\n",
    "        for label in set(df[[f'Labels_{m}' for m in models]].values.flatten()):\n",
    "            if pd.notna(label):\n",
    "                mean_scores_df[label] = df[[f'Scores_{m}' for m in models if df[f'Labels_{m}'].iloc[0] == label]].mean(axis=1)\n",
    "\n",
    "        # Determine the label with the highest mean score for each row\n",
    "        max_mean_scores = mean_scores_df.idxmax(axis=1)\n",
    "\n",
    "        final_labels = []\n",
    "        for i, label in enumerate(max_mean_scores):\n",
    "            # Check for ties in mean scores\n",
    "            tied_labels = mean_scores_df.columns[mean_scores_df.iloc[i] == mean_scores_df.iloc[i][label]]\n",
    "            if len(tied_labels) > 1:\n",
    "                # If there's a tie, choose the label with the most votes\n",
    "                vote_counts = df[[f'Labels_{m}' for m in models]].apply(lambda x: (x == tied_labels).sum(), axis=1).iloc[i]\n",
    "                final_labels.append(vote_counts.idxmax())\n",
    "            else:\n",
    "                final_labels.append(label)\n",
    "\n",
    "        return final_labels\n",
    "    \n",
    "    def rank_voting(df, models, top_n=3):\n",
    "        \"\"\" Rank Voting \"\"\"\n",
    "        final_labels = []\n",
    "        for row in range(len(df)):\n",
    "            points = {label: 0 for label in set(df.iloc[row, [df.columns.get_loc(f'Labels_{m}') for m in models]].values) if pd.notna(label)}\n",
    "            for m in models:\n",
    "                label = df.at[row, f'Labels_{m}']\n",
    "                score = df.at[row, f'Scores_{m}']\n",
    "                # Assign points based on score, higher score gets more points\n",
    "                if pd.notna(label):\n",
    "                    points[label] += score\n",
    "            # Sort the labels based on points and take the top label\n",
    "            sorted_labels = sorted(points, key=points.get, reverse=True)\n",
    "            final_labels.append(sorted_labels[0] if sorted_labels else np.nan)\n",
    "        return final_labels\n",
    "    \n",
    "    def borda_count_voting(df, models):\n",
    "        \"\"\" Borda Count Voting \"\"\"\n",
    "        final_labels = []\n",
    "\n",
    "        for row in range(len(df)):\n",
    "            points = {label: 0 for label in set(df[[f'Labels_{m}' for m in models]].iloc[row].values) if pd.notna(label)}\n",
    "\n",
    "            # Collect scores and labels for the current row across all models\n",
    "            scores_labels = [(df.at[row, f'Scores_{m}'], df.at[row, f'Labels_{m}']) for m in models]\n",
    "\n",
    "            # Sort the scores_labels based on scores\n",
    "            scores_labels.sort(reverse=True)\n",
    "\n",
    "            # Assign Borda points based on ranking\n",
    "            for i, (_, label) in enumerate(scores_labels):\n",
    "                if pd.notna(label):\n",
    "                    points[label] += len(models) - i\n",
    "\n",
    "            # Choose the label with maximum points, handle the case with no valid labels\n",
    "            final_labels.append(max(points, key=points.get) if points else np.nan)\n",
    "\n",
    "        return final_labels\n",
    "\n",
    "    \n",
    "    return {\n",
    "        'Majority Voting':majority_voting(df, models),\n",
    "        # 'Average Voting':averaging_ensemble(df, models),\n",
    "        'Majority Score Tie Break Voting': majority_voting_with_tie_breaking(df, models),\n",
    "        # 'Mean Score Voting': mean_score_voting(df, models),\n",
    "        'Rank Voting': rank_voting(df, models),\n",
    "        'Borda Count Voting': borda_count_voting(df, models),\n",
    "    }\n",
    "    \n",
    "    \n",
    "ensemble_results = ensemble_methods(df, models)\n",
    "\n",
    "for voting_method, details in ensemble_results.items():\n",
    "    print(f'''{voting_method}: {getMetrics(details,labels)['accuracy']}''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model Combinations: 100%|██████████| 7/7 [03:59<00:00, 34.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Accuracy Score: 0.768\n",
      "Best Model Combination: ('roberta-base', 'multimodal-roberta-base')\n",
      "Best Voting Method: Majority Voting\n",
      "Best Metrics: {'accuracy': 0.768, 'f1': 0.7266056974216345, 'precision': 0.7286733771545394, 'recall': 0.7679999999999999}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "\n",
    "# Assuming all_models is a list of your model names\n",
    "all_models = [MODEL1, MODEL2, MODEL3, MODEL4, MODEL5, MODEL6, MODEL7]\n",
    "\n",
    "# This will store the best F1 score, the corresponding model combination, and the voting method\n",
    "best_f1_score = 0\n",
    "best_model_combination = None\n",
    "best_voting_method = None\n",
    "best_metrics=None\n",
    "\n",
    "# Define the voting methods you want to evaluate\n",
    "voting_methods = ['Majority Voting','Majority Score Tie Break Voting','Rank Voting','Borda Count Voting']\n",
    "\n",
    "# Try all possible combinations of the models\n",
    "for r in tqdm(range(1, len(all_models) + 1), desc='Model Combinations'):\n",
    "    for model_combination in itertools.combinations(all_models, r):\n",
    "        # Generate the predictions using the ensemble of the current combination of models\n",
    "        ensemble_results = ensemble_methods(df, model_combination)\n",
    "        \n",
    "        # Evaluate each voting method\n",
    "        for method in voting_methods:\n",
    "            metrics=getMetrics(ensemble_results[method], labels)\n",
    "            f1_score=metrics['accuracy']\n",
    "            \n",
    "            # Update the best combination if the current one is better\n",
    "            if f1_score > best_f1_score:\n",
    "                best_f1_score = f1_score\n",
    "                best_model_combination = model_combination\n",
    "                best_voting_method = method\n",
    "                best_metrics=metrics\n",
    "\n",
    "# Print the best combination, its score, and the voting method\n",
    "print(f\"Best Accuracy Score: {best_f1_score}\")\n",
    "print(f\"Best Model Combination: {best_model_combination}\")\n",
    "print(f\"Best Voting Method: {best_voting_method}\")\n",
    "print(f\"Best Metrics: {best_metrics}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
